#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Epic7 í†µí•© ëª¨ë‹ˆí„° v4.6 - Mode ë¶„ë¦¬ ì™„ì„±ë³¸
Master ìš”êµ¬ì‚¬í•­: --mode íŒŒë¼ë¯¸í„°ì™€ 15ë¶„ ì£¼ê¸° korea/global ë¶„ë¦¬ ë¡œì§ ì¶”ê°€

v4.6 í•µì‹¬ ì¶”ê°€ì‚¬í•­:
- --mode íŒŒë¼ë¯¸í„° ì§€ì› (korea/global/all) âœ¨NEWâœ¨
- 15ë¶„ ì£¼ê¸° ìŠ¤ì¼€ì¤„ ë¡œì§ êµ¬í˜„ âœ¨NEWâœ¨
- korea/global ëª¨ë“œë³„ í¬ë¡¤ë§ ë¶„ë¦¬ âœ¨NEWâœ¨
- ì‚¬ì´íŠ¸ë³„ ë…ë¦½ í¬ë¡¤ë§ í•¨ìˆ˜ âœ¨NEWâœ¨

v4.5 ê¸°ì¡´ ê¸°ëŠ¥ ì™„ì „ ë³´ì¡´:
- ì—ëŸ¬ í•¸ë“¤ë§ ê³ ë„í™” ë° ê´€ë¦¬ì ì•Œë¦¼ ì‹œìŠ¤í…œ âœ…
- ì¹˜ëª…ì  ì—ëŸ¬ ìë™ ì•Œë¦¼ âœ…
- ì—ëŸ¬ ë³µêµ¬ ì „ëµ ë° ìë™ ë³µêµ¬ âœ…
- ê²Œì‹œê¸€ë³„ ì¦‰ì‹œ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì•ˆì •ì„± ê°•í™” âœ…

Author: Epic7 Monitoring Team
Version: 4.6 (Mode ë¶„ë¦¬ ì™„ì„±ë³¸)
Date: 2025-07-28
Enhanced: --mode íŒŒë¼ë¯¸í„°, 15ë¶„ ì£¼ê¸° korea/global ë¶„ë¦¬
Fixed: ë°ì´í„° ê²€ì¦, ì˜ì¡´ì„± ì•ˆì „í™”, json ì²˜ë¦¬, ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€ (Master 5ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ)
"""

import os
import sys
import json
import argparse
import time
import asyncio
import concurrent.futures
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Callable
import logging
from pathlib import Path
import signal
import fcntl
import traceback
import psutil
import requests

# ğŸ”§ ìˆ˜ì • 3: crawler ì˜ì¡´ì„± ì•ˆì „í™” (try-except import ë³´í˜¸)
try:
    from crawler import (
        crawl_by_schedule,
        crawl_frequent_sites,
        crawl_regular_sites,
        get_all_posts_for_report,
        mark_as_processed
    )
    CRAWLER_AVAILABLE = True
except ImportError as e:
    logger.warning(f"crawler ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    CRAWLER_AVAILABLE = False
    # í´ë°± í•¨ìˆ˜ë“¤ ì •ì˜
    def crawl_by_schedule(*args, **kwargs):
        return []
    def crawl_frequent_sites(*args, **kwargs):
        return []
    def crawl_regular_sites(*args, **kwargs):
        return []
    def get_all_posts_for_report(*args, **kwargs):
        return []
    def mark_as_processed(*args, **kwargs):
        pass

try:
    from classifier import (
        Epic7Classifier,
        is_bug_post,
        is_high_priority_bug,
        extract_bug_severity,
        should_send_realtime_alert
    )
    CLASSIFIER_AVAILABLE = True
except ImportError as e:
    logger.warning(f"classifier ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    CLASSIFIER_AVAILABLE = False
    # í´ë°± í´ë˜ìŠ¤ ì •ì˜
    class Epic7Classifier:
        def classify_post(self, post_data):
            return {
                'category': 'neutral',
                'sentiment_analysis': {'sentiment': 'neutral', 'confidence': 0.5}
            }

from notifier import (
    send_bug_alert,
    send_sentiment_notification,
    send_daily_report,
    send_health_check
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# =============================================================================
# v4.5 ì—ëŸ¬ ê´€ë¦¬ ì‹œìŠ¤í…œ ì™„ì „ ë³´ì¡´
# =============================================================================

class ErrorType:
    """ì—ëŸ¬ ìœ í˜• ì •ì˜"""
    FILE_IO = "file_io"
    NETWORK = "network"
    MEMORY = "memory"
    IMPORT = "import"
    DATA_PARSING = "data_parsing"
    CLASSIFICATION = "classification"
    NOTIFICATION = "notification"
    CRAWLING = "crawling"
    CRITICAL = "critical"

class ErrorSeverity:
    """ì—ëŸ¬ ì‹¬ê°ë„ ì •ì˜"""
    LOW = 1
    MEDIUM = 2
    HIGH = 3
    CRITICAL = 4

class ErrorRecoveryStrategy:
    """ì—ëŸ¬ ë³µêµ¬ ì „ëµ ì •ì˜"""
    RETRY = "retry"
    FALLBACK = "fallback"
    SKIP = "skip"
    ALERT_AND_CONTINUE = "alert_and_continue"
    EMERGENCY_SHUTDOWN = "emergency_shutdown"

class ErrorManager:
    """ê³ ë„í™”ëœ ì—ëŸ¬ ê´€ë¦¬ ì‹œìŠ¤í…œ (v4.5 ì™„ì „ ë³´ì¡´)"""
    
    def __init__(self):
        self.error_stats = {
            'total_errors': 0,
            'by_type': {},
            'by_severity': {},
            'recovery_attempts': 0,
            'recovery_success': 0,
            'critical_alerts_sent': 0,
            'last_critical_alert': None,
            'start_time': datetime.now().isoformat()
        }
        self.critical_alert_cooldown = 300  # 5ë¶„ ì¿¨ë‹¤ìš´
        self.max_recovery_attempts = 3
        
        # ì—ëŸ¬ ìœ í˜•ë³„ ë³µêµ¬ ì „ëµ
        self.recovery_strategies = {
            ErrorType.FILE_IO: ErrorRecoveryStrategy.RETRY,
            ErrorType.NETWORK: ErrorRecoveryStrategy.RETRY,
            ErrorType.MEMORY: ErrorRecoveryStrategy.FALLBACK,
            ErrorType.IMPORT: ErrorRecoveryStrategy.FALLBACK,
            ErrorType.DATA_PARSING: ErrorRecoveryStrategy.SKIP,
            ErrorType.CLASSIFICATION: ErrorRecoveryStrategy.SKIP,
            ErrorType.NOTIFICATION: ErrorRecoveryStrategy.ALERT_AND_CONTINUE,
            ErrorType.CRAWLING: ErrorRecoveryStrategy.RETRY,
            ErrorType.CRITICAL: ErrorRecoveryStrategy.EMERGENCY_SHUTDOWN
        }
        
        logger.info("âœ¨ ê³ ë„í™”ëœ ì—ëŸ¬ ê´€ë¦¬ ì‹œìŠ¤í…œ v4.5 ì´ˆê¸°í™” ì™„ë£Œ")
    
    def handle_error(self, 
                      error: Exception, 
                      error_type: str, 
                      severity: int, 
                      context: Dict = None,
                      recovery_callback: Callable = None) -> bool:
        """í†µí•© ì—ëŸ¬ í•¸ë“¤ë§ ì‹œìŠ¤í…œ (v4.5 ì™„ì „ ë³´ì¡´)"""
        try:
            # ì—ëŸ¬ í†µê³„ ì—…ë°ì´íŠ¸
            self._update_error_stats(error_type, severity)
            
            # ì—ëŸ¬ ìƒì„¸ ì •ë³´ ë¡œê¹…
            error_info = self._format_error_info(error, error_type, severity, context)
            
            if severity == ErrorSeverity.CRITICAL:
                logger.critical(error_info)
                self._send_critical_alert(error, error_type, context)
                
                if self.recovery_strategies.get(error_type) == ErrorRecoveryStrategy.EMERGENCY_SHUTDOWN:
                    logger.critical("ğŸš¨ ì¹˜ëª…ì  ì—ëŸ¬ë¡œ ì¸í•œ ë¹„ìƒ ì¢…ë£Œ ì‹œì‘")
                    return False
                    
            elif severity == ErrorSeverity.HIGH:
                logger.error(error_info)
                self._send_high_priority_alert(error, error_type, context)
            elif severity == ErrorSeverity.MEDIUM:
                logger.warning(error_info)
            else:
                logger.info(error_info)
            
            # ë³µêµ¬ ì‹œë„
            recovery_success = self._attempt_recovery(error, error_type, recovery_callback)
            
            return recovery_success
            
        except Exception as e:
            logger.critical(f"ğŸ’¥ ì—ëŸ¬ í•¸ë“¤ëŸ¬ ìì²´ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False
    
    def _update_error_stats(self, error_type: str, severity: int):
        """ì—ëŸ¬ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.error_stats['total_errors'] += 1
        
        if error_type not in self.error_stats['by_type']:
            self.error_stats['by_type'][error_type] = 0
        self.error_stats['by_type'][error_type] += 1
        
        if severity not in self.error_stats['by_severity']:
            self.error_stats['by_severity'][severity] = 0
        self.error_stats['by_severity'][severity] += 1
    
    def _format_error_info(self, error: Exception, error_type: str, severity: int, context: Dict = None) -> str:
        """ì—ëŸ¬ ì •ë³´ í¬ë§·íŒ…"""
        severity_labels = {
            ErrorSeverity.LOW: "ğŸŸ¢ LOW",
            ErrorSeverity.MEDIUM: "ğŸŸ¡ MEDIUM",
            ErrorSeverity.HIGH: "ğŸŸ  HIGH",
            ErrorSeverity.CRITICAL: "ğŸ”´ CRITICAL"
        }
        
        error_info = f"""
âš ï¸ ì—ëŸ¬ ë°œìƒ ìƒì„¸ ì •ë³´:
- ìœ í˜•: {error_type}
- ì‹¬ê°ë„: {severity_labels.get(severity, 'UNKNOWN')}
- ë©”ì‹œì§€: {str(error)}
- ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}
"""
        
        if context:
            error_info += f"- ì»¨í…ìŠ¤íŠ¸: {json.dumps(context, ensure_ascii=False, indent=2)}"
        
        return error_info.strip()
    
    def _send_critical_alert(self, error: Exception, error_type: str, context: Dict = None):
        """ì¹˜ëª…ì  ì—ëŸ¬ ìë™ ì•Œë¦¼ (v4.5 ì™„ì „ ë³´ì¡´)"""
        try:
            if self._is_alert_cooldown():
                logger.warning("ì¹˜ëª…ì  ì—ëŸ¬ ì•Œë¦¼ì´ ì¿¨ë‹¤ìš´ ì¤‘ì…ë‹ˆë‹¤")
                return
            
            critical_webhook = os.environ.get('DISCORD_WEBHOOK_CRITICAL_ERROR')
            if not critical_webhook:
                logger.error("ì¹˜ëª…ì  ì—ëŸ¬ ì›¹í›…ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return
            
            system_info = self._get_system_info()
            
            alert_message = {
                "username": "Epic7 Critical Alert",
                "avatar_url": "https://cdn.discordapp.com/emojis/1234567890123456789.png",
                "embeds": [{
                    "title": "ğŸš¨ Epic7 ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì¹˜ëª…ì  ì˜¤ë¥˜",
                    "description": f"**ì—ëŸ¬ ìœ í˜•:** {error_type}\\n**ì—ëŸ¬ ë©”ì‹œì§€:** {str(error)}",
                    "color": 16711680,
                    "fields": [
                        {
                            "name": "ğŸ• ë°œìƒ ì‹œê°„",
                            "value": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                            "inline": True
                        },
                        {
                            "name": "ğŸ’» ì‹œìŠ¤í…œ ìƒíƒœ",
                            "value": system_info,
                            "inline": True
                        },
                        {
                            "name": "ğŸ“Š ì—ëŸ¬ í†µê³„",
                            "value": f"ì´ ì—ëŸ¬: {self.error_stats['total_errors']}ê°œ\\nì¹˜ëª…ì  ì•Œë¦¼: {self.error_stats['critical_alerts_sent']}ê°œ",
                            "inline": False
                        }
                    ],
                    "footer": {
                        "text": "Epic7 Critical Error Alert System v4.6"
                    },
                    "timestamp": datetime.now().isoformat()
                }]
            }
            
            if context:
                alert_message["embeds"][0]["fields"].append({
                    "name": "ğŸ“‹ ì»¨í…ìŠ¤íŠ¸",
                    "value": f"```json\\n{json.dumps(context, ensure_ascii=False, indent=2)[:500]}```",
                    "inline": False
                })
            
            response = requests.post(
                critical_webhook,
                json=alert_message,
                timeout=10
            )
            
            if response.status_code == 204:
                self.error_stats['critical_alerts_sent'] += 1
                self.error_stats['last_critical_alert'] = datetime.now().isoformat()
                logger.info("ğŸš¨ ì¹˜ëª…ì  ì—ëŸ¬ ì•Œë¦¼ ì „ì†¡ ì„±ê³µ")
            else:
                logger.error(f"ì¹˜ëª…ì  ì—ëŸ¬ ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {response.status_code}")
                
        except Exception as e:
            logger.error(f"ì¹˜ëª…ì  ì—ëŸ¬ ì•Œë¦¼ ì „ì†¡ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _send_high_priority_alert(self, error: Exception, error_type: str, context: Dict = None):
        """ë†’ì€ ìš°ì„ ìˆœìœ„ ì—ëŸ¬ ì•Œë¦¼"""
        try:
            bug_webhook = os.environ.get('DISCORD_WEBHOOK_BUG')
            if not bug_webhook:
                return
            
            alert_message = {
                "username": "Epic7 High Priority Alert",
                "content": f"âš ï¸ **ë†’ì€ ìš°ì„ ìˆœìœ„ ì—ëŸ¬ ë°œìƒ**\\nì—ëŸ¬ ìœ í˜•: {error_type}\\në©”ì‹œì§€: {str(error)[:200]}..."
            }
            
            requests.post(bug_webhook, json=alert_message, timeout=5)
            logger.info("âš ï¸ ë†’ì€ ìš°ì„ ìˆœìœ„ ì—ëŸ¬ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë†’ì€ ìš°ì„ ìˆœìœ„ ì—ëŸ¬ ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {e}")
    
    def _is_alert_cooldown(self) -> bool:
        """ì•Œë¦¼ ì¿¨ë‹¤ìš´ ì²´í¬"""
        if not self.error_stats['last_critical_alert']:
            return False
        
        last_alert = datetime.fromisoformat(self.error_stats['last_critical_alert'])
        return (datetime.now() - last_alert).total_seconds() < self.critical_alert_cooldown
    
    def _get_system_info(self) -> str:
        """ì‹œìŠ¤í…œ ìƒíƒœ ì •ë³´ ìˆ˜ì§‘"""
        try:
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            disk = psutil.disk_usage('/')
            
            return f"CPU: {cpu_percent}%\\nMEM: {memory.percent}%\\nDISK: {disk.percent}%"
        except:
            return "ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨"
    
    def _attempt_recovery(self, error: Exception, error_type: str, recovery_callback: Callable = None) -> bool:
        """ì—ëŸ¬ ë³µêµ¬ ì‹œë„ (v4.5 ì™„ì „ ë³´ì¡´)"""
        try:
            self.error_stats['recovery_attempts'] += 1
            
            strategy = self.recovery_strategies.get(error_type, ErrorRecoveryStrategy.SKIP)
            
            if strategy == ErrorRecoveryStrategy.RETRY:
                return self._retry_recovery(error, error_type, recovery_callback)
            elif strategy == ErrorRecoveryStrategy.FALLBACK:
                return self._fallback_recovery(error, error_type)
            elif strategy == ErrorRecoveryStrategy.SKIP:
                logger.info(f"ì—ëŸ¬ ë³µêµ¬: {error_type} ê±´ë„ˆë›°ê¸°")
                return True
            elif strategy == ErrorRecoveryStrategy.ALERT_AND_CONTINUE:
                logger.warning(f"ì—ëŸ¬ ë³µêµ¬: {error_type} ì•Œë¦¼ í›„ ê³„ì† ì§„í–‰")
                return True
            else:
                logger.error(f"ì—ëŸ¬ ë³µêµ¬: {error_type} ë³µêµ¬ ë¶ˆê°€")
                return False
                
        except Exception as e:
            logger.error(f"ì—ëŸ¬ ë³µêµ¬ ì‹œë„ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def _retry_recovery(self, error: Exception, error_type: str, recovery_callback: Callable = None) -> bool:
        """ì¬ì‹œë„ ë³µêµ¬ ì „ëµ"""
        for attempt in range(self.max_recovery_attempts):
            try:
                logger.info(f"ì—ëŸ¬ ë³µêµ¬ ì¬ì‹œë„ {attempt + 1}/{self.max_recovery_attempts}: {error_type}")
                
                if attempt > 0:
                    wait_time = 2 ** attempt
                    logger.info(f"ì¬ì‹œë„ ì „ ëŒ€ê¸°: {wait_time}ì´ˆ")
                    time.sleep(wait_time)
                
                if error_type == ErrorType.FILE_IO:
                    self._recover_file_io()
                elif error_type == ErrorType.NETWORK:
                    self._recover_network()
                elif error_type == ErrorType.CRAWLING:
                    self._recover_crawling()
                
                if recovery_callback:
                    recovery_callback()
                
                logger.info(f"ì—ëŸ¬ ë³µêµ¬ ì„±ê³µ: {error_type}")
                self.error_stats['recovery_success'] += 1
                return True
                
            except Exception as e:
                logger.warning(f"ì¬ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")
                continue
        
        logger.error(f"ì—ëŸ¬ ë³µêµ¬ ì‹¤íŒ¨: {error_type} (ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨)")
        return False
    
    def _fallback_recovery(self, error: Exception, error_type: str) -> bool:
        """í´ë°± ë³µêµ¬ ì „ëµ"""
        try:
            logger.info(f"í´ë°± ë³µêµ¬ ì‹œë„: {error_type}")
            
            if error_type == ErrorType.MEMORY:
                self._cleanup_memory()
            elif error_type == ErrorType.IMPORT:
                self._use_alternative_import()
            
            self.error_stats['recovery_success'] += 1
            return True
            
        except Exception as e:
            logger.error(f"í´ë°± ë³µêµ¬ ì‹¤íŒ¨: {e}")
            return False
    
    def _recover_file_io(self):
        """íŒŒì¼ I/O ë³µêµ¬"""
        data_files = [
            "epic7_monitor_execution.lock",
            "epic7_monitor_retry_queue.json",
            "daily_sentiment_data.json"
        ]
        
        for file_path in data_files:
            if os.path.exists(file_path):
                os.chmod(file_path, 0o644)
            else:
                with open(file_path, 'w', encoding='utf-8') as f:
                    if file_path.endswith('.json'):
                        json.dump([], f)
                    else:
                        f.write('')
    
    def _recover_network(self):
        """ë„¤íŠ¸ì›Œí¬ ë³µêµ¬"""
        try:
            requests.get('https://www.google.com', timeout=5)
            logger.info("ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë³µêµ¬ í™•ì¸")
        except:
            raise Exception("ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë³µêµ¬ ì‹¤íŒ¨")
    
    def _recover_crawling(self):
        """í¬ë¡¤ë§ ë³µêµ¬"""
        cache_files = ["crawled_links.json", "content_cache.json"]
        for cache_file in cache_files:
            if os.path.exists(cache_file):
                try:
                    with open(cache_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        if len(data) > 500:
                            data = data[-500:]
                            with open(cache_file, 'w', encoding='utf-8') as f:
                                json.dump(data, f, ensure_ascii=False, indent=2)
                except:
                    with open(cache_file, 'w', encoding='utf-8') as f:
                        json.dump([], f)
    
    def _cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        import gc
        gc.collect()
        logger.info("ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
    
    def _use_alternative_import(self):
        """ëŒ€ì•ˆ ì„í¬íŠ¸ ì‚¬ìš©"""
        logger.info("ëŒ€ì•ˆ ì„í¬íŠ¸ ë°©ì‹ ì‚¬ìš©")
    
    def get_error_report(self) -> str:
        """ì—ëŸ¬ í†µê³„ ë³´ê³ ì„œ ìƒì„±"""
        return f"""
âœ¨ Epic7 ì—ëŸ¬ ê´€ë¦¬ ì‹œìŠ¤í…œ v4.6 í†µê³„ ë³´ê³ ì„œ

ğŸ“Š ì „ì²´ í†µê³„:
- ì´ ì—ëŸ¬ ë°œìƒ: {self.error_stats['total_errors']}ê°œ
- ë³µêµ¬ ì‹œë„: {self.error_stats['recovery_attempts']}ê°œ
- ë³µêµ¬ ì„±ê³µ: {self.error_stats['recovery_success']}ê°œ
- ë³µêµ¬ ì„±ê³µë¥ : {(self.error_stats['recovery_success'] / max(1, self.error_stats['recovery_attempts']) * 100):.1f}%

ğŸ”¥ ì—ëŸ¬ ìœ í˜•ë³„ í†µê³„:
{chr(10).join([f"- {error_type}: {count}ê°œ" for error_type, count in self.error_stats['by_type'].items()])}

âš ï¸ ì‹¬ê°ë„ë³„ í†µê³„:
{chr(10).join([f"- {severity}: {count}ê°œ" for severity, count in self.error_stats['by_severity'].items()])}

ğŸš¨ ì¹˜ëª…ì  ì•Œë¦¼:
- ì „ì†¡ëœ ì¹˜ëª…ì  ì•Œë¦¼: {self.error_stats['critical_alerts_sent']}ê°œ
- ë§ˆì§€ë§‰ ì¹˜ëª…ì  ì•Œë¦¼: {self.error_stats['last_critical_alert'] or 'None'}

ì‹œì‘ ì‹œê°„: {self.error_stats['start_time']}
""".strip()

# ì „ì—­ ì—ëŸ¬ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ (v4.5 ì™„ì „ ë³´ì¡´)
error_manager = ErrorManager()

# =============================================================================
# ì‹¤í–‰ ìƒíƒœ ê´€ë¦¬ (v4.5 ì™„ì „ ë³´ì¡´)
# =============================================================================

EXECUTION_LOCK_FILE = "epic7_monitor_execution.lock"
RETRY_QUEUE_FILE = "epic7_monitor_retry_queue.json"

MAX_RETRY_QUEUE_SIZE = 1000
RETRY_QUEUE_CLEANUP_THRESHOLD = 800
RETRY_QUEUE_CLEANUP_HOURS = 24

class ExecutionManager:
    """ì‹¤í–‰ ìƒíƒœ ê´€ë¦¬ì (v4.5 ì™„ì „ ë³´ì¡´)"""
    
    @staticmethod
    def is_running() -> bool:
        """ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸"""
        if not os.path.exists(EXECUTION_LOCK_FILE):
            return False
        
        try:
            with open(EXECUTION_LOCK_FILE, 'r') as f:
                lock_data = json.load(f)
                start_time = datetime.fromisoformat(lock_data['start_time'])
                
                if datetime.now() - start_time > timedelta(hours=2):
                    logger.warning("ì‹¤í–‰ ë½ì´ 2ì‹œê°„ ì´ìƒ ìœ ì§€ë¨ - ë¹„ì •ìƒ ì¢…ë£Œë¡œ ê°„ì£¼í•˜ì—¬ ë½ í•´ì œ")
                    ExecutionManager.release_lock()
                    return False
                
                return True
        except Exception as e:
            error_manager.handle_error(e, ErrorType.FILE_IO, ErrorSeverity.MEDIUM, 
                                     {'function': 'ExecutionManager.is_running'})
            return False
    
    @staticmethod
    def acquire_lock() -> bool:
        """ì‹¤í–‰ ë½ íšë“"""
        try:
            if ExecutionManager.is_running():
                return False
            
            lock_data = {
                'start_time': datetime.now().isoformat(),
                'pid': os.getpid()
            }
            
            with open(EXECUTION_LOCK_FILE, 'w') as f:
                json.dump(lock_data, f, indent=2)
            
            logger.info("ì‹¤í–‰ ë½ íšë“ ì„±ê³µ")
            return True
        except Exception as e:
            error_manager.handle_error(e, ErrorType.FILE_IO, ErrorSeverity.HIGH, 
                                     {'function': 'ExecutionManager.acquire_lock'})
            return False
    
    @staticmethod
    def release_lock():
        """ì‹¤í–‰ ë½ í•´ì œ"""
        try:
            if os.path.exists(EXECUTION_LOCK_FILE):
                os.remove(EXECUTION_LOCK_FILE)
                logger.info("ì‹¤í–‰ ë½ í•´ì œ ì™„ë£Œ")
        except Exception as e:
            error_manager.handle_error(e, ErrorType.FILE_IO, ErrorSeverity.MEDIUM, 
                                     {'function': 'ExecutionManager.release_lock'})

# =============================================================================
# âœ¨ NEW v4.6: ì¸ì íŒŒì‹± - --mode íŒŒë¼ë¯¸í„° ì¶”ê°€
# =============================================================================

def parse_arguments() -> argparse.Namespace:
    """âœ¨ v4.6: --mode íŒŒë¼ë¯¸í„° ì¶”ê°€ëœ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(
        description="Epic7 í†µí•© ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ v4.6",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # í•œêµ­ ì‚¬ì´íŠ¸ë§Œ 15ë¶„ ì£¼ê¸° ëª¨ë‹ˆí„°ë§
  python monitor_bugs.py --schedule 15min --mode korea
  
  # ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ë§Œ 15ë¶„ ì£¼ê¸° ëª¨ë‹ˆí„°ë§
  python monitor_bugs.py --schedule 15min --mode global
  
  # ëª¨ë“  ì‚¬ì´íŠ¸ 15ë¶„ ì£¼ê¸° ëª¨ë‹ˆí„°ë§ (ê¸°ë³¸ê°’)
  python monitor_bugs.py --schedule 15min --mode all
  
  # ê¸°ì¡´ 30ë¶„ ì£¼ê¸° í†µí•© ëª¨ë‹ˆí„°ë§ (í•˜ìœ„ í˜¸í™˜ì„±)
  python monitor_bugs.py --schedule 30min
  
  # 24ì‹œê°„ ì¼ê°„ ë¦¬í¬íŠ¸
  python monitor_bugs.py --schedule 24h
        """
    )
    
    # ê¸°ì¡´ íŒŒë¼ë¯¸í„°ë“¤ (v4.5 ì™„ì „ ë³´ì¡´)
    parser.add_argument(
        '--schedule', 
        choices=['15min', '30min', '24h'], 
        default='15min',
        help='ì‹¤í–‰ ìŠ¤ì¼€ì¤„ (15min: 15ë¶„ ì£¼ê¸°, 30min: 30ë¶„ ì£¼ê¸°, 24h: 24ì‹œê°„ ë¦¬í¬íŠ¸)'
    )
    
    parser.add_argument(
        '--debug', 
        action='store_true',
        help='ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”'
    )
    
    parser.add_argument(
        '--force-crawl', 
        action='store_true',
        help='ê°•ì œ í¬ë¡¤ë§ (ìºì‹œ ë¬´ì‹œ)'
    )
    
    # âœ¨ NEW v4.6: --mode íŒŒë¼ë¯¸í„° ì¶”ê°€
    parser.add_argument(
        '--mode', 
        choices=['korea', 'global', 'all'], 
        default='all',
        help='í¬ë¡¤ë§ ëª¨ë“œ (korea: í•œêµ­ ì‚¬ì´íŠ¸ë§Œ, global: ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ë§Œ, all: ëª¨ë“  ì‚¬ì´íŠ¸)'
    )
    
    return parser.parse_args()

# =============================================================================
# Epic7 í†µí•© ëª¨ë‹ˆí„° v4.6 - Mode ë¶„ë¦¬ ì™„ì„±ë³¸
# =============================================================================

class Epic7Monitor:
    """Epic7 í†µí•© ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ v4.6 - Mode ë¶„ë¦¬ ì™„ì„±ë³¸"""
    
    def __init__(self, mode: str = "all", schedule: str = "15min", debug: bool = False, force_crawl: bool = False):
        """ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (v4.6: mode íŒŒë¼ë¯¸í„° ì¶”ê°€)"""
        self.mode = mode  # âœ¨ NEW v4.6
        self.schedule = schedule
        self.debug = debug
        self.force_crawl = force_crawl
        self.start_time = datetime.now()
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        if CLASSIFIER_AVAILABLE:
            self.classifier = Epic7Classifier()
        else:
            self.classifier = Epic7Classifier()  # í´ë°± í´ë˜ìŠ¤ ì‚¬ìš©
        self.error_manager = error_manager
        
        # í†µê³„ ì´ˆê¸°í™” (v4.5 ê¸°ì¡´ + v4.6 í™•ì¥)
        self.stats = {
            'total_crawled': 0,
            'new_posts': 0,
            'bug_posts': 0,
            'sentiment_posts': 0,
            'immediate_bug_alerts': 0,
            'immediate_sentiment_alerts': 0,
            'processed_posts': 0,
            'failed_posts': 0,
            'retry_processed': 0,
            'sentiment_save_success': 0,
            'sentiment_save_failed': 0,
            'errors': 0,
            'error_recoveries': 0,
            'critical_alerts': 0,
            'high_priority_alerts': 0,
            # âœ¨ NEW v4.6: ëª¨ë“œë³„ í†µê³„
            'mode': mode,
            'korea_sites_crawled': 0,
            'global_sites_crawled': 0,
            'schedule': schedule,
            'debug': debug,
            'force_crawl': force_crawl,
            'start_time': self.start_time.isoformat()
        }
        
        # ì›¹í›… í™•ì¸
        self.webhooks = self._check_discord_webhooks()
        
        # ë””ë²„ê·¸ ì„¤ì •
        if debug:
            logging.getLogger().setLevel(logging.DEBUG)
        
        logger.info(f"Epic7 ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ v4.6 ì´ˆê¸°í™” ì™„ë£Œ - Mode ë¶„ë¦¬ (ëª¨ë“œ: {mode}, ìŠ¤ì¼€ì¤„: {schedule})")
    
    def _check_discord_webhooks(self) -> Dict[str, str]:
        """Discord ì›¹í›… í™˜ê²½ë³€ìˆ˜ í™•ì¸ (v4.5 ì™„ì „ ë³´ì¡´)"""
        webhooks = {}
        
        try:
            bug_webhook = os.environ.get('DISCORD_WEBHOOK_BUG')
            if bug_webhook:
                webhooks['bug'] = bug_webhook
                logger.info("Discord ë²„ê·¸ ì•Œë¦¼ ì›¹í›… í™•ì¸ë¨")
            
            sentiment_webhook = os.environ.get('DISCORD_WEBHOOK_SENTIMENT')
            if sentiment_webhook:
                webhooks['sentiment'] = sentiment_webhook
                logger.info("Discord ê°ì„± ì•Œë¦¼ ì›¹í›… í™•ì¸ë¨")
            
            report_webhook = os.environ.get('DISCORD_WEBHOOK_REPORT')
            if report_webhook:
                webhooks['report'] = report_webhook
                logger.info("Discord ë¦¬í¬íŠ¸ ì›¹í›… í™•ì¸ë¨")
            
            critical_webhook = os.environ.get('DISCORD_WEBHOOK_CRITICAL_ERROR')
            if critical_webhook:
                webhooks['critical'] = critical_webhook
                logger.info("Discord ì¹˜ëª…ì  ì—ëŸ¬ ì›¹í›… í™•ì¸ë¨")
            else:
                logger.warning("Discord ì¹˜ëª…ì  ì—ëŸ¬ ì›¹í›…ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (DISCORD_WEBHOOK_CRITICAL_ERROR)")
            
            if not webhooks:
                logger.warning("Discord ì›¹í›… í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            return webhooks
            
        except Exception as e:
            self.error_manager.handle_error(e, ErrorType.IMPORT, ErrorSeverity.MEDIUM, 
                                          {'function': '_check_discord_webhooks'})
            return {}

    def _crawl_site(self, site: str) -> List[Dict]:
        """âœ¨ v4.6: ê°œë³„ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ (ì‚¬ì´íŠ¸ë³„ í•¨ìˆ˜ í˜¸ì¶œ)"""
        try:
            if not CRAWLER_AVAILABLE:
                logger.warning(f"crawler ëª¨ë“ˆ ì‚¬ìš© ë¶ˆê°€ - {site} ê±´ë„ˆë›°ê¸°")
                return []
                
            # ì‚¬ì´íŠ¸ë³„ í¬ë¡¤ë§ í•¨ìˆ˜ ë§¤í•‘
            site_crawlers = {
                'stove_korea_bug': lambda: crawl_by_schedule('stove_korea_bug', False, 'korea'),
                'stove_korea_general': lambda: crawl_by_schedule('stove_korea_general', False, 'korea'),
                'stove_global_bug': lambda: crawl_by_schedule('stove_global_bug', False, 'global'),
                'stove_global_general': lambda: crawl_by_schedule('stove_global_general', False, 'global'),
                'ruliweb_epic7': lambda: crawl_by_schedule('ruliweb_epic7', False, 'korea'),
                'reddit_epicseven': lambda: crawl_by_schedule('reddit_epicseven', False, 'global')
            }
        
            if site not in site_crawlers:
                logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸: {site}")
                return []
        
            # í¬ë¡¤ë§ ì‹¤í–‰
            crawler_func = site_crawlers[site]
            posts = crawler_func()
        
            return posts if posts else []
        
        except Exception as e:
            self.error_manager.handle_error(e, ErrorType.CRAWLING, ErrorSeverity.MEDIUM, 
                                          {'site': site})
            return []

    # =============================================================================
    # âœ¨ NEW v4.6: 15ë¶„ ì£¼ê¸° ëª¨ë“œë³„ ë¶„ë¦¬ ë¡œì§
    # =============================================================================
    
    def run_15min_crawling_and_bug_alert(self) -> bool:
        """âœ¨ v4.6: 15ë¶„ ì£¼ê¸° í¬ë¡¤ë§ ë° ë²„ê·¸ ì•Œë¦¼ (ëª¨ë“œë³„ ë¶„ë¦¬)"""
        try:
            logger.info(f"15ë¶„ ì£¼ê¸° í¬ë¡¤ë§ ì‹œì‘ - ëª¨ë“œ: {self.mode}")
            
            if self.mode == 'korea':
                return self._crawl_korea_sites_only()
            elif self.mode == 'global':
                return self._crawl_global_sites_only()
            elif self.mode == 'all':
                return self._crawl_all_sites()
            else:
                logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë“œ: {self.mode}")
                return False
                
        except Exception as e:
            self.error_manager.handle_error(e, ErrorType.CRITICAL, ErrorSeverity.HIGH, 
                                          {'function': 'run_15min_crawling_and_bug_alert', 'mode': self.mode})
            return False
    
    def _crawl_korea_sites_only(self) -> bool:
        """âœ¨ v4.6: í•œêµ­ ì‚¬ì´íŠ¸ë§Œ í¬ë¡¤ë§"""
        try:
            logger.info("ğŸ‡°ğŸ‡· í•œêµ­ ì‚¬ì´íŠ¸ ì „ìš© í¬ë¡¤ë§ ì‹œì‘")
            
            # í•œêµ­ ì‚¬ì´íŠ¸ ëª©ë¡
            korea_sites = [
                'stove_korea_bug',      # ìŠ¤í† ë¸Œ í•œêµ­ ë²„ê·¸ ê²Œì‹œíŒ
                'stove_korea_general',  # ìŠ¤í† ë¸Œ í•œêµ­ ììœ  ê²Œì‹œíŒ
                'ruliweb_epic7'         # ë£¨ë¦¬ì›¹ ì—í”½ì„¸ë¸
            ]
            
            total_success = True
            
            for site in korea_sites:
                try:
                    logger.info(f"ğŸ•·ï¸ {site} í¬ë¡¤ë§ ì‹œì‘")
                    
                    # ì‚¬ì´íŠ¸ë³„ í¬ë¡¤ë§ ì‹¤í–‰
                    posts = self._crawl_site(site)
                    
                    if posts:
                        # ê²Œì‹œê¸€ë³„ ì¦‰ì‹œ ì²˜ë¦¬
                        for post in posts:
                            self.process_post_immediately(post)
                        
                        self.stats['korea_sites_crawled'] += len(posts)
                        logger.info(f"âœ… {site} í¬ë¡¤ë§ ì™„ë£Œ: {len(posts)}ê°œ ê²Œì‹œê¸€")
                    else:
                        logger.info(f"ğŸ“­ {site} ìƒˆë¡œìš´ ê²Œì‹œê¸€ ì—†ìŒ")
                        
                except Exception as e:
                    self.error_manager.handle_error(e, ErrorType.CRAWLING, ErrorSeverity.MEDIUM, 
                                                  {'site': site, 'mode': 'korea'})
                    total_success = False
                    continue
            
            logger.info(f"ğŸ‡°ğŸ‡· í•œêµ­ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì™„ë£Œ - ì´ {self.stats['korea_sites_crawled']}ê°œ ê²Œì‹œê¸€")
            return total_success
            
        except Exception as e:
            self.error_manager.handle_error(e, ErrorType.CRAWLING, ErrorSeverity.HIGH, 
                                          {'function': '_crawl_korea_sites_only'})
            return False
    
    def _crawl_global_sites_only(self) -> bool:
        """âœ¨ v4.6: ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ë§Œ í¬ë¡¤ë§"""
        try:
            logger.info("ğŸŒ ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ ì „ìš© í¬ë¡¤ë§ ì‹œì‘")
            
            # ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ ëª©ë¡
            global_sites = [
                'stove_global_bug',      # ìŠ¤í† ë¸Œ ê¸€ë¡œë²Œ ë²„ê·¸ ê²Œì‹œíŒ
                'stove_global_general',  # ìŠ¤í† ë¸Œ ê¸€ë¡œë²Œ ììœ  ê²Œì‹œíŒ
                'reddit_epicseven'       # Reddit r/EpicSeven
            ]
            
            total_success = True
            
            for site in global_sites:
                try:
                    logger.info(f"ğŸ•·ï¸ {site} í¬ë¡¤ë§ ì‹œì‘")
                    
                    # ì‚¬ì´íŠ¸ë³„ í¬ë¡¤ë§ ì‹¤í–‰
                    posts = self._crawl_site(site)
                    
                    if posts:
                        # ê²Œì‹œê¸€ë³„ ì¦‰ì‹œ ì²˜ë¦¬
                        for post in posts:
                            self.process_post_immediately(post)
                        
                        self.stats['global_sites_crawled'] += len(posts)
                        logger.info(f"âœ… {site} í¬ë¡¤ë§ ì™„ë£Œ: {len(posts)}ê°œ ê²Œì‹œê¸€")
                    else:
                        logger.info(f"ğŸ“­ {site} ìƒˆë¡œìš´ ê²Œì‹œê¸€ ì—†ìŒ")
                        
                except Exception as e:
                    self.error_manager.handle_error(e, ErrorType.CRAWLING, ErrorSeverity.MEDIUM, 
                                                  {'site': site, 'mode': 'global'})
                    total_success = False
                    continue
            
            logger.info(f"ğŸŒ ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì™„ë£Œ - ì´ {self.stats['global_sites_crawled']}ê°œ ê²Œì‹œê¸€")
            return total_success
            
        except Exception as e:
            self.error_manager.handle_error(e, ErrorType.CRAWLING, ErrorSeverity.HIGH, 
                                          {'function': '_crawl_global_sites_only'})
            return False
    
    def _crawl_all_sites(self) -> bool:
        """âœ¨ v4.6: ëª¨ë“  ì‚¬ì´íŠ¸ í¬ë¡¤ë§ (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)"""
        try:
            logger.info("ğŸŒ ì „ì²´ ì‚¬ì´íŠ¸ í†µí•© í¬ë¡¤ë§ ì‹œì‘")
            
            # í•œêµ­ + ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ ìˆœì°¨ ì‹¤í–‰
            korea_success = self._crawl_korea_sites_only()
            global_success = self._crawl_global_sites_only()
            
            total_posts = self.stats['korea_sites_crawled'] + self.stats['global_sites_crawled']
            logger.info(f"ğŸŒ ì „ì²´ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì™„ë£Œ - ì´ {total_posts}ê°œ ê²Œì‹œê¸€")
            
            return korea_success and global_success
            
        except Exception as e:
            self.error_manager.handle_error(e, ErrorType.CRAWLING, ErrorSeverity.HIGH, 
                                          {'function': '_crawl_all_sites'})
            return False
    
    # =============================================================================
    # ê¸°ì¡´ v4.5 ê¸°ëŠ¥ë“¤ ì™„ì „ ë³´ì¡´
    # =============================================================================
    
    def process_post_immediately(self, post_data: Dict) -> bool:
        """ê²Œì‹œê¸€ë³„ ì¦‰ì‹œ ì²˜ë¦¬ ì½œë°± í•¨ìˆ˜ - v4.5 ì™„ì „ ë³´ì¡´ + ğŸ”§ ìˆ˜ì • 2: ë°ì´í„° ê²€ì¦ ê°•í™”"""
        try:
            self.stats['total_crawled'] += 1
            
            # ğŸ”§ ìˆ˜ì • 2: ê°•í™”ëœ ë°ì´í„° ìœ íš¨ì„± ê²€ì¦
            if not post_data or not isinstance(post_data, dict):
                raise ValueError("ìœ íš¨í•˜ì§€ ì•Šì€ post_data êµ¬ì¡°")
            
            # í•„ìˆ˜ í•„ë“œ ì¡´ì¬ í™•ì¸
            required_fields = ['title', 'content', 'url', 'source']
            for field in required_fields:
                if field not in post_data:
                    logger.warning(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}, ê¸°ë³¸ê°’ ì„¤ì •")
                    post_data[field] = ''
            
            # íƒ€ì… ê²€ì¦ ë° ì•ˆì „í™”
            title = str(post_data.get('title', '')).strip()
            content = str(post_data.get('content', '')).strip()
            url = str(post_data.get('url', '')).strip()
            source = str(post_data.get('source', '')).strip()
            
            if not title and not content:
                raise ValueError("ì œëª©ê³¼ ë‚´ìš©ì´ ëª¨ë‘ ë¹„ì–´ìˆëŠ” ê²Œì‹œê¸€")
            
            # 1. ê°ì„± ë¶„ì„
            logger.info(f"[IMMEDIATE] ì¦‰ì‹œ ì²˜ë¦¬ ì‹œì‘: {title[:50]}...")
            
            try:
                classification = self.classifier.classify_post(post_data)
                post_data['classification'] = classification
            except Exception as e:
                recovery_success = self.error_manager.handle_error(
                    e, ErrorType.CLASSIFICATION, ErrorSeverity.MEDIUM, 
                    {'post_title': title[:50], 'post_url': url}
                )
                if not recovery_success:
                    raise Exception(f"ê°ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            
            # 2. ì•Œë¦¼ ì „ì†¡
            category = classification.get('category', 'neutral')
            
            if source.endswith('_bug') or category == 'bug' or classification.get('realtime_alert', {}).get('should_alert', False):
                # ë²„ê·¸ ì•Œë¦¼
                success = self._send_immediate_bug_alert(post_data)
                if success:
                    self.stats['immediate_bug_alerts'] += 1
                    self.stats['bug_posts'] += 1
                    logger.info(f"ğŸš¨ ì¦‰ì‹œ ë²„ê·¸ ì•Œë¦¼ ì „ì†¡ ì„±ê³µ: {title[:30]}...")
                else:
                    raise Exception("ë²„ê·¸ ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨")
            else:
                # ê°ì„± ì•Œë¦¼
                success = self._send_immediate_sentiment_alert(post_data)
                if success:
                    self.stats['immediate_sentiment_alerts'] += 1
                    self.stats['sentiment_posts'] += 1
                    logger.info(f"ğŸ“Š ì¦‰ì‹œ ê°ì„± ì•Œë¦¼ ì „ì†¡ ì„±ê³µ: {title[:30]}...")
                else:
                    raise Exception("ê°ì„± ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨")
            
            # 3. ê°ì„± ë°ì´í„° ì €ì¥
            sentiment_save_success = self._save_sentiment_for_daily_report(post_data, classification)
            if sentiment_save_success:
                self.stats['sentiment_save_success'] += 1
            else:
                self.stats['sentiment_save_failed'] += 1
                logger.warning(f"ê°ì„± ë°ì´í„° ì €ì¥ ì‹¤íŒ¨í•˜ì˜€ì§€ë§Œ ì²˜ë¦¬ ê³„ì†: {title[:30]}...")
            
            # 4. ì²˜ë¦¬ ì™„ë£Œ ë§ˆí‚¹
            try:
                mark_as_processed(url, notified=True)
            except Exception as e:
                self.error_manager.handle_error(e, ErrorType.FILE_IO, ErrorSeverity.LOW, 
                                              {'url': url})
            
            self.stats['processed_posts'] += 1
            logger.info(f"âœ… [SUCCESS] ì¦‰ì‹œ ì²˜ë¦¬ ì™„ë£Œ: {title[:30]}...")
            return True
            
        except ValueError as e:
            self.error_manager.handle_error(e, ErrorType.DATA_PARSING, ErrorSeverity.LOW, 
                                          {'post_data': str(post_data)[:200]})
            self.stats['failed_posts'] += 1
            return False
        
        except (IOError, OSError) as e:
            recovery_success = self.error_manager.handle_error(
                e, ErrorType.FILE_IO, ErrorSeverity.MEDIUM, 
                {'post_title': post_data.get('title', 'N/A')[:50]},
                recovery_callback=lambda: self._recreate_data_files()
            )
            if recovery_success:
                self.stats['error_recoveries'] += 1
                return self.process_post_immediately(post_data)
            else:
                self.stats['failed_posts'] += 1
                return False
        
        except (requests.exceptions.RequestException, requests.exceptions.Timeout) as e:
            recovery_success = self.error_manager.handle_error(
                e, ErrorType.NETWORK, ErrorSeverity.MEDIUM, 
                {'post_title': post_data.get('title', 'N/A')[:50]}
            )
            if recovery_success:
                self.stats['error_recoveries'] += 1
                return self.process_post_immediately(post_data)
            else:
                self.stats['failed_posts'] += 1
                return False
        
        except Exception as e:
            self.error_manager.handle_error(e, ErrorType.CRITICAL, ErrorSeverity.HIGH, 
                                          {'function': 'process_post_immediately',
                                           'post_title': post_data.get('title', 'N/A')[:50]})
            
            self.stats['failed_posts'] += 1
            self.stats['errors'] += 1
            return False
    
    def _send_immediate_bug_alert(self, post_data: Dict) -> bool:
        """ì¦‰ì‹œ ë²„ê·¸ ì•Œë¦¼ ì „ì†¡ - v4.5 ì™„ì „ ë³´ì¡´"""
        try:
            if not self.webhooks.get('bug'):
                raise Exception("ë²„ê·¸ ì•Œë¦¼ ì›¹í›…ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            success = send_bug_alert([post_data])
            return success
            
        except Exception as e:
            self.error_manager.handle_error(e, ErrorType.NOTIFICATION, ErrorSeverity.HIGH, 
                                          {'alert_type': 'bug', 'post_title': post_data.get('title', '')[:50]})
            return False
    
    def _send_immediate_sentiment_alert(self, post_data: Dict) -> bool:
        """ì¦‰ì‹œ ê°ì„± ì•Œë¦¼ ì „ì†¡ - v4.5 ì™„ì „ ë³´ì¡´"""
        try:
            if not self.webhooks.get('sentiment'):
                raise Exception("ê°ì„± ì•Œë¦¼ ì›¹í›…ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            classification = post_data.get('classification', {})
            sentiment = classification.get('sentiment_analysis', {}).get('sentiment', 'neutral')
            
            sentiment_summary = {
                'total_posts': 1,
                'sentiment_distribution': {sentiment: 1},
                'time_period': 'ì¦‰ì‹œ ì²˜ë¦¬',
                'timestamp': datetime.now().isoformat()
            }
            
            success = send_sentiment_notification([post_data], sentiment_summary)
            return success
            
        except Exception as e:
            self.error_manager.handle_error(e, ErrorType.NOTIFICATION, ErrorSeverity.HIGH, 
                                          {'alert_type': 'sentiment', 'post_title': post_data.get('title', '')[:50]})
            return False
    
    def _save_sentiment_for_daily_report(self, post_data: Dict, classification: Dict) -> bool:
        """ì¼ê°„ ë¦¬í¬íŠ¸ìš© ê°ì„± ë°ì´í„° ì €ì¥ - v4.5 ì™„ì „ ë³´ì¡´ + ğŸ”§ ìˆ˜ì • 4: json íŒŒì¼ ì²˜ë¦¬ ì•ˆì „í™”"""
        try:
            # ğŸ”§ ìˆ˜ì • 4: json íŒŒì¼ ì²˜ë¦¬ ì•ˆì „í™”
            try:
                from sentiment_data_manager import save_sentiment_data_immediately
                
                sentiment_data = {
                    'title': post_data.get('title', ''),
                    'content': post_data.get('content', '')[:200],
                    'url': post_data.get('url', ''),
                    'source': post_data.get('source', ''),
                    'sentiment': classification.get('sentiment_analysis', {}).get('sentiment', 'neutral'),
                    'confidence': classification.get('sentiment_analysis', {}).get('confidence', 0.0),
                    'category': classification.get('category', 'neutral'),
                    'timestamp': datetime.now().isoformat()
                }
                
                success = save_sentiment_data_immediately(sentiment_data)
                if success:
                    logger.debug(f"ê°ì„± ë°ì´í„° ì €ì¥ ì„±ê³µ: {sentiment_data['title'][:30]}...")
                    return True
                else:
                    raise Exception("sentiment_data_manager ì €ì¥ ì‹¤íŒ¨")
                    
            except ImportError:
                # í´ë°±: ì§ì ‘ JSON íŒŒì¼ ì²˜ë¦¬ (ì•ˆì „í™”)
                return self._save_sentiment_direct(post_data, classification)
                
        except Exception as e:
            self.error_manager.handle_error(e, ErrorType.FILE_IO, ErrorSeverity.MEDIUM, 
                                          {'function': '_save_sentiment_for_daily_report',
                                           'post_title': post_data.get('title', '')[:30]})
            return False
    
    def _save_sentiment_direct(self, post_data: Dict, classification: Dict) -> bool:
        """ê°ì„± ë°ì´í„° ì§ì ‘ ì €ì¥ (í´ë°±) - ğŸ”§ ìˆ˜ì • 4: json íŒŒì¼ ì²˜ë¦¬ ì•ˆì „í™” ì ìš©"""
        try:
            sentiment_file = "daily_sentiment_data.json"
            
            # ğŸ”§ ìˆ˜ì • 4: ì•ˆì „í•œ json íŒŒì¼ ì²˜ë¦¬
            try:
                if os.path.exists(sentiment_file):
                    with open(sentiment_file, 'r', encoding='utf-8') as f:
                        content = f.read().strip()
                        if not content:
                            sentiment_data = []
                        else:
                            sentiment_data = json.loads(content)
                            
                    # ë°ì´í„° íƒ€ì… ê²€ì¦
                    if not isinstance(sentiment_data, list):
                        logger.warning("sentiment_data.jsonì´ ë°°ì—´ì´ ì•„ë‹˜ - ìƒˆë¡œ ìƒì„±")
                        sentiment_data = []
                else:
                    sentiment_data = []
            except (json.JSONDecodeError, ValueError) as e:
                logger.warning(f"sentiment_data.json íŒŒì‹± ì‹¤íŒ¨ - ìƒˆë¡œ ìƒì„±: {e}")
                sentiment_data = []
            except Exception as e:
                logger.error(f"sentiment_data.json ì½ê¸° ì‹¤íŒ¨: {e}")
                return False
                
            # ìƒˆë¡œìš´ ë°ì´í„° ì¶”ê°€
            new_entry = {
                'title': post_data.get('title', ''),
                'content': post_data.get('content', '')[:200],
                'url': post_data.get('url', ''),
                'source': post_data.get('source', ''),
                'sentiment': classification.get('sentiment_analysis', {}).get('sentiment', 'neutral'),
                'confidence': classification.get('sentiment_analysis', {}).get('confidence', 0.0),
                'category': classification.get('category', 'neutral'),
                'timestamp': datetime.now().isoformat(),
                'saved_at': datetime.now().isoformat()
            }
            
            sentiment_data.append(new_entry)
            
            # 24ì‹œê°„ ì´ì „ ë°ì´í„° ì •ë¦¬
            cutoff_time = datetime.now() - timedelta(hours=24)
            sentiment_data = [
                entry for entry in sentiment_data
                if datetime.fromisoformat(entry.get('saved_at', entry.get('timestamp', datetime.now().isoformat()))) > cutoff_time
            ]
            
            # íŒŒì¼ì— ì•ˆì „í•˜ê²Œ ì €ì¥
            try:
                with open(sentiment_file, 'w', encoding='utf-8') as f:
                    json.dump(sentiment_data, f, ensure_ascii=False, indent=2)
                    
                logger.debug(f"ê°ì„± ë°ì´í„° ì§ì ‘ ì €ì¥ ì„±ê³µ: {new_entry['title'][:30]}...")
                return True
                
            except Exception as e:
                logger.error(f"sentiment_data.json ì“°ê¸° ì‹¤íŒ¨: {e}")
                return False
                
        except Exception as e:
            logger.error(f"ê°ì„± ë°ì´í„° ì§ì ‘ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def _recreate_data_files(self):
        """ë°ì´í„° íŒŒì¼ ì¬ìƒì„± - v4.5 ì™„ì „ ë³´ì¡´"""
        try:
            # í•„ìˆ˜ ë°ì´í„° íŒŒì¼ë“¤ ì¬ìƒì„±
            essential_files = [
                ("daily_sentiment_data.json", []),
                ("epic7_monitor_retry_queue.json", []),
                ("crawled_links.json", {}),
                ("content_cache.json", {})
            ]
            
            for file_path, default_data in essential_files:
                if not os.path.exists(file_path):
                    with open(file_path, 'w', encoding='utf-8') as f:
                        json.dump(default_data, f, ensure_ascii=False, indent=2)
                    logger.info(f"ë°ì´í„° íŒŒì¼ ì¬ìƒì„±: {file_path}")
                    
        except Exception as e:
            logger.error(f"ë°ì´í„° íŒŒì¼ ì¬ìƒì„± ì‹¤íŒ¨: {e}")
    
    def run_30min_sentiment_notification(self) -> bool:
        """30ë¶„ ì£¼ê¸° ê°ì„± ë™í–¥ ì•Œë¦¼ - v4.5 ì™„ì „ ë³´ì¡´"""
        try:
            logger.info("ğŸ“Š 30ë¶„ ì£¼ê¸° ê°ì„± ë™í–¥ ì•Œë¦¼ ì‹œì‘")
            
            if not self.webhooks.get('sentiment'):
                logger.warning("ê°ì„± ì•Œë¦¼ ì›¹í›…ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return False
            
            # 30ë¶„ê°„ ê°ì„± ë°ì´í„° ìˆ˜ì§‘
            sentiment_summary = self._get_30min_sentiment_summary()
            
            if not sentiment_summary or sentiment_summary.get('total_posts', 0) == 0:
                logger.info("ğŸ“­ 30ë¶„ê°„ ê°ì„± ë¶„ì„í•  ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤")
                return True
            
            # ê°ì„± ë™í–¥ ì•Œë¦¼ ì „ì†¡
            success = send_sentiment_notification([], sentiment_summary)
            
            if success:
                logger.info(f"ğŸ“Š 30ë¶„ ì£¼ê¸° ê°ì„± ë™í–¥ ì•Œë¦¼ ì „ì†¡ ì„±ê³µ: {sentiment_summary.get('total_posts', 0)}ê°œ ê²Œì‹œê¸€")
                return True
            else:
                raise Exception("30ë¶„ ì£¼ê¸° ê°ì„± ë™í–¥ ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨")
            
        except Exception as e:
            self.error_manager.handle_error(e, ErrorType.NOTIFICATION, ErrorSeverity.HIGH, 
                                          {'function': 'run_30min_sentiment_notification'})
            return False
    
    def _get_30min_sentiment_summary(self) -> Dict:
        """30ë¶„ê°„ ê°ì„± ìš”ì•½ ë°ì´í„° ìƒì„± - v4.5 ì™„ì „ ë³´ì¡´ + ğŸ”§ ìˆ˜ì • 4: json ì²˜ë¦¬ ì•ˆì „í™”"""
        try:
            sentiment_file = "daily_sentiment_data.json"
            
            # ğŸ”§ ìˆ˜ì • 4: ì•ˆì „í•œ json íŒŒì¼ ì²˜ë¦¬
            try:
                if not os.path.exists(sentiment_file):
                    return {'total_posts': 0}
                    
                with open(sentiment_file, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
                    if not content:
                        return {'total_posts': 0}
                    sentiment_data = json.loads(content)
                    
                if not isinstance(sentiment_data, list):
                    logger.warning("sentiment_data.jsonì´ ë°°ì—´ì´ ì•„ë‹˜")
                    return {'total_posts': 0}
                    
            except (json.JSONDecodeError, ValueError) as e:
                logger.warning(f"sentiment_data.json íŒŒì‹± ì‹¤íŒ¨: {e}")
                return {'total_posts': 0}
            except Exception as e:
                logger.error(f"sentiment_data.json ì½ê¸° ì‹¤íŒ¨: {e}")
                return {'total_posts': 0}
            
            # 30ë¶„ ì´ì „ ë°ì´í„° í•„í„°ë§
            cutoff_time = datetime.now() - timedelta(minutes=30)
            recent_data = [
                entry for entry in sentiment_data
                if datetime.fromisoformat(entry.get('timestamp', datetime.now().isoformat())) > cutoff_time
            ]
            
            if not recent_data:
                return {'total_posts': 0}
            
            # ê°ì„±ë³„ í†µê³„ ê³„ì‚°
            sentiment_counts = {'positive': 0, 'negative': 0, 'neutral': 0}
            
            for entry in recent_data:
                sentiment = entry.get('sentiment', 'neutral')
                if sentiment in sentiment_counts:
                    sentiment_counts[sentiment] += 1
            
            return {
                'total_posts': len(recent_data),
                'sentiment_distribution': sentiment_counts,
                'time_period': 'ìµœê·¼ 30ë¶„ê°„',
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"30ë¶„ ê°ì„± ìš”ì•½ ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
            return {'total_posts': 0}
    
    def run_24h_daily_report(self) -> bool:
        """24ì‹œê°„ ì¼ê°„ ë¦¬í¬íŠ¸ - v4.5 ì™„ì „ ë³´ì¡´"""
        try:
            logger.info("ğŸ“ˆ 24ì‹œê°„ ì¼ê°„ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘")
            
            if not self.webhooks.get('report'):
                logger.warning("ë¦¬í¬íŠ¸ ì›¹í›…ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return False
            
            # 24ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘
            if not CRAWLER_AVAILABLE:
                logger.warning("crawler ëª¨ë“ˆ ì‚¬ìš© ë¶ˆê°€ - ë¦¬í¬íŠ¸ ë°ì´í„° ìˆ˜ì§‘ ì œí•œ")
                all_posts = []
            else:
                try:
                    all_posts = get_all_posts_for_report()
                except Exception as e:
                    logger.warning(f"ë¦¬í¬íŠ¸ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ - ë¹ˆ ë¦¬í¬íŠ¸ ìƒì„±: {e}")
                    all_posts = []
            
            # ë¦¬í¬íŠ¸ ë°ì´í„° êµ¬ì„±
            report_data = {
                'date': datetime.now().strftime('%Y-%m-%d'),
                'total_posts': len(all_posts) if all_posts else 0,
                'bug_posts': len([p for p in all_posts if p.get('source', '').endswith('_bug')]) if all_posts else 0,
                'sentiment_summary': self._get_24h_sentiment_summary(),
                'top_keywords': self._extract_top_keywords(all_posts) if all_posts else [],
                'system_stats': self.get_execution_report()
            }
            
            # ì¼ê°„ ë¦¬í¬íŠ¸ ì „ì†¡
            success = send_daily_report(report_data)
            
            if success:
                logger.info(f"ğŸ“ˆ 24ì‹œê°„ ì¼ê°„ ë¦¬í¬íŠ¸ ì „ì†¡ ì„±ê³µ: {report_data['total_posts']}ê°œ ê²Œì‹œê¸€")
                return True
            else:
                raise Exception("24ì‹œê°„ ì¼ê°„ ë¦¬í¬íŠ¸ ì „ì†¡ ì‹¤íŒ¨")
            
        except Exception as e:
            self.error_manager.handle_error(e, ErrorType.NOTIFICATION, ErrorSeverity.HIGH, 
                                          {'function': 'run_24h_daily_report'})
            return False
    
    def _get_24h_sentiment_summary(self) -> Dict:
        """24ì‹œê°„ ê°ì„± ìš”ì•½ - v4.5 ì™„ì „ ë³´ì¡´ + ğŸ”§ ìˆ˜ì • 4: json ì²˜ë¦¬ ì•ˆì „í™”"""
        try:
            sentiment_file = "daily_sentiment_data.json"
            
            # ğŸ”§ ìˆ˜ì • 4: ì•ˆì „í•œ json íŒŒì¼ ì²˜ë¦¬
            try:
                if not os.path.exists(sentiment_file):
                    return {'positive': 0, 'negative': 0, 'neutral': 0}
                    
                with open(sentiment_file, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
                    if not content:
                        return {'positive': 0, 'negative': 0, 'neutral': 0}
                    sentiment_data = json.loads(content)
                    
                if not isinstance(sentiment_data, list):
                    logger.warning("sentiment_data.jsonì´ ë°°ì—´ì´ ì•„ë‹˜")
                    return {'positive': 0, 'negative': 0, 'neutral': 0}
                    
            except (json.JSONDecodeError, ValueError) as e:
                logger.warning(f"sentiment_data.json íŒŒì‹± ì‹¤íŒ¨: {e}")
                return {'positive': 0, 'negative': 0, 'neutral': 0}
            except Exception as e:
                logger.error(f"sentiment_data.json ì½ê¸° ì‹¤íŒ¨: {e}")
                return {'positive': 0, 'negative': 0, 'neutral': 0}
            
            # 24ì‹œê°„ ë°ì´í„°ë§Œ í•„í„°ë§
            cutoff_time = datetime.now() - timedelta(hours=24)
            daily_data = [
                entry for entry in sentiment_data
                if datetime.fromisoformat(entry.get('timestamp', datetime.now().isoformat())) > cutoff_time
            ]
            
            # ê°ì„±ë³„ ì¹´ìš´íŠ¸
            sentiment_counts = {'positive': 0, 'negative': 0, 'neutral': 0}
            
            for entry in daily_data:
                sentiment = entry.get('sentiment', 'neutral')
                if sentiment in sentiment_counts:
                    sentiment_counts[sentiment] += 1
            
            return sentiment_counts
            
        except Exception as e:
            logger.error(f"24ì‹œê°„ ê°ì„± ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return {'positive': 0, 'negative': 0, 'neutral': 0}
    
    def _extract_top_keywords(self, posts: List[Dict]) -> List[str]:
        """ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ - v4.5 ì™„ì „ ë³´ì¡´"""
        try:
            if not posts:
                return []
            
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ NLP ì²˜ë¦¬ í•„ìš”)
            from collections import Counter
            import re
            
            all_text = ""
            for post in posts:
                title = post.get('title', '')
                content = post.get('content', '')
                all_text += f" {title} {content}"
            
            # ë‹¨ìˆœ í‚¤ì›Œë“œ ì¶”ì¶œ (í•œê¸€, ì˜ë¬¸ 3ì ì´ìƒ)
            keywords = re.findall(r'[ê°€-í£]{2,}|[a-zA-Z]{3,}', all_text.lower())
            
            # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ ë°˜í™˜
            counter = Counter(keywords)
            return [keyword for keyword, count in counter.most_common(10)]
            
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def get_execution_report(self) -> str:
        """ì‹¤í–‰ í†µê³„ ë³´ê³ ì„œ - v4.5 ì™„ì „ ë³´ì¡´"""
        execution_time = datetime.now() - self.start_time
        
        report = f"""
âœ¨ Epic7 ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ v4.6 ì‹¤í–‰ ë³´ê³ ì„œ

ğŸ• ì‹¤í–‰ ì •ë³´:
- ëª¨ë“œ: {self.stats['mode']}
- ìŠ¤ì¼€ì¤„: {self.stats['schedule']}
- ì‹œì‘ ì‹œê°„: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}
- ì‹¤í–‰ ì‹œê°„: {str(execution_time).split('.')[0]}

ğŸ“Š í¬ë¡¤ë§ í†µê³„:
- ì´ í¬ë¡¤ë§: {self.stats['total_crawled']}ê°œ
- í•œêµ­ ì‚¬ì´íŠ¸: {self.stats['korea_sites_crawled']}ê°œ
- ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸: {self.stats['global_sites_crawled']}ê°œ
- ì²˜ë¦¬ ì„±ê³µ: {self.stats['processed_posts']}ê°œ
- ì²˜ë¦¬ ì‹¤íŒ¨: {self.stats['failed_posts']}ê°œ

ğŸš¨ ì•Œë¦¼ ì „ì†¡:
- ì¦‰ì‹œ ë²„ê·¸ ì•Œë¦¼: {self.stats['immediate_bug_alerts']}ê°œ
- ì¦‰ì‹œ ê°ì„± ì•Œë¦¼: {self.stats['immediate_sentiment_alerts']}ê°œ
- ë²„ê·¸ ê²Œì‹œê¸€: {self.stats['bug_posts']}ê°œ
- ê°ì„± ê²Œì‹œê¸€: {self.stats['sentiment_posts']}ê°œ

ğŸ’¾ ë°ì´í„° ì €ì¥:
- ê°ì„± ë°ì´í„° ì €ì¥ ì„±ê³µ: {self.stats['sentiment_save_success']}ê°œ
- ê°ì„± ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {self.stats['sentiment_save_failed']}ê°œ

âš ï¸ ì—ëŸ¬ í†µê³„:
- ì´ ì—ëŸ¬: {self.stats['errors']}ê°œ
- ì—ëŸ¬ ë³µêµ¬ ì„±ê³µ: {self.stats['error_recoveries']}ê°œ
- ì¹˜ëª…ì  ì•Œë¦¼: {self.stats['critical_alerts']}ê°œ
- ë†’ì€ ìš°ì„ ìˆœìœ„ ì•Œë¦¼: {self.stats['high_priority_alerts']}ê°œ

ğŸ’¡ ì‹œìŠ¤í…œ ìƒíƒœ:
- ë””ë²„ê·¸ ëª¨ë“œ: {'í™œì„±í™”' if self.stats['debug'] else 'ë¹„í™œì„±í™”'}
- ê°•ì œ í¬ë¡¤ë§: {'í™œì„±í™”' if self.stats['force_crawl'] else 'ë¹„í™œì„±í™”'}
- Crawler ëª¨ë“ˆ: {'ì‚¬ìš© ê°€ëŠ¥' if CRAWLER_AVAILABLE else 'ì‚¬ìš© ë¶ˆê°€'}
- Classifier ëª¨ë“ˆ: {'ì‚¬ìš© ê°€ëŠ¥' if CLASSIFIER_AVAILABLE else 'ì‚¬ìš© ë¶ˆê°€'}

{self.error_manager.get_error_report()}
""".strip()
        
        return report
    
    def run(self) -> bool:
        """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - v4.6 ìŠ¤ì¼€ì¤„ë³„ ë¶„ê¸°"""
        try:
            logger.info(f"Epic7 ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ v4.6 ì‹¤í–‰ ì‹œì‘: {self.schedule} ìŠ¤ì¼€ì¤„, {self.mode} ëª¨ë“œ")
            
            if self.schedule == "15min":
                return self.run_15min_crawling_and_bug_alert()
            elif self.schedule == "30min":
                return self.run_30min_sentiment_notification()
            elif self.schedule == "24h":
                return self.run_24h_daily_report()
            else:
                logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ìŠ¤ì¼€ì¤„: {self.schedule}")
                return False
                
        except Exception as e:
            self.error_manager.handle_error(e, ErrorType.CRITICAL, ErrorSeverity.CRITICAL, 
                                          {'function': 'run', 'schedule': self.schedule, 'mode': self.mode})
            return False
        finally:
            logger.info("Epic7 ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ v4.6 ì‹¤í–‰ ì™„ë£Œ")

# =============================================================================
# ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ (v4.5 ì™„ì „ ë³´ì¡´)
# =============================================================================

def signal_handler(signum, frame):
    """ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ - ì•ˆì „í•œ ì¢…ë£Œ"""
    logger.info(f"ì‹œê·¸ë„ {signum} ìˆ˜ì‹  - ì•ˆì „í•œ ì¢…ë£Œ ì‹œì‘")
    ExecutionManager.release_lock()
    sys.exit(0)

# =============================================================================
# ë©”ì¸ í•¨ìˆ˜
# =============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - ğŸ”§ ìˆ˜ì • 5: ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€ ì ìš©"""
    # ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ë“±ë¡
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        # ì¸ì íŒŒì‹±
        args = parse_arguments()
        
        # ì‹¤í–‰ ë½ íšë“
        if not ExecutionManager.acquire_lock():
            logger.error("ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return False
        
        logger.info("=" * 80)
        logger.info(f"Epic7 í†µí•© ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ v4.6 ì‹œì‘")
        logger.info(f"ëª¨ë“œ: {args.mode}, ìŠ¤ì¼€ì¤„: {args.schedule}, ë””ë²„ê·¸: {args.debug}")
        logger.info("=" * 80)
        
        # ğŸ”§ ìˆ˜ì • 5: ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€ - crawl_all() í˜¸ì¶œ ì œê±°
        # ê¸°ì¡´ì— ìˆë˜ ì¤‘ë³µ crawl_all() í˜¸ì¶œì„ ì œê±°í•˜ì—¬ ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€
        
        # Epic7Monitor ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ì‹¤í–‰
        monitor = Epic7Monitor(
            mode=args.mode,
            schedule=args.schedule,
            debug=args.debug,
            force_crawl=args.force_crawl
        )
        
        # ëª¨ë‹ˆí„°ë§ ì‹¤í–‰
        success = monitor.run()
        
        # ì‹¤í–‰ ê²°ê³¼ ë³´ê³ 
        logger.info("=" * 80)
        logger.info("Epic7 ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ v4.6 ì‹¤í–‰ ì™„ë£Œ ë³´ê³ ì„œ")
        logger.info("=" * 80)
        print(monitor.get_execution_report())
        
        return success
        
    except KeyboardInterrupt:
        logger.info("ì‚¬ìš©ìì— ì˜í•œ ì¤‘ë‹¨")
        return False
    except Exception as e:
        error_manager.handle_error(e, ErrorType.CRITICAL, ErrorSeverity.CRITICAL, 
                                 {'function': 'main'})
        return False
    finally:
        # ì‹¤í–‰ ë½ í•´ì œ
        ExecutionManager.release_lock()
        logger.info("Epic7 ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ v4.6 ì¢…ë£Œ")

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)