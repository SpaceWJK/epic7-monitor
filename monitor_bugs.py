# monitor_bugs.py
import json
import time
from crawler import crawl_all_sites
from classifier import is_bug_post
from notifier import send_bug_alert

# Discord ì›¹í›… URL (GitHub Secrets ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ ê¶Œì¥)
WEBHOOK_URL = "https://discord.com/api/webhooks/xxx/yyy"

# ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ ìƒíƒœ íŒŒì¼
STATE_FILE = "crawled_links.json"

def load_state():
    try:
        with open(STATE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        return []

def save_state(links):
    with open(STATE_FILE, "w", encoding="utf-8") as f:
        json.dump(links, f, ensure_ascii=False, indent=2)

def main():
    print("--- ì‹¤ì‹œê°„ ë²„ê·¸ ê°ì‹œ ì‹œì‘ ---")

    # ì´ì „ì— ì²˜ë¦¬í•œ ë§í¬ ë¡œë”©
    crawled_links = load_state()
    print(f"ì´ë¯¸ ì²˜ë¦¬ëœ ê²Œì‹œê¸€ ìˆ˜: {len(crawled_links)}")

    # ëª¨ë“  ì»¤ë®¤ë‹ˆí‹° í¬ë¡¤ë§
    posts = crawl_all_sites()

    new_bugs_detected = 0

    for post in posts:
        title = post["title"]
        url = post["url"]
        source = post.get("source", "Unknown")

        # ì´ë¯¸ ì²˜ë¦¬ëœ ë§í¬ì¸ì§€ í™•ì¸
        if url in crawled_links:
            continue

        # ğŸš€ Stove ë²„ê·¸ ê²Œì‹œíŒ force_bug or í‚¤ì›Œë“œ í•„í„°ë§
        if post.get("force_bug") or is_bug_post(title):
            send_bug_alert(WEBHOOK_URL, f"[{source}] {title}", url)
            new_bugs_detected += 1

        # ìƒíƒœ ì—…ë°ì´íŠ¸
        crawled_links.append(url)

    # ìƒíƒœ íŒŒì¼ ì €ì¥
    save_state(crawled_links)

    print(f"ì´ë²ˆ íƒìƒ‰ì—ì„œ ìƒˆë¡œ ê°ì§€ëœ ë²„ê·¸ ê²Œì‹œê¸€ ìˆ˜: {new_bugs_detected}")

if __name__ == "__main__":
    main()
