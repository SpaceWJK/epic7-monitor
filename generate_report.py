# generate_report.py - ì½”ë“œ ë°°ì—´ ìˆ˜ì • ë° ë²ˆì—­ ê¸°ëŠ¥ ì œì™¸ ë²„ì „
# Epic7 ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ - í†µê³„ ë°ì´í„° ì „ìš© ë¦¬í¬íŠ¸ ìƒì„±ê¸°

import json
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
import requests
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter
import statistics
import re
import hashlib
import time

# í¬ë¡¤ë§ ë° ë¶„ë¥˜ ëª¨ë“ˆ ì„í¬íŠ¸
from crawler import load_crawled_links, load_content_cache
from classifier import classify_post, is_bug_post, is_positive_post, is_negative_post

@dataclass
class ReportData:
    """ë¦¬í¬íŠ¸ ë°ì´í„° êµ¬ì¡°"""
    date: str
    total_posts: int
    korean_posts: int
    global_posts: int
    bug_posts: int
    positive_posts: int
    negative_posts: int
    neutral_posts: int
    top_sources: Dict[str, int]
    trend_analysis: Dict[str, Any]
    insights: List[str]
    recommendations: List[str]

class GlobalDataManager:
    """ê¸€ë¡œë²Œ ë°ì´í„° í†µí•© ê´€ë¦¬ì"""
    
    def __init__(self):
        self.korean_sources = [
            "stove_bug", "stove_general", 
            "ruliweb_epic7", "arca_epic7"
        ]
        self.global_sources = [
            "stove_global_bug", "stove_global_general", 
            "reddit_epic7", "global_forum"
        ]
        self.data_cache = {}
        self.trend_cache = {}
        
    def load_all_data(self, hours: int = 24) -> Dict[str, List[Dict]]:
        """ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ë¡œë“œ"""
        all_data = {
            "korean": [],
            "global": [],
            "combined": []
        }
        
        # í•œêµ­ ì‚¬ì´íŠ¸ ë°ì´í„° ë¡œë“œ
        try:
            korean_links = self._load_links_file("crawled_links_korean.json")
            korean_cache = self._load_cache_file("content_cache_korean.json")
            
            for link in korean_links.get("links", []):
                if self._is_within_timeframe(link, hours):
                    post_data = self._get_post_data(link, korean_cache)
                    if post_data:
                        all_data["korean"].append(post_data)
                        all_data["combined"].append(post_data)
                        
        except Exception as e:
            print(f"[ERROR] í•œêµ­ ì‚¬ì´íŠ¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            
        # ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ ë°ì´í„° ë¡œë“œ
        try:
            global_links = self._load_links_file("crawled_links_global.json")
            global_cache = self._load_cache_file("content_cache_global.json")
            
            for link in global_links.get("links", []):
                if self._is_within_timeframe(link, hours):
                    post_data = self._get_post_data(link, global_cache)
                    if post_data:
                        all_data["global"].append(post_data)
                        all_data["combined"].append(post_data)
                        
        except Exception as e:
            print(f"[ERROR] ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            
        # í´ë°±: í†µí•© íŒŒì¼ ì‚¬ìš©
        if not all_data["korean"] and not all_data["global"]:
            try:
                fallback_links = self._load_links_file("crawled_links.json")
                fallback_cache = self._load_cache_file("content_cache.json")
                
                for link in fallback_links.get("links", []):
                    if self._is_within_timeframe(link, hours):
                        post_data = self._get_post_data(link, fallback_cache)
                        if post_data:
                            all_data["combined"].append(post_data)
                            
            except Exception as e:
                print(f"[ERROR] í´ë°± ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
                
        return all_data
        
    def _load_links_file(self, filename: str) -> Dict:
        """ë§í¬ íŒŒì¼ ë¡œë“œ"""
        if os.path.exists(filename):
            try:
                with open(filename, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"[ERROR] {filename} ë¡œë“œ ì‹¤íŒ¨: {e}")
                
        return {"links": [], "last_updated": datetime.now().isoformat()}
        
    def _load_cache_file(self, filename: str) -> Dict:
        """ìºì‹œ íŒŒì¼ ë¡œë“œ"""
        if os.path.exists(filename):
            try:
                with open(filename, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"[ERROR] {filename} ë¡œë“œ ì‹¤íŒ¨: {e}")
                
        return {}
        
    def _is_within_timeframe(self, link_data: Any, hours: int) -> bool:
        """ì‹œê°„ ë²”ìœ„ ë‚´ ë°ì´í„° í™•ì¸"""
        try:
            if isinstance(link_data, dict):
                timestamp_str = link_data.get('timestamp', '')
            else:
                # ë¬¸ìì—´ ë§í¬ì¸ ê²½ìš° í˜„ì¬ ì‹œê°„ ì‚¬ìš©
                return True
                
            if not timestamp_str:
                return True
                
            link_time = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
            cutoff_time = datetime.now() - timedelta(hours=hours)
            
            return link_time > cutoff_time
            
        except:
            return True
            
    def _get_post_data(self, link_data: Any, cache: Dict) -> Optional[Dict]:
        """ê²Œì‹œê¸€ ë°ì´í„° ì¶”ì¶œ"""
        try:
            if isinstance(link_data, dict):
                url = link_data.get('url', '')
                title = link_data.get('title', '')
                source = link_data.get('source', 'unknown')
                timestamp = link_data.get('timestamp', datetime.now().isoformat())
            else:
                url = link_data
                title = ''
                source = 'unknown'
                timestamp = datetime.now().isoformat()
                
            # ìºì‹œì—ì„œ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
            url_hash = hashlib.md5(url.encode('utf-8')).hexdigest()
            cached_item = cache.get(url_hash, {})
            content = cached_item.get('content', '')
            
            # ì œëª©ì´ ì—†ìœ¼ë©´ ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê¸°
            if not title and cached_item.get('title'):
                title = cached_item['title']
                
            if not title:
                title = "ì œëª© ì—†ìŒ"
                
            return {
                'url': url,
                'title': title,
                'content': content,
                'source': source,
                'timestamp': timestamp
            }
            
        except Exception as e:
            print(f"[ERROR] ê²Œì‹œê¸€ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

class TrendAnalyzer:
    """íŠ¸ë Œë“œ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.trend_data = {}
        self.analysis_cache = {}
        
    def analyze_sentiment_trends(self, data: List[Dict], hours: int = 24) -> Dict[str, Any]:
        """ê°ì„± íŠ¸ë Œë“œ ë¶„ì„"""
        hourly_sentiment = defaultdict(lambda: {
            'positive': 0, 'negative': 0, 'neutral': 0, 'bug': 0
        })
        
        for post in data:
            try:
                timestamp = datetime.fromisoformat(post['timestamp'].replace('Z', '+00:00'))
                hour_key = timestamp.strftime('%Y-%m-%d-%H')
                
                title = post.get('title', '')
                content = post.get('content', '')
                text = f"{title} {content}"
                
                # ë¶„ë¥˜
                if is_bug_post(title):
                    hourly_sentiment[hour_key]['bug'] += 1
                elif is_positive_post(title):
                    hourly_sentiment[hour_key]['positive'] += 1
                elif is_negative_post(title):
                    hourly_sentiment[hour_key]['negative'] += 1
                else:
                    hourly_sentiment[hour_key]['neutral'] += 1
                    
            except Exception as e:
                print(f"[ERROR] ê°ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
                continue
                
        # íŠ¸ë Œë“œ ê³„ì‚°
        trend_analysis = {
            'hourly_data': dict(hourly_sentiment),
            'overall_sentiment': self._calculate_overall_sentiment(hourly_sentiment),
            'peak_hours': self._find_peak_hours(hourly_sentiment),
            'sentiment_velocity': self._calculate_sentiment_velocity(hourly_sentiment)
        }
        
        return trend_analysis
        
    def analyze_source_trends(self, data: List[Dict]) -> Dict[str, Any]:
        """ì†ŒìŠ¤ë³„ íŠ¸ë Œë“œ ë¶„ì„"""
        source_counts = Counter()
        source_sentiment = defaultdict(lambda: {
            'positive': 0, 'negative': 0, 'neutral': 0, 'bug': 0
        })
        
        for post in data:
            source = post.get('source', 'unknown')
            title = post.get('title', '')
            
            source_counts[source] += 1
            
            if is_bug_post(title):
                source_sentiment[source]['bug'] += 1
            elif is_positive_post(title):
                source_sentiment[source]['positive'] += 1
            elif is_negative_post(title):
                source_sentiment[source]['negative'] += 1
            else:
                source_sentiment[source]['neutral'] += 1
                
        return {
            'source_counts': dict(source_counts),
            'source_sentiment': dict(source_sentiment),
            'most_active_sources': source_counts.most_common(5),
            'source_analysis': self._analyze_source_patterns(source_sentiment)
        }
        
    def _calculate_overall_sentiment(self, hourly_data: Dict) -> Dict[str, float]:
        """ì „ì²´ ê°ì„± ì ìˆ˜ ê³„ì‚°"""
        total_positive = sum(hour['positive'] for hour in hourly_data.values())
        total_negative = sum(hour['negative'] for hour in hourly_data.values())
        total_neutral = sum(hour['neutral'] for hour in hourly_data.values())
        total_bug = sum(hour['bug'] for hour in hourly_data.values())
        
        total_posts = total_positive + total_negative + total_neutral + total_bug
        
        if total_posts == 0:
            return {'positive': 0, 'negative': 0, 'neutral': 0, 'bug': 0}
            
        return {
            'positive': (total_positive / total_posts) * 100,
            'negative': (total_negative / total_posts) * 100,
            'neutral': (total_neutral / total_posts) * 100,
            'bug': (total_bug / total_posts) * 100
        }
        
    def _find_peak_hours(self, hourly_data: Dict) -> Dict[str, str]:
        """í”¼í¬ ì‹œê°„ ì°¾ê¸°"""
        max_activity = 0
        peak_hour = ""
        
        for hour, data in hourly_data.items():
            total_activity = sum(data.values())
            if total_activity > max_activity:
                max_activity = total_activity
                peak_hour = hour
                
        return {
            'peak_hour': peak_hour,
            'peak_activity': max_activity
        }
        
    def _calculate_sentiment_velocity(self, hourly_data: Dict) -> Dict[str, float]:
        """ê°ì„± ë³€í™” ì†ë„ ê³„ì‚°"""
        if len(hourly_data) < 2:
            return {'velocity': 0.0, 'direction': 'stable'}
            
        hours = sorted(hourly_data.keys())
        sentiment_scores = []
        
        for hour in hours:
            data = hourly_data[hour]
            total = sum(data.values())
            
            if total > 0:
                sentiment_score = (data['positive'] - data['negative']) / total
                sentiment_scores.append(sentiment_score)
                
        if len(sentiment_scores) < 2:
            return {'velocity': 0.0, 'direction': 'stable'}
            
        velocity = sentiment_scores[-1] - sentiment_scores[0]
        direction = 'improving' if velocity > 0.1 else 'declining' if velocity < -0.1 else 'stable'
        
        return {
            'velocity': velocity,
            'direction': direction
        }
        
    def _analyze_source_patterns(self, source_sentiment: Dict) -> Dict[str, Any]:
        """ì†ŒìŠ¤ íŒ¨í„´ ë¶„ì„"""
        patterns = {}
        
        for source, sentiment in source_sentiment.items():
            total = sum(sentiment.values())
            
            if total > 0:
                patterns[source] = {
                    'total_posts': total,
                    'bug_ratio': sentiment['bug'] / total,
                    'positive_ratio': sentiment['positive'] / total,
                    'negative_ratio': sentiment['negative'] / total,
                    'dominant_sentiment': max(sentiment.items(), key=lambda x: x[1])[0]
                }
                
        return patterns

class InsightGenerator:
    """ì¸ì‚¬ì´íŠ¸ ìƒì„±ê¸°"""
    
    def __init__(self):
        self.insight_templates = {
            'bug_trend': "ë²„ê·¸ ë¦¬í¬íŠ¸ê°€ {period}ì— {change}% {direction}í–ˆìŠµë‹ˆë‹¤.",
            'sentiment_change': "ì „ì²´ ê°ì„±ì´ {previous}ì—ì„œ {current}ë¡œ ë³€í™”í–ˆìŠµë‹ˆë‹¤.",
            'peak_activity': "ê°€ì¥ í™œë°œí•œ ì‹œê°„ì€ {hour}ì´ë©°, ì´ {count}ê°œì˜ ê²Œì‹œê¸€ì´ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.",
            'source_dominance': "{source}ê°€ ì „ì²´ ê²Œì‹œê¸€ì˜ {percentage}%ë¥¼ ì°¨ì§€í•˜ë©° ê°€ì¥ í™œë°œí•œ ì†ŒìŠ¤ì…ë‹ˆë‹¤."
        }
        
    def generate_insights(self, data: List[Dict], trend_analysis: Dict) -> List[str]:
        """ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        try:
            # ë²„ê·¸ íŠ¸ë Œë“œ ì¸ì‚¬ì´íŠ¸
            bug_posts = [post for post in data if is_bug_post(post.get('title', ''))]
            if bug_posts:
                bug_insight = f"ì§€ë‚œ 24ì‹œê°„ ë™ì•ˆ ì´ {len(bug_posts)}ê°œì˜ ë²„ê·¸ ë¦¬í¬íŠ¸ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤."
                insights.append(bug_insight)
                
            # ê°ì„± íŠ¸ë Œë“œ ì¸ì‚¬ì´íŠ¸
            overall_sentiment = trend_analysis.get('overall_sentiment', {})
            if overall_sentiment:
                dominant_sentiment = max(overall_sentiment.items(), key=lambda x: x[1])
                sentiment_insight = f"ì „ì²´ ê°ì„± ì¤‘ {dominant_sentiment[0]}ê°€ {dominant_sentiment[1]:.1f}%ë¡œ ê°€ì¥ ë†’ìŠµë‹ˆë‹¤."
                insights.append(sentiment_insight)
                
            # í”¼í¬ ì‹œê°„ ì¸ì‚¬ì´íŠ¸
            peak_info = trend_analysis.get('peak_hours', {})
            if peak_info.get('peak_hour'):
                peak_insight = f"ê°€ì¥ í™œë°œí•œ ì‹œê°„ì€ {peak_info['peak_hour'][-2:]}ì‹œì´ë©°, {peak_info['peak_activity']}ê°œì˜ ê²Œì‹œê¸€ì´ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤."
                insights.append(peak_insight)
                
            # ì†ŒìŠ¤ ë¶„ì„ ì¸ì‚¬ì´íŠ¸
            source_analysis = trend_analysis.get('source_analysis', {})
            if source_analysis:
                most_active_source = max(source_analysis.items(), key=lambda x: x[1]['total_posts'])
                source_insight = f"{most_active_source[0]} ì†ŒìŠ¤ê°€ {most_active_source[1]['total_posts']}ê°œ ê²Œì‹œê¸€ë¡œ ê°€ì¥ í™œë°œí•©ë‹ˆë‹¤."
                insights.append(source_insight)
                
            # ê¸€ë¡œë²Œ vs í•œêµ­ ë¹„êµ
            korean_posts = [post for post in data if post.get('source', '').startswith('stove_') or 'ruliweb' in post.get('source', '')]
            global_posts = [post for post in data if 'global' in post.get('source', '') or 'reddit' in post.get('source', '')]
            
            if korean_posts and global_posts:
                ratio = len(korean_posts) / len(global_posts)
                if ratio > 2:
                    insights.append(f"í•œêµ­ ì‚¬ì´íŠ¸ í™œë™ì´ ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ë³´ë‹¤ {ratio:.1f}ë°° í™œë°œí•©ë‹ˆë‹¤.")
                elif ratio < 0.5:
                    insights.append(f"ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ í™œë™ì´ í•œêµ­ ì‚¬ì´íŠ¸ë³´ë‹¤ {1/ratio:.1f}ë°° í™œë°œí•©ë‹ˆë‹¤.")
                    
        except Exception as e:
            print(f"[ERROR] ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            insights.append("ì¸ì‚¬ì´íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            
        return insights[:10]  # ìµœëŒ€ 10ê°œ ì¸ì‚¬ì´íŠ¸
        
    def generate_recommendations(self, data: List[Dict], insights: List[str]) -> List[str]:
        """ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        try:
            # ë²„ê·¸ ë¦¬í¬íŠ¸ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
            bug_posts = [post for post in data if is_bug_post(post.get('title', ''))]
            if len(bug_posts) > 10:
                recommendations.append("ë²„ê·¸ ë¦¬í¬íŠ¸ê°€ ë§ì´ ì¦ê°€í–ˆìŠµë‹ˆë‹¤. ê°œë°œíŒ€ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                
            # ê°ì„± ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
            negative_posts = [post for post in data if is_negative_post(post.get('title', ''))]
            if len(negative_posts) > len(data) * 0.3:
                recommendations.append("ë¶€ì •ì ì¸ ê²Œì‹œê¸€ì´ 30% ì´ìƒì…ë‹ˆë‹¤. ì»¤ë®¤ë‹ˆí‹° ê´€ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                
            # ì†ŒìŠ¤ ë‹¤ì–‘ì„± ê¶Œì¥ì‚¬í•­
            sources = set(post.get('source', '') for post in data)
            if len(sources) < 3:
                recommendations.append("ëª¨ë‹ˆí„°ë§ ì†ŒìŠ¤ê°€ ì œí•œì ì…ë‹ˆë‹¤. ì¶”ê°€ ì†ŒìŠ¤ í™•ì¥ì„ ê³ ë ¤í•˜ì„¸ìš”.")
                
            # í™œë™ íŒ¨í„´ ê¶Œì¥ì‚¬í•­
            if len(data) < 50:
                recommendations.append("ì „ì²´ ê²Œì‹œê¸€ ìˆ˜ê°€ ì ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ë²”ìœ„ í™•ì¥ì„ ê³ ë ¤í•˜ì„¸ìš”.")
                
        except Exception as e:
            print(f"[ERROR] ê¶Œì¥ì‚¬í•­ ìƒì„± ì‹¤íŒ¨: {e}")
            recommendations.append("ê¶Œì¥ì‚¬í•­ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            
        return recommendations[:5]  # ìµœëŒ€ 5ê°œ ê¶Œì¥ì‚¬í•­

class ReportGenerator:
    """í†µí•© ë¦¬í¬íŠ¸ ìƒì„±ê¸°"""
    
    def __init__(self):
        self.data_manager = GlobalDataManager()
        self.trend_analyzer = TrendAnalyzer()
        self.insight_generator = InsightGenerator()
        
    def generate_daily_report(self, hours: int = 24) -> ReportData:
        """ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        print(f"[INFO] {hours}ì‹œê°„ ë°ì´í„° ê¸°ë°˜ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘...")
        
        # ë°ì´í„° ë¡œë“œ
        all_data = self.data_manager.load_all_data(hours)
        combined_data = all_data['combined']
        
        print(f"[INFO] ì´ {len(combined_data)}ê°œ ê²Œì‹œê¸€ ë¶„ì„ ì¤‘...")
        
        # ê¸°ë³¸ í†µê³„ ê³„ì‚°
        total_posts = len(combined_data)
        korean_posts = len(all_data['korean'])
        global_posts = len(all_data['global'])
        
        # ë¶„ë¥˜ë³„ í†µê³„
        bug_posts = len([post for post in combined_data if is_bug_post(post.get('title', ''))])
        positive_posts = len([post for post in combined_data if is_positive_post(post.get('title', ''))])
        negative_posts = len([post for post in combined_data if is_negative_post(post.get('title', ''))])
        neutral_posts = total_posts - bug_posts - positive_posts - negative_posts
        
        # ì†ŒìŠ¤ë³„ í†µê³„
        source_counts = Counter(post.get('source', 'unknown') for post in combined_data)
        top_sources = dict(source_counts.most_common(10))
        
        # íŠ¸ë Œë“œ ë¶„ì„
        sentiment_trends = self.trend_analyzer.analyze_sentiment_trends(combined_data, hours)
        source_trends = self.trend_analyzer.analyze_source_trends(combined_data)
        
        # í†µí•© íŠ¸ë Œë“œ ë¶„ì„
        trend_analysis = {
            **sentiment_trends,
            **source_trends
        }
        
        # ì¸ì‚¬ì´íŠ¸ ìƒì„±
        insights = self.insight_generator.generate_insights(combined_data, trend_analysis)
        recommendations = self.insight_generator.generate_recommendations(combined_data, insights)
        
        # ë¦¬í¬íŠ¸ ë°ì´í„° ìƒì„±
        report_data = ReportData(
            date=datetime.now().strftime('%Y-%m-%d'),
            total_posts=total_posts,
            korean_posts=korean_posts,
            global_posts=global_posts,
            bug_posts=bug_posts,
            positive_posts=positive_posts,
            negative_posts=negative_posts,
            neutral_posts=neutral_posts,
            top_sources=top_sources,
            trend_analysis=trend_analysis,
            insights=insights,
            recommendations=recommendations
        )
        
        print(f"[INFO] ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {total_posts}ê°œ ê²Œì‹œê¸€ ë¶„ì„")
        return report_data
        
    def format_report_for_discord(self, report_data: ReportData) -> str:
        """Discordìš© ë¦¬í¬íŠ¸ í¬ë§·íŒ…"""
        try:
            lines = []
            
            # í—¤ë”
            lines.append("ğŸ” **Epic7 ì¼ì¼ ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸**")
            lines.append(f"ğŸ“… **ë‚ ì§œ**: {report_data.date}")
            lines.append("=" * 40)
            
            # ê¸°ë³¸ í†µê³„
            lines.append("ğŸ“Š **ê¸°ë³¸ í†µê³„**")
            lines.append(f"â€¢ ì´ ê²Œì‹œê¸€: {report_data.total_posts}ê°œ")
            lines.append(f"â€¢ í•œêµ­ ì‚¬ì´íŠ¸: {report_data.korean_posts}ê°œ")
            lines.append(f"â€¢ ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸: {report_data.global_posts}ê°œ")
            lines.append("")
            
            # ë¶„ë¥˜ë³„ í†µê³„
            lines.append("ğŸ·ï¸ **ë¶„ë¥˜ë³„ í†µê³„**")
            lines.append(f"â€¢ ğŸ› ë²„ê·¸ ë¦¬í¬íŠ¸: {report_data.bug_posts}ê°œ")
            lines.append(f"â€¢ ğŸ˜Š ê¸ì •ì : {report_data.positive_posts}ê°œ")
            lines.append(f"â€¢ ğŸ˜ ë¶€ì •ì : {report_data.negative_posts}ê°œ")
            lines.append(f"â€¢ ğŸ˜ ì¤‘ë¦½ì : {report_data.neutral_posts}ê°œ")
            lines.append("")
            
            # ìƒìœ„ ì†ŒìŠ¤
            if report_data.top_sources:
                lines.append("ğŸ† **ìƒìœ„ í™œë™ ì†ŒìŠ¤**")
                for source, count in list(report_data.top_sources.items())[:5]:
                    source_name = self._get_source_display_name(source)
                    lines.append(f"â€¢ {source_name}: {count}ê°œ")
                lines.append("")
                
            # í•µì‹¬ ì¸ì‚¬ì´íŠ¸
            if report_data.insights:
                lines.append("ğŸ’¡ **í•µì‹¬ ì¸ì‚¬ì´íŠ¸**")
                for insight in report_data.insights[:5]:
                    lines.append(f"â€¢ {insight}")
                lines.append("")
                
            # ê¶Œì¥ì‚¬í•­
            if report_data.recommendations:
                lines.append("ğŸ¯ **ê¶Œì¥ì‚¬í•­**")
                for recommendation in report_data.recommendations[:3]:
                    lines.append(f"â€¢ {recommendation}")
                lines.append("")
                
            # í‘¸í„°
            lines.append("â”€" * 40)
            lines.append(f"ğŸ¤– **ìƒì„±ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            
            return "\n".join(lines)
            
        except Exception as e:
            print(f"[ERROR] Discord ë¦¬í¬íŠ¸ í¬ë§·íŒ… ì‹¤íŒ¨: {e}")
            return f"âŒ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            
    def _get_source_display_name(self, source: str) -> str:
        """ì†ŒìŠ¤ í‘œì‹œëª… ë³€í™˜"""
        source_names = {
            'stove_bug': 'ğŸª ìŠ¤í† ë¸Œ ë²„ê·¸ê²Œì‹œíŒ',
            'stove_general': 'ğŸª ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ',
            'stove_global_bug': 'ğŸŒ ìŠ¤í† ë¸Œ ê¸€ë¡œë²Œ ë²„ê·¸',
            'stove_global_general': 'ğŸŒ ìŠ¤í† ë¸Œ ê¸€ë¡œë²Œ ììœ ',
            'ruliweb_epic7': 'ğŸ® ë£¨ë¦¬ì›¹ ì—í”½ì„¸ë¸',
            'arca_epic7': 'ğŸ”¥ ì•„ì¹´ë¼ì´ë¸Œ ì—í”½ì„¸ë¸',
            'reddit_epic7': 'ğŸŒ Reddit EpicSeven',
            'global_forum': 'ğŸŒ ê¸€ë¡œë²Œ í¬ëŸ¼'
        }
        
        return source_names.get(source, source)

class DiscordReporter:
    """Discord ë¦¬í¬íŠ¸ ì „ì†¡ê¸°"""
    
    def __init__(self):
        self.webhook_url = os.environ.get('DISCORD_WEBHOOK_REPORT')
        self.max_message_length = 1900
        
    def send_daily_report(self, report_data: ReportData) -> bool:
        """ì¼ì¼ ë¦¬í¬íŠ¸ ì „ì†¡"""
        if not self.webhook_url:
            print("[WARNING] Discord ì›¹í›… URLì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
            
        try:
            generator = ReportGenerator()
            report_text = generator.format_report_for_discord(report_data)
            
            # ë©”ì‹œì§€ ê¸¸ì´ í™•ì¸ ë° ë¶„í• 
            if len(report_text) > self.max_message_length:
                messages = self._split_message(report_text)
                for i, message in enumerate(messages):
                    success = self._send_message(
                        message, 
                        f"Epic7 ì¼ì¼ ë¦¬í¬íŠ¸ ({i+1}/{len(messages)})"
                    )
                    if not success:
                        return False
                    time.sleep(1)  # ë©”ì‹œì§€ ê°„ ëŒ€ê¸°
            else:
                success = self._send_message(report_text, "Epic7 ì¼ì¼ ë¦¬í¬íŠ¸")
                if not success:
                    return False
                    
            print("[INFO] Discord ì¼ì¼ ë¦¬í¬íŠ¸ ì „ì†¡ ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"[ERROR] Discord ë¦¬í¬íŠ¸ ì „ì†¡ ì‹¤íŒ¨: {e}")
            return False
            
    def _split_message(self, text: str) -> List[str]:
        """ë©”ì‹œì§€ ë¶„í• """
        lines = text.split('\n')
        messages = []
        current_message = ""
        
        for line in lines:
            if len(current_message + line + '\n') > self.max_message_length:
                if current_message:
                    messages.append(current_message.strip())
                current_message = line + '\n'
            else:
                current_message += line + '\n'
                
        if current_message:
            messages.append(current_message.strip())
            
        return messages
        
    def _send_message(self, message: str, title: str) -> bool:
        """ë©”ì‹œì§€ ì „ì†¡"""
        try:
            data = {
                "embeds": [{
                    "title": title,
                    "description": message,
                    "color": 0x00ff00,
                    "timestamp": datetime.now().isoformat()
                }]
            }
            
            response = requests.post(self.webhook_url, json=data, timeout=10)
            return response.status_code == 204
            
        except Exception as e:
            print(f"[ERROR] Discord ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨: {e}")
            return False

# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜ë“¤
def get_all_posts_for_report(hours: int = 24) -> List[Dict]:
    """ë¦¬í¬íŠ¸ìš© ê²Œì‹œê¸€ ìˆ˜ì§‘ (í•˜ìœ„ í˜¸í™˜ì„±)"""
    manager = GlobalDataManager()
    all_data = manager.load_all_data(hours)
    return all_data['combined']

def send_daily_report():
    """ì¼ì¼ ë¦¬í¬íŠ¸ ì „ì†¡ (ë©”ì¸ í•¨ìˆ˜)"""
    try:
        print("[INFO] ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘...")
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        generator = ReportGenerator()
        report_data = generator.generate_daily_report(24)
        
        # Discord ì „ì†¡
        reporter = DiscordReporter()
        success = reporter.send_daily_report(report_data)
        
        if success:
            print("[INFO] ì¼ì¼ ë¦¬í¬íŠ¸ ì „ì†¡ ì™„ë£Œ")
            
            # ë¦¬í¬íŠ¸ ë°ì´í„° ì €ì¥
            report_file = f"daily_report_{datetime.now().strftime('%Y%m%d')}.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(asdict(report_data), f, ensure_ascii=False, indent=2)
            print(f"[INFO] ë¦¬í¬íŠ¸ ë°ì´í„° ì €ì¥: {report_file}")
            
        else:
            print("[ERROR] ì¼ì¼ ë¦¬í¬íŠ¸ ì „ì†¡ ì‹¤íŒ¨")
            
    except Exception as e:
        print(f"[ERROR] ì¼ì¼ ë¦¬í¬íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ì—ëŸ¬ ì•Œë¦¼
        error_webhook = os.environ.get('DISCORD_WEBHOOK_BUG')
        if error_webhook:
            try:
                error_data = {
                    "embeds": [{
                        "title": "âŒ ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨",
                        "description": f"ì˜¤ë¥˜ ë‚´ìš©: {str(e)[:1000]}",
                        "color": 0xff0000,
                        "timestamp": datetime.now().isoformat()
                    }]
                }
                requests.post(error_webhook, json=error_data, timeout=10)
            except:
                pass

if __name__ == "__main__":
    send_daily_report()