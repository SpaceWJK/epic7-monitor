#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Epic7 ì£¼ê¸°ë³„ í¬ë¡¤ëŸ¬ v3.3 - Force Crawl ì§€ì›
- CSS Selector ë‹¤ì¤‘ í´ë°± ì‹œìŠ¤í…œ ì ìš©
- JavaScript ë Œë”ë§ ëŒ€ê¸°ì‹œê°„ ìµœì í™” (20ì´ˆ/25ì´ˆ)
- Force Crawl ì˜µì…˜ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬ ìš°íšŒ ê°€ëŠ¥
- ë²„ê·¸ ê²Œì‹œíŒ: 15ë¶„ ê°„ê²©, ì¼ë°˜ ê²Œì‹œíŒ: 30ë¶„ ê°„ê²©
- ì‹¤ì‹œê°„ ì•Œë¦¼: ë²„ê·¸ ê²Œì‹œíŒ ì¦‰ì‹œ ì „ì†¡
"""

import time
import random
import re
import requests
import concurrent.futures
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from urllib.parse import urljoin, urlparse

# Selenium ê´€ë ¨ import
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from selenium.webdriver.chrome.service import Service

# HTML íŒŒì‹±
from bs4 import BeautifulSoup

# ê³µí†µ ëª¨ë“ˆ ì„í¬íŠ¸
from config import config
from file_manager import load_json, save_json, with_file_lock
from utils import (
    get_url_hash, extract_content_summary, fix_stove_url,
    is_frequent_schedule, is_regular_schedule, retry_on_failure,
    get_random_user_agent, get_random_delay, setup_logging,
    format_timestamp, clean_data_list
)

# ë¡œê¹… ì„¤ì •
import logging
logger = logging.getLogger(__name__)

# =============================================================================
# í†µí•© íŒŒì¼ ì‹œìŠ¤í…œ ë° ìœ í‹¸ë¦¬í‹°
# =============================================================================

def load_crawled_links():
    """í¬ë¡¤ë§ëœ ë§í¬ë“¤ì„ ë¡œë“œ"""
    default_data = {
        "links": [], 
        "last_updated": datetime.now().isoformat()
    }
    return load_json(config.Files.CRAWLED_LINKS, default_data)

def save_crawled_links(link_data):
    """í¬ë¡¤ë§ëœ ë§í¬ë“¤ì„ ì €ì¥"""
    try:
        # ë§í¬ ìˆ˜ ì œí•œ
        if len(link_data["links"]) > 2000:
            link_data["links"] = link_data["links"][-2000:]
        
        link_data["last_updated"] = datetime.now().isoformat()
        
        success = save_json(config.Files.CRAWLED_LINKS, link_data)
        if success:
            logger.info(f"í¬ë¡¤ë§ ë§í¬ ì €ì¥ ì™„ë£Œ: {len(link_data['links'])}ê°œ")
        else:
            logger.error("í¬ë¡¤ë§ ë§í¬ ì €ì¥ ì‹¤íŒ¨")
        
        return success
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ë§í¬ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

def load_content_cache():
    """ê²Œì‹œê¸€ ë‚´ìš© ìºì‹œ ë¡œë“œ"""
    return load_json(config.Files.CONTENT_CACHE, {})

def save_content_cache(cache_data):
    """ê²Œì‹œê¸€ ë‚´ìš© ìºì‹œ ì €ì¥"""
    try:
        # ìºì‹œ í¬ê¸° ì œí•œ
        if len(cache_data) > 1000:
            cache_data = clean_data_list(list(cache_data.items()), 1000)
            cache_data = dict(cache_data)
        
        success = save_json(config.Files.CONTENT_CACHE, cache_data)
        if success:
            logger.info(f"ì½˜í…ì¸  ìºì‹œ ì €ì¥ ì™„ë£Œ: {len(cache_data)}ê°œ")
        else:
            logger.error("ì½˜í…ì¸  ìºì‹œ ì €ì¥ ì‹¤íŒ¨")
        
        return success
    except Exception as e:
        logger.error(f"ì½˜í…ì¸  ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

# =============================================================================
# Chrome ë“œë¼ì´ë²„
# =============================================================================

@retry_on_failure(max_retries=2, delay=1.0)
def get_chrome_driver(schedule_type='frequent'):
    """Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” (ì£¼ê¸°ë³„ ìµœì í™”)"""
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--disable-extensions')
    options.add_argument('--disable-plugins')
    options.add_argument('--disable-images')
    options.add_argument('--window-size=1920,1080')
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument('--disable-web-security')
    options.add_argument('--disable-features=VizDisplayCompositor')
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    
    # 15ë¶„ ê°„ê²© í¬ë¡¤ë§ ìµœì í™”
    if schedule_type == 'frequent':
        options.add_argument('--disable-background-timer-throttling')
        options.add_argument('--disable-renderer-backgrounding')
        options.add_argument('--disable-backgrounding-occluded-windows')
        options.add_argument('--disable-features=TranslateUI')
        options.add_argument('--disable-ipc-flooding-protection')
        timeout = config.Crawling.BUG_BOARD_TIMEOUT
    else:
        timeout = config.Crawling.GENERAL_BOARD_TIMEOUT
    
    # ëœë¤ User-Agent ì‚¬ìš©
    options.add_argument(f'--user-agent={get_random_user_agent()}')
    
    # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
    prefs = {
        'profile.default_content_setting_values': {
            'images': 2,
            'plugins': 2,
            'popups': 2,
            'geolocation': 2,
            'notifications': 2,
            'media_stream': 2,
        }
    }
    options.add_experimental_option('prefs', prefs)
    
    # Chrome Driver ê²½ë¡œ íƒìƒ‰
    possible_paths = [
        '/usr/local/bin/chromedriver',
        '/usr/bin/chromedriver',
        '/snap/bin/chromium.chromedriver',
        '/opt/chrome/chromedriver'
    ]
    
    for path in possible_paths:
        try:
            if os.path.exists(path):
                service = Service(path)
                driver = webdriver.Chrome(service=service, options=options)
                driver.set_page_load_timeout(timeout)
                driver.implicitly_wait(10)
                logger.info(f"ChromeDriver ì´ˆê¸°í™” ì„±ê³µ: {path} (íƒ€ì„ì•„ì›ƒ: {timeout}ì´ˆ)")
                return driver
        except Exception as e:
            continue
    
    # ì‹œìŠ¤í…œ ê¸°ë³¸ ê²½ë¡œ ì‹œë„
    try:
        driver = webdriver.Chrome(options=options)
        driver.set_page_load_timeout(timeout)
        driver.implicitly_wait(10)
        logger.info(f"ì‹œìŠ¤í…œ ê¸°ë³¸ ChromeDriver ì´ˆê¸°í™” ì„±ê³µ (íƒ€ì„ì•„ì›ƒ: {timeout}ì´ˆ)")
        return driver
    except Exception as e:
        logger.error(f"ChromeDriver ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise Exception("ChromeDriver ì´ˆê¸°í™” ì‹¤íŒ¨")

# =============================================================================
# ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ
# =============================================================================

def get_stove_post_content(post_url: str, driver: webdriver.Chrome = None, source: str = "", schedule_type: str = 'frequent') -> str:
    """ìŠ¤í† ë¸Œ ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ"""
    cache = load_content_cache()
    url_hash = get_url_hash(post_url)
    
    # ì£¼ê¸°ë³„ ìºì‹œ ì‹œê°„ ì°¨ë³„í™”
    cache_hours = config.Crawling.BUG_CACHE_HOURS if schedule_type == 'frequent' else config.Crawling.GENERAL_CACHE_HOURS
    
    if url_hash in cache:
        cached_item = cache[url_hash]
        try:
            cache_time = datetime.fromisoformat(cached_item.get('timestamp', '2000-01-01'))
            if datetime.now() - cache_time < timedelta(hours=cache_hours):
                return cached_item.get('content', "ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”.")
        except:
            pass
    
    driver_created = False
    if driver is None:
        try:
            driver = get_chrome_driver(schedule_type)
            driver_created = True
        except Exception as e:
            logger.error(f"Driver ìƒì„± ì‹¤íŒ¨: {e}")
            return "ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”."
    
    content_summary = "ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”."
    
    try:
        driver.get(post_url)
        
        # ì£¼ê¸°ë³„ ë¡œë”© ì‹œê°„ ìµœì í™”
        if schedule_type == 'frequent':
            time.sleep(8)
        else:
            time.sleep(10)
        
        WebDriverWait(driver, 15).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        
        # ì£¼ê¸°ë³„ ìŠ¤í¬ë¡¤ë§ ìµœì í™”
        scroll_count = 2 if schedule_type == 'frequent' else 3
        for i in range(scroll_count):
            driver.execute_script(f"window.scrollTo(0, {400 * (i + 1)});")
            time.sleep(2)
        
        driver.execute_script("window.scrollTo(0, 0);")
        time.sleep(2)
        
        # ì½˜í…ì¸  ì„ íƒìë“¤
        content_selectors = [
            'div.s-article-content',
            'div.s-article-content-text',
            'section.s-article-body',
            'div.s-board-content',
            'div.article-content',
            'div.post-content'
        ]
        
        extracted_content = ""
        for selector in content_selectors:
            try:
                elements = driver.find_elements(By.CSS_SELECTOR, selector)
                if elements:
                    for element in elements:
                        text = element.text.strip()
                        if text and len(text) > 30:
                            if not any(skip_text in text.lower() for skip_text in 
                                     ['install stove', 'ìŠ¤í† ë¸Œë¥¼ ì„¤ì¹˜', 'ë¡œê·¸ì¸ì´ í•„ìš”', 'javascript']):
                                extracted_content = text
                                break
                    if extracted_content:
                        break
            except Exception:
                continue
        
        if extracted_content:
            lines = extracted_content.split('\n')
            meaningful_lines = []
            for line in lines:
                line = line.strip()
                if (len(line) > 15 and 
                    not any(skip in line for skip in ['ë¡œê·¸ì¸', 'íšŒì›ê°€ì…', 'ë©”ë‰´', 'ê²€ìƒ‰', 'ê³µì§€ì‚¬í•­', 'ì´ë²¤íŠ¸', 'Install STOVE', 'ìŠ¤í† ë¸Œë¥¼ ì„¤ì¹˜'])):
                    meaningful_lines.append(line)
            
            if meaningful_lines:
                content_summary = extract_content_summary(meaningful_lines[0])
        
        # ìºì‹œ ì €ì¥
        cache[url_hash] = {
            'content': content_summary,
            'timestamp': datetime.now().isoformat(),
            'url': post_url
        }
        save_content_cache(cache)
        
    except TimeoutException:
        logger.error(f"í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ: {post_url}")
        content_summary = "â° ê²Œì‹œê¸€ ë¡œë”© ì‹œê°„ ì´ˆê³¼. ë§í¬ë¥¼ í´ë¦­í•˜ì—¬ í™•ì¸í•˜ì„¸ìš”."
    except Exception as e:
        logger.error(f"ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        content_summary = "ğŸ”— ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”."
    finally:
        if driver_created and driver:
            try:
                driver.quit()
            except:
                pass
    
    return content_summary

# =============================================================================
# â­ í•µì‹¬ ìˆ˜ì •: STOVE í¬ë¡¤ë§ í•¨ìˆ˜ (Force Crawl ì§€ì›)
# =============================================================================

@retry_on_failure(max_retries=2, delay=2.0)
def crawl_stove_board(source: str, site_config: Dict, schedule_type: str = 'frequent', force_crawl: bool = False):
    """ìŠ¤í† ë¸Œ ê²Œì‹œíŒ í¬ë¡¤ë§ (CSS Selector ë‹¤ì¤‘ í´ë°± + Force Crawl ì§€ì›)"""
    posts = []
    link_data = load_crawled_links()
    crawled_links = link_data["links"]
    
    logger.info(f"ğŸ”„ {site_config['site']} í¬ë¡¤ë§ ì‹œì‘ ({schedule_type}, force_crawl={force_crawl})")
    
    driver = None
    try:
        driver = get_chrome_driver(schedule_type)
        driver.get(site_config['url'])
        
        # ëŒ€ê¸°ì‹œê°„ ìµœì í™”
        if schedule_type == 'frequent':
            time.sleep(20)  # 15ë¶„ ê°„ê²©
        else:
            time.sleep(25)  # 30ë¶„ ê°„ê²©
        
        WebDriverWait(driver, 30).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        
        # ìŠ¤í¬ë¡¤ë§ ìµœì í™”
        scroll_count = 2 if schedule_type == 'frequent' else 3
        for i in range(scroll_count):
            driver.execute_script(f"window.scrollTo(0, {400 * (i + 1)});")
            time.sleep(3)
        
        driver.execute_script("window.scrollTo(0, 0);")
        time.sleep(3)
        
        # JavaScript ë‹¤ì¤‘ í´ë°± ì‹œìŠ¤í…œ
        user_posts = []
        try:
            user_posts = driver.execute_script(f"""
            var posts = [];
            
            // ë‹¤ì¤‘ CSS Selector í´ë°± ì‹œìŠ¤í…œ
            var containerSelectors = [
                'section.s-board-item',
                'div[class*="board-item"]',
                'article[class*="post"]',
                'div[class*="list-item"]',
                '[data-testid*="post"]',
                '.post-item',
                '.board-list-item',
                'li[class*="item"]',
                'div[class*="post"]'
            ];
            
            var items = [];
            for (var selector of containerSelectors) {{
                try {{
                    items = document.querySelectorAll(selector);
                    if (items && items.length > 0) {{
                        console.log('ì„±ê³µí•œ ì„ íƒì:', selector, 'ê°œìˆ˜:', items.length);
                        break;
                    }}
                }} catch(e) {{
                    continue;
                }}
            }}
            
            // ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ì „ì²´ ë§í¬ íƒìƒ‰
            if (!items || items.length === 0) {{
                console.log('ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì§€ ëª»í•¨, ì „ì²´ ë§í¬ íƒìƒ‰');
                var allLinks = document.querySelectorAll('a[href*="/view/"]');
                for (var i = 0; i < Math.min(allLinks.length, {site_config['limit']}); i++) {{
                    var link = allLinks[i];
                    if (link.href && link.innerText && link.innerText.trim().length > 3) {{
                        posts.push({{
                            title: link.innerText.trim(),
                            href: link.href,
                            id: link.href.split('/').pop()
                        }});
                    }}
                }}
                return posts;
            }}
            
            // ì •ìƒì ìœ¼ë¡œ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì€ ê²½ìš° ì²˜ë¦¬
            for (var i = 0; i < Math.min(items.length, {site_config['limit']}); i++) {{
                var item = items[i];
                
                // ë§í¬ ì¶”ì¶œ ë‹¤ì¤‘ í´ë°±
                var link = item.querySelector('a[href*="/view/"]') ||
                          item.querySelector('a[href*="/board/"]') ||
                          item.querySelector('a[href*="/post/"]') ||
                          item.querySelector('a[href]');
                
                // ì œëª© ì¶”ì¶œ ë‹¤ì¤‘ í´ë°±
                var titleSelectors = [
                    '.s-board-title-text',
                    '.board-title',
                    'h3 span',
                    '[class*="title"]',
                    'h1, h2, h3, h4, h5, h6',
                    '.post-title',
                    'span[class*="text"]',
                    '.text'
                ];
                
                var title = null;
                for (var titleSelector of titleSelectors) {{
                    try {{
                        var titleElement = item.querySelector(titleSelector);
                        if (titleElement && titleElement.innerText && titleElement.innerText.trim().length > 3) {{
                            title = titleElement;
                            break;
                        }}
                    }} catch(e) {{
                        continue;
                    }}
                }}
                
                // ì œëª©ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° ë§í¬ì˜ í…ìŠ¤íŠ¸ ì‚¬ìš©
                if (!title && link && link.innerText && link.innerText.trim().length > 3) {{
                    title = link;
                }}
                
                if (link && title && link.href && title.innerText) {{
                    var titleText = title.innerText.trim();
                    if (titleText.length > 3) {{
                        // ê³µì§€ì‚¬í•­/ì´ë²¤íŠ¸ ì œì™¸
                        var isNotice = item.querySelector('.notice, [class*="notice"], [class*="Notice"]');
                        var isEvent = item.querySelector('.event, [class*="event"], [class*="Event"]');
                        var isPinned = item.querySelector('[class*="pin"], [class*="top"], [class*="sticky"]');
                        
                        if (!isNotice && !isEvent && !isPinned) {{
                            posts.push({{
                                title: titleText,
                                href: link.href,
                                id: link.href.split('/').pop()
                            }});
                        }}
                    }}
                }}
            }}
            
            console.log('ìµœì¢… ì¶”ì¶œëœ ê²Œì‹œê¸€ ìˆ˜:', posts.length);
            return posts;
            """)
            
            logger.info(f"JavaScript í¬ë¡¤ë§ ì„±ê³µ: {len(user_posts)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")
            
        except Exception as js_error:
            logger.warning(f"JavaScript ì‹¤í–‰ ì‹¤íŒ¨: {js_error}")
            # BeautifulSoup ë°±ì—… í¬ë¡¤ë§
            logger.info("BeautifulSoup ë°±ì—… í¬ë¡¤ë§ ì‹œì‘")
            try:
                html = driver.page_source
                soup = BeautifulSoup(html, 'html.parser')
                
                # ë‹¤ì–‘í•œ ë§í¬ íŒ¨í„´ìœ¼ë¡œ íƒìƒ‰
                link_patterns = [
                    'a[href*="/view/"]',
                    'a[href*="/board/"]', 
                    'a[href*="/post/"]'
                ]
                
                found_links = []
                for pattern in link_patterns:
                    links = soup.select(pattern)
                    if links:
                        found_links = links[:site_config['limit']]
                        logger.info(f"BeautifulSoupë¡œ {pattern} íŒ¨í„´ì—ì„œ {len(found_links)}ê°œ ë§í¬ ë°œê²¬")
                        break
                
                for link in found_links:
                    title = link.get_text(strip=True)
                    href = link.get('href')
                    if title and href and len(title) > 3:
                        if href.startswith('/'):
                            href = 'https://page.onstove.com' + href
                        user_posts.append({
                            'title': title,
                            'href': href, 
                            'id': href.split('/')[-1]
                        })
                        
            except Exception as soup_error:
                logger.error(f"BeautifulSoup ë°±ì—…ë„ ì‹¤íŒ¨: {soup_error}")
                user_posts = []
        
        # â­ í•µì‹¬ ìˆ˜ì •: Force Crawl ì²˜ë¦¬
        for i, post_info in enumerate(user_posts, 1):
            try:
                href = fix_stove_url(post_info['href'])
                title = post_info['title']
                
                # â­ ì¤‘ë³µ ì²´í¬ ìˆ˜ì •: force_crawl=Trueë©´ ì¤‘ë³µ ì²´í¬ ìŠ¤í‚µ
                if not force_crawl and href in crawled_links:
                    logger.debug(f"ì¤‘ë³µ ê²Œì‹œê¸€ ìŠ¤í‚µ: {title[:30]}...")
                    continue
                
                if title and href and len(title) > 3:
                    content = get_stove_post_content(href, driver, source, schedule_type)
                    
                    post_data = {
                        "title": title,
                        "url": href,
                        "content": content,
                        "timestamp": datetime.now().isoformat(),
                        "source": source,
                        "site": site_config['site'],
                        "language": site_config['language'],
                        "priority": site_config['priority'],
                        "schedule_type": schedule_type,
                        "is_realtime": source in config.Crawling.REALTIME_ALERT_SOURCES
                    }
                    
                    posts.append(post_data)
                    crawled_links.append(href)
                    
                    # Force Crawl ëª¨ë“œì— ë”°ë¥¸ ë¡œê·¸ êµ¬ë¶„
                    if force_crawl:
                        logger.info(f"ğŸ”„ {site_config['site']} Force Crawl: {title[:50]}...")
                    else:
                        logger.info(f"âœ… {site_config['site']} ìƒˆ ê²Œì‹œê¸€: {title[:50]}...")
                    
                    # ì£¼ê¸°ë³„ ì§€ì—° ì‹œê°„ ìµœì í™”
                    if schedule_type == 'frequent':
                        time.sleep(get_random_delay(1, 2))
                    else:
                        time.sleep(get_random_delay(2, 3))
                    
                    # ì‹¤ì‹œê°„ ì•Œë¦¼ ì†ŒìŠ¤ëŠ” 5ê°œ ì´ìƒ ë°œê²¬ì‹œ ì¦‰ì‹œ ë°˜í™˜
                    if source in config.Crawling.REALTIME_ALERT_SOURCES and len(posts) >= 5:
                        break
                        
            except Exception as e:
                logger.error(f"{site_config['site']} ê²Œì‹œê¸€ {i} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
                
    except Exception as e:
        logger.error(f"{site_config['site']} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    finally:
        if driver:
            driver.quit()
    
    # ë§í¬ ë°ì´í„° ì €ì¥
    link_data["links"] = crawled_links
    save_crawled_links(link_data)
    
    return posts

# =============================================================================
# ê¸°íƒ€ í¬ë¡¤ë§ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ì½”ë“œ ë³´ì¡´)
# =============================================================================

@retry_on_failure(max_retries=2, delay=2.0)
def crawl_ruliweb_board():
    """ë£¨ë¦¬ì›¹ ì—í”½ì„¸ë¸ ê²Œì‹œíŒ í¬ë¡¤ë§ (30ë¶„ ê°„ê²©)"""
    posts = []
    link_data = load_crawled_links()
    crawled_links = link_data["links"]
    
    logger.info("ğŸŒ ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì‹œì‘ (30ë¶„ ê°„ê²©)")
    
    driver = None
    try:
        driver = get_chrome_driver('regular')
        site_config = config.Crawling.REGULAR_SOURCES['ruliweb_epic7']
        driver.get(site_config['url'])
        time.sleep(10)
        
        selectors = [
            ".subject_link",
            ".table_body .subject a",
            "td.subject a",
            "a[href*='/read/']",
            ".board_list_table .subject_link",
            "table tr td a[href*='read']"
        ]
        
        articles = []
        for selector in selectors:
            try:
                articles = driver.find_elements(By.CSS_SELECTOR, selector)
                if articles:
                    logger.info(f"ë£¨ë¦¬ì›¹ ì„ íƒì ì„±ê³µ: {selector} ({len(articles)}ê°œ)")
                    break
            except NoSuchElementException:
                continue
        
        if not articles:
            logger.warning("ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return posts
        
        # ê²Œì‹œê¸€ ì²˜ë¦¬
        for i, article in enumerate(articles[:site_config['limit']]):
            try:
                title = article.text.strip()
                link = article.get_attribute("href")
                
                if not title or not link or len(title) < 3:
                    continue
                
                if any(keyword in title for keyword in ['ê³µì§€', 'í•„ë…', 'ì´ë²¤íŠ¸', 'ì¶”ì²œ', 'ë² ìŠ¤íŠ¸', 'ê³µì§€ì‚¬í•­']):
                    continue
                
                if link.startswith('/'):
                    link = 'https://bbs.ruliweb.com' + link
                
                if link not in crawled_links:
                    post_data = {
                        "title": title,
                        "url": link,
                        "content": "ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”.",
                        "timestamp": datetime.now().isoformat(),
                        "source": "ruliweb_epic7",
                        "site": site_config['site'],
                        "language": site_config['language'],
                        "priority": site_config['priority'],
                        "schedule_type": "regular",
                        "is_realtime": False
                    }
                    
                    posts.append(post_data)
                    crawled_links.append(link)
                    
                    logger.info(f"ğŸ“° ë£¨ë¦¬ì›¹ ìƒˆ ê²Œì‹œê¸€: {title[:50]}...")
                    
            except Exception as e:
                logger.error(f"ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ {i+1} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
                
    except Exception as e:
        logger.error(f"ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    finally:
        if driver:
            driver.quit()
    
    # ë§í¬ ë°ì´í„° ì €ì¥
    link_data["links"] = crawled_links
    save_crawled_links(link_data)
    
    return posts

@retry_on_failure(max_retries=2, delay=2.0)
def crawl_reddit_board():
    """Reddit r/EpicSeven ìµœì‹ ê¸€ í¬ë¡¤ë§ (30ë¶„ ê°„ê²©)"""
    posts = []
    link_data = load_crawled_links()
    crawled_links = link_data["links"]
    
    logger.info("ğŸŒ Reddit í¬ë¡¤ë§ ì‹œì‘ (30ë¶„ ê°„ê²©)")
    
    try:
        site_config = config.Crawling.REGULAR_SOURCES['reddit_epic7']
        url = site_config['url']
        headers = {
            "User-Agent": get_random_user_agent(),
            "Accept": "application/json"
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        data = response.json()
        
        if 'data' in data and 'children' in data['data']:
            for child in data['data']['children']:
                try:
                    item = child['data']
                    title = item.get('title', '').strip()
                    permalink = "https://www.reddit.com" + item.get('permalink', '')
                    
                    if not title or not permalink or len(title) < 3:
                        continue
                    
                    if permalink in crawled_links:
                        continue
                    
                    post_data = {
                        "title": title,
                        "url": permalink,
                        "content": f"Reddit ê²Œì‹œê¸€: {title[:100]}...",
                        "timestamp": datetime.now().isoformat(),
                        "source": "reddit_epic7",
                        "site": site_config['site'],
                        "language": site_config['language'],
                        "priority": site_config['priority'],
                        "schedule_type": "regular",
                        "is_realtime": False
                    }
                    
                    posts.append(post_data)
                    crawled_links.append(permalink)
                    
                    logger.info(f"ğŸ“° Reddit ìƒˆ ê²Œì‹œê¸€: {title[:50]}...")
                    
                except Exception as e:
                    logger.error(f"Reddit ê²Œì‹œê¸€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
                    
    except requests.RequestException as e:
        logger.error(f"Reddit API ìš”ì²­ ì‹¤íŒ¨: {e}")
    except Exception as e:
        logger.error(f"Reddit í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    # ë§í¬ ë°ì´í„° ì €ì¥
    link_data["links"] = crawled_links
    save_crawled_links(link_data)
    
    return posts

# =============================================================================
# â­ ìˆ˜ì •: ì£¼ê¸°ë³„ í†µí•© í¬ë¡¤ë§ ì‹¤í–‰ (Force Crawl ì§€ì›)
# =============================================================================

def crawl_frequent_sites(force_crawl=False):
    """15ë¶„ ê°„ê²© í¬ë¡¤ë§ (ë²„ê·¸ ê²Œì‹œíŒ)"""
    logger.info(f"ğŸ”¥ === 15ë¶„ ê°„ê²© í¬ë¡¤ë§ ì‹œì‘ (ë²„ê·¸ ê²Œì‹œíŒ, force_crawl={force_crawl}) ===")
    
    frequent_posts = []
    
    # ë²„ê·¸ ê²Œì‹œíŒ ë³‘ë ¬ í¬ë¡¤ë§
    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        futures = {}
        
        for source, site_config in config.Crawling.FREQUENT_SOURCES.items():
            futures[executor.submit(crawl_stove_board, source, site_config, 'frequent', force_crawl)] = source
        
        for future in concurrent.futures.as_completed(futures, timeout=90):
            source = futures[future]
            try:
                posts = future.result()
                if posts:
                    frequent_posts.extend(posts)
                    logger.info(f"âœ… {source}: {len(posts)}ê°œ (15ë¶„ ê°„ê²©, force_crawl={force_crawl})")
                else:
                    logger.info(f"â­• {source}: ìƒˆ ê²Œì‹œê¸€ ì—†ìŒ")
            except Exception as e:
                logger.error(f"âŒ {source} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    logger.info(f"ğŸ”¥ 15ë¶„ ê°„ê²© í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(frequent_posts)}ê°œ")
    return frequent_posts

def crawl_regular_sites(force_crawl=False):
    """30ë¶„ ê°„ê²© í¬ë¡¤ë§ (ì¼ë°˜ ê²Œì‹œíŒ)"""
    logger.info(f"ğŸ“ === 30ë¶„ ê°„ê²© í¬ë¡¤ë§ ì‹œì‘ (ì¼ë°˜ ê²Œì‹œíŒ, force_crawl={force_crawl}) ===")
    
    regular_posts = []
    
    # ìŠ¤í† ë¸Œ ì¼ë°˜ ê²Œì‹œíŒ í¬ë¡¤ë§
    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        futures = {}
        
        for source, site_config in config.Crawling.REGULAR_SOURCES.items():
            if source in ['stove_general', 'stove_global_general']:
                futures[executor.submit(crawl_stove_board, source, site_config, 'regular', force_crawl)] = source
        
        for future in concurrent.futures.as_completed(futures, timeout=120):
            source = futures[future]
            try:
                posts = future.result()
                if posts:
                    regular_posts.extend(posts)
                    logger.info(f"âœ… {source}: {len(posts)}ê°œ (30ë¶„ ê°„ê²©, force_crawl={force_crawl})")
                else:
                    logger.info(f"â­• {source}: ìƒˆ ê²Œì‹œê¸€ ì—†ìŒ")
            except Exception as e:
                logger.error(f"âŒ {source} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    # ì»¤ë®¤ë‹ˆí‹° ì‚¬ì´íŠ¸ í¬ë¡¤ë§ (force_crawl ì˜í–¥ ì—†ìŒ)
    try:
        ruliweb_posts = crawl_ruliweb_board()
        if ruliweb_posts:
            regular_posts.extend(ruliweb_posts)
            logger.info(f"âœ… ruliweb_epic7: {len(ruliweb_posts)}ê°œ (30ë¶„ ê°„ê²©)")
    except Exception as e:
        logger.error(f"âŒ ruliweb_epic7 í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    try:
        reddit_posts = crawl_reddit_board()
        if reddit_posts:
            regular_posts.extend(reddit_posts)
            logger.info(f"âœ… reddit_epic7: {len(reddit_posts)}ê°œ (30ë¶„ ê°„ê²©)")
    except Exception as e:
        logger.error(f"âŒ reddit_epic7 í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    logger.info(f"ğŸ“ 30ë¶„ ê°„ê²© í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(regular_posts)}ê°œ")
    return regular_posts

def crawl_by_schedule(force_crawl=False):
    """ìŠ¤ì¼€ì¤„ì— ë”°ë¥¸ í¬ë¡¤ë§ ì‹¤í–‰"""
    logger.info(f"ğŸ“… === ìŠ¤ì¼€ì¤„ ê¸°ë°˜ í¬ë¡¤ë§ ì‹œì‘ (force_crawl={force_crawl}) ===")
    
    all_posts = []
    
    # 15ë¶„ ê°„ê²© ì²´í¬ (ë²„ê·¸ ê²Œì‹œíŒ)
    if is_frequent_schedule():
        frequent_posts = crawl_frequent_sites(force_crawl)
        all_posts.extend(frequent_posts)
    
    # 30ë¶„ ê°„ê²© ì²´í¬ (ì¼ë°˜ ê²Œì‹œíŒ)
    if is_regular_schedule():
        regular_posts = crawl_regular_sites(force_crawl)
        all_posts.extend(regular_posts)
    
    # ìŠ¤ì¼€ì¤„ ì™¸ ìˆ˜ë™ ì‹¤í–‰ì‹œ ëª¨ë“  ì‚¬ì´íŠ¸ í¬ë¡¤ë§
    if not all_posts:
        logger.info("âš ï¸ ìŠ¤ì¼€ì¤„ ì™¸ ì‹¤í–‰ - ëª¨ë“  ì‚¬ì´íŠ¸ í¬ë¡¤ë§")
        frequent_posts = crawl_frequent_sites(force_crawl)
        regular_posts = crawl_regular_sites(force_crawl)
        all_posts.extend(frequent_posts)
        all_posts.extend(regular_posts)
    
    # ìš°ì„ ìˆœìœ„ë³„ ì •ë ¬
    all_posts.sort(key=lambda x: (x.get('priority', 99), x.get('timestamp', '')), reverse=True)
    
    logger.info(f"ğŸ“… === ìŠ¤ì¼€ì¤„ ê¸°ë°˜ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_posts)}ê°œ ===")
    return all_posts

# =============================================================================
# ë¦¬í¬íŠ¸ìš© ë°ì´í„° ìˆ˜ì§‘
# =============================================================================

def get_all_posts_for_report():
    """ì¼ê°„ ë¦¬í¬íŠ¸ìš© ê²Œì‹œê¸€ ìˆ˜ì§‘"""
    logger.info("ğŸ“Š ì¼ê°„ ë¦¬í¬íŠ¸ìš© ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
    
    cutoff_time = datetime.now() - timedelta(hours=24)
    cache = load_content_cache()
    recent_posts = []
    
    for url_hash, cached_item in cache.items():
        try:
            timestamp = datetime.fromisoformat(cached_item.get('timestamp', '2000-01-01'))
            if timestamp >= cutoff_time:
                post_data = {
                    'title': cached_item.get('title', ''),
                    'url': cached_item.get('url', ''),
                    'content': cached_item.get('content', ''),
                    'timestamp': cached_item.get('timestamp', ''),
                    'source': cached_item.get('source', 'unknown'),
                    'site': cached_item.get('site', 'unknown'),
                    'language': cached_item.get('language', 'unknown'),
                    'priority': cached_item.get('priority', 99),
                    'schedule_type': cached_item.get('schedule_type', 'regular'),
                    'is_realtime': cached_item.get('is_realtime', False)
                }
                recent_posts.append(post_data)
        except Exception as e:
            logger.error(f"ìºì‹œ í•­ëª© ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            continue
    
    logger.info(f"ğŸ“Š ì¼ê°„ ë¦¬í¬íŠ¸ìš© ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(recent_posts)}ê°œ")
    return recent_posts

# =============================================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# =============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ Epic7 ì£¼ê¸°ë³„ í¬ë¡¤ëŸ¬ v3.3 ì‹œì‘")
    
    try:
        # ìŠ¤ì¼€ì¤„ ê¸°ë°˜ í¬ë¡¤ë§
        all_posts = crawl_by_schedule()
        
        if all_posts:
            logger.info(f"ì´ {len(all_posts)}ê°œì˜ ìƒˆ ê²Œì‹œê¸€ì„ í¬ë¡¤ë§í–ˆìŠµë‹ˆë‹¤.")
            
            # ì£¼ê¸°ë³„ í†µê³„
            schedule_stats = {}
            for post in all_posts:
                schedule_type = post.get('schedule_type', 'unknown')
                if schedule_type not in schedule_stats:
                    schedule_stats[schedule_type] = 0
                schedule_stats[schedule_type] += 1
            
            logger.info("ì£¼ê¸°ë³„ í†µê³„:")
            for schedule_type, count in schedule_stats.items():
                schedule_name = {
                    'frequent': "15ë¶„ ê°„ê²©",
                    'regular': "30ë¶„ ê°„ê²©"
                }.get(schedule_type, schedule_type)
                logger.info(f"  {schedule_name}: {count}ê°œ")
            
            # ì‹¤ì‹œê°„ ì•Œë¦¼ ëŒ€ìƒ í†µê³„
            realtime_posts = [post for post in all_posts if post.get('is_realtime', False)]
            logger.info(f"ì‹¤ì‹œê°„ ì•Œë¦¼ ëŒ€ìƒ: {len(realtime_posts)}ê°œ")
            
            # ì‚¬ì´íŠ¸ë³„ í†µê³„
            site_stats = {}
            for post in all_posts:
                site = post.get('site', 'unknown')
                if site not in site_stats:
                    site_stats[site] = 0
                site_stats[site] += 1
            
            logger.info("ì‚¬ì´íŠ¸ë³„ í†µê³„:")
            for site, count in site_stats.items():
                logger.info(f"  {site}: {count}ê°œ")
        else:
            logger.info("ìƒˆë¡œìš´ ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        return all_posts
        
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

if __name__ == "__main__":
    main()
