import json
import os
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.keys import Keys
import time
from datetime import datetime, timedelta
import re
import random
import requests
import hashlib
from typing import Dict, List, Optional, Tuple

# ==================== ë¬¸ì œ 1 í•´ê²°: ì•ˆì „í•œ ì›Œí¬í”Œë¡œìš°ë³„ íŒŒì¼ ë¶„ë¦¬ ====================

def get_workflow_files():
    """ì›Œí¬í”Œë¡œìš°ë³„ íŒŒì¼ ì•ˆì „ ë¶„ë¦¬ - ë‹¨ìˆœí™”ëœ ë¡œì§"""
    workflow_name = os.environ.get('GITHUB_WORKFLOW', '').lower()
    
    # ê¸°ë³¸ íŒŒì¼ (ì‹¤ì œ ì›Œí¬í”Œë¡œìš°)
    base_links = "crawled_links.json"
    base_cache = "content_cache.json"
    
    # ë””ë²„ê·¸/í…ŒìŠ¤íŠ¸ ëª¨ë“œë§Œ ë¶„ë¦¬
    if 'debug' in workflow_name or 'test' in workflow_name:
        debug_links = "crawled_links_debug.json"
        debug_cache = "content_cache_debug.json"
        print(f"[DEBUG] ë””ë²„ê·¸ ëª¨ë“œ íŒŒì¼ ì‚¬ìš©: {debug_links}, {debug_cache}")
        return debug_links, debug_cache
    
    print(f"[INFO] ê¸°ë³¸ íŒŒì¼ ì‚¬ìš©: {base_links}, {base_cache}")
    return base_links, base_cache

# íŒŒì¼ ì„¤ì •
CRAWLED_LINKS_FILE, CONTENT_CACHE_FILE = get_workflow_files()

def load_crawled_links():
    """ì´ë¯¸ í¬ë¡¤ë§ëœ ë§í¬ë“¤ì„ ë¡œë“œ"""
    if os.path.exists(CRAWLED_LINKS_FILE):
        try:
            with open(CRAWLED_LINKS_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, list):
                    return {"links": data, "last_updated": datetime.now().isoformat()}
                return data
        except (json.JSONDecodeError, FileNotFoundError):
            print(f"[WARNING] {CRAWLED_LINKS_FILE} íŒŒì¼ ì½ê¸° ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±")
    return {"links": [], "last_updated": datetime.now().isoformat()}

def save_crawled_links(link_data):
    """í¬ë¡¤ë§ëœ ë§í¬ë“¤ì„ ì €ì¥ (ìµœëŒ€ 1000ê°œ ìœ ì§€)"""
    try:
        if len(link_data["links"]) > 1000:
            link_data["links"] = link_data["links"][-1000:]
        link_data["last_updated"] = datetime.now().isoformat()
        with open(CRAWLED_LINKS_FILE, 'w', encoding='utf-8') as f:
            json.dump(link_data, f, ensure_ascii=False, indent=2)
        print(f"[SUCCESS] ë§í¬ ì €ì¥: {len(link_data['links'])}ê°œ")
    except Exception as e:
        print(f"[ERROR] ë§í¬ ì €ì¥ ì‹¤íŒ¨: {e}")

def load_content_cache():
    """ê²Œì‹œê¸€ ë‚´ìš© ìºì‹œ ë¡œë“œ"""
    if os.path.exists(CONTENT_CACHE_FILE):
        try:
            with open(CONTENT_CACHE_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            print(f"[WARNING] {CONTENT_CACHE_FILE} íŒŒì¼ ì½ê¸° ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±")
    return {}

def save_content_cache(cache_data):
    """ê²Œì‹œê¸€ ë‚´ìš© ìºì‹œ ì €ì¥"""
    try:
        if len(cache_data) > 500:
            sorted_items = sorted(cache_data.items(), key=lambda x: x[1].get('timestamp', ''), reverse=True)
            cache_data = dict(sorted_items[:500])
        with open(CONTENT_CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        print(f"[SUCCESS] ìºì‹œ ì €ì¥: {len(cache_data)}ê°œ")
    except Exception as e:
        print(f"[ERROR] ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

def get_url_hash(url: str) -> str:
    """URLì˜ í•´ì‹œê°’ ìƒì„±"""
    return hashlib.md5(url.encode('utf-8')).hexdigest()

def extract_content_summary(content: str) -> str:
    """ê²Œì‹œê¸€ ë‚´ìš©ì„ í•œ ì¤„ë¡œ ìš”ì•½"""
    if not content or len(content.strip()) < 10:
        return "ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”."
    
    content = re.sub(r'\s+', ' ', content.strip())
    content = re.sub(r'[^\w\sê°€-í£.,!?]', '', content)
    sentences = re.split(r'[.!?]', content)
    first_sentence = sentences[0].strip() if sentences else content
    
    if len(first_sentence) > 100:
        first_sentence = first_sentence[:97] + '...'
    elif len(first_sentence) > 10:
        first_sentence = first_sentence + '...'
    
    return first_sentence if first_sentence else "ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”."

def fix_stove_url(url):
    """ìŠ¤í† ë¸Œ URL ìˆ˜ì • í•¨ìˆ˜"""
    if not url:
        return url
    
    if url.startswith('ttps://'):
        url = 'h' + url
        print(f"[FIX] URL ìˆ˜ì •ë¨: {url}")
    elif url.startswith('ttp://'):
        url = 'h' + url
        print(f"[FIX] URL ìˆ˜ì •ë¨: {url}")
    elif url.startswith('/'):
        url = 'https://page.onstove.com' + url
        print(f"[FIX] ìƒëŒ€ê²½ë¡œ URL ìˆ˜ì •ë¨: {url}")
    elif not url.startswith('http'):
        url = 'https://page.onstove.com' + ('/' if not url.startswith('/') else '') + url
        print(f"[FIX] ë¹„ì •ìƒ URL ìˆ˜ì •ë¨: {url}")
    
    return url

def get_chrome_driver():
    """Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™”"""
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--disable-extensions')
    options.add_argument('--disable-plugins')
    options.add_argument('--disable-images')
    options.add_argument('--window-size=1920,1080')
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
    ]
    options.add_argument(f'--user-agent={random.choice(user_agents)}')
    
    prefs = {
        'profile.default_content_setting_values': {
            'images': 2, 'plugins': 2, 'popups': 2,
            'geolocation': 2, 'notifications': 2, 'media_stream': 2,
        }
    }
    options.add_experimental_option('prefs', prefs)
    
    possible_paths = [
        '/usr/bin/chromedriver',
        '/usr/local/bin/chromedriver',
        '/snap/bin/chromium.chromedriver'
    ]
    
    for path in possible_paths:
        try:
            if os.path.exists(path):
                print(f"[DEBUG] ChromeDriver ì‹œë„: {path}")
                service = Service(path)
                driver = webdriver.Chrome(service=service, options=options)
                print(f"[DEBUG] ChromeDriver ì„±ê³µ: {path}")
                return driver
        except Exception as e:
            print(f"[DEBUG] ChromeDriver ì‹¤íŒ¨ {path}: {str(e)[:100]}...")
            continue
    
    try:
        print("[DEBUG] ì‹œìŠ¤í…œ ê¸°ë³¸ ChromeDriver ì‹œë„")
        driver = webdriver.Chrome(options=options)
        print("[DEBUG] ì‹œìŠ¤í…œ ê¸°ë³¸ ChromeDriver ì„±ê³µ")
        return driver
    except Exception as e:
        print(f"[DEBUG] ì‹œìŠ¤í…œ ê¸°ë³¸ ChromeDriver ì‹¤íŒ¨: {str(e)[:100]}...")
    
    try:
        print("[DEBUG] WebDriver Manager ì‹œë„")
        from webdriver_manager.chrome import ChromeDriverManager
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        print("[DEBUG] WebDriver Manager ì„±ê³µ")
        return driver
    except Exception as e:
        print(f"[DEBUG] WebDriver Manager ì‹¤íŒ¨: {str(e)[:100]}...")
    
    raise Exception("ëª¨ë“  ChromeDriver ì´ˆê¸°í™” ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

def check_discord_webhooks():
    """Discord ì›¹í›… í™˜ê²½ë³€ìˆ˜ í™•ì¸"""
    webhooks = {}
    
    # ë²„ê·¸ ì•Œë¦¼ ì›¹í›…
    bug_webhook = os.environ.get('DISCORD_WEBHOOK_BUG') or os.getenv('DISCORD_WEBHOOK_BUG')
    if bug_webhook:
        webhooks['bug'] = bug_webhook
        print("[INFO] ë²„ê·¸ ì•Œë¦¼ ì›¹í›… ì„¤ì • ì™„ë£Œ")
    else:
        print("[WARNING] DISCORD_WEBHOOK_BUG í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    # ê°ì„± ë™í–¥ ì›¹í›…
    sentiment_webhook = os.environ.get('DISCORD_WEBHOOK_SENTIMENT') or os.getenv('DISCORD_WEBHOOK_SENTIMENT')
    if sentiment_webhook:
        webhooks['sentiment'] = sentiment_webhook
        print("[INFO] ê°ì„± ë™í–¥ ì›¹í›… ì„¤ì • ì™„ë£Œ")
    else:
        print("[WARNING] DISCORD_WEBHOOK_SENTIMENT í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("[WARNING] ê°ì„± ë™í–¥ ì•Œë¦¼ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
    
    # ì¼ê°„ ë³´ê³ ì„œ ì›¹í›…
    report_webhook = os.environ.get('DISCORD_WEBHOOK_REPORT') or os.getenv('DISCORD_WEBHOOK_REPORT')
    if report_webhook:
        webhooks['report'] = report_webhook
        print("[INFO] ì¼ê°„ ë³´ê³ ì„œ ì›¹í›… ì„¤ì • ì™„ë£Œ")
    else:
        print("[WARNING] DISCORD_WEBHOOK_REPORT í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    return webhooks

def send_discord_message(webhook_url: str, message: str, title: str = "Epic7 ëª¨ë‹ˆí„°ë§"):
    """Discord ë©”ì‹œì§€ ì „ì†¡"""
    if not webhook_url:
        print("[WARNING] ì›¹í›… URLì´ ì—†ì–´ ë©”ì‹œì§€ ì „ì†¡ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return False
    
    try:
        data = {
            "embeds": [{
                "title": title,
                "description": message,
                "color": 0x00ff00,
                "timestamp": datetime.now().isoformat()
            }]
        }
        
        response = requests.post(webhook_url, json=data, timeout=10)
        if response.status_code == 204:
            print("[INFO] Discord ë©”ì‹œì§€ ì „ì†¡ ì„±ê³µ")
            return True
        else:
            print(f"[ERROR] Discord ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"[ERROR] Discord ë©”ì‹œì§€ ì „ì†¡ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

# ==================== ë¬¸ì œ 2 í•´ê²°: JavaScript ë™ì  ë¡œë”© ìµœì í™” ====================

def get_stove_post_content(post_url: str, driver: webdriver.Chrome = None) -> str:
    """ìŠ¤í† ë¸Œ ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ - ì™„ì „ ê°œì„ ëœ ë²„ì „"""
    
    # ìºì‹œ í™•ì¸
    cache = load_content_cache()
    url_hash = get_url_hash(post_url)
    
    if url_hash in cache:
        cached_item = cache[url_hash]
        cache_time = datetime.fromisoformat(cached_item.get('timestamp', '2000-01-01'))
        if datetime.now() - cache_time < timedelta(hours=24):
            print(f"[CACHE] ìºì‹œëœ ë‚´ìš© ì‚¬ìš©: {post_url}")
            return cached_item.get('content', "ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”.")
    
    # Driver ìƒì„± í™•ì¸
    driver_created = False
    if driver is None:
        try:
            driver = get_chrome_driver()
            driver_created = True
        except Exception as e:
            print(f"[ERROR] Driver ìƒì„± ì‹¤íŒ¨: {e}")
            return "ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”."
    
    content_summary = "ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”."
    
    try:
        print(f"[DEBUG] ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ ì‹œë„: {post_url}")
        
        # í˜ì´ì§€ ë¡œë“œ íƒ€ì„ì•„ì›ƒ ì¦ê°€
        driver.set_page_load_timeout(45)
        driver.get(post_url)
        
        # í™•ì¥ëœ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        print("[DEBUG] í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘... (45ì´ˆ)")
        time.sleep(20)  # ì´ˆê¸° ëŒ€ê¸° ì‹œê°„ ì¦ê°€
        
        # JavaScript ì™„ì „ ë¡œë”© í™•ì¸ - ëŒ€ê¸° ì‹œê°„ ì¦ê°€
        WebDriverWait(driver, 25).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        print("[DEBUG] JavaScript ë¡œë”© ì™„ë£Œ í™•ì¸")
        
        # ì¶”ê°€ ë™ì  ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸° - ë” ë§ì€ ìŠ¤í¬ë¡¤ê³¼ ëŒ€ê¸°
        driver.execute_script("window.scrollTo(0, 300);")
        time.sleep(8)
        driver.execute_script("window.scrollTo(0, 600);")
        time.sleep(8)
        driver.execute_script("window.scrollTo(0, 1000);")
        time.sleep(8)
        driver.execute_script("window.scrollTo(0, 0);")
        time.sleep(8)
        
        print("[DEBUG] í™•ì¥ëœ ë™ì  ì½˜í…ì¸  ë¡œë”© ì™„ë£Œ")
        
        # ìŠ¤í† ë¸Œ ì‚¬ì´íŠ¸ ìµœì‹  êµ¬ì¡° ëŒ€ì‘ ì„ íƒì - ëŒ€í­ í™•ì¥
        content_selectors = [
            # ìŠ¤í† ë¸Œ ê²Œì‹œê¸€ ì „ìš© ì„ íƒì (ìµœì‹  êµ¬ì¡°)
            'div.s-article-content',
            'div.s-article-content-text',
            'div[class*="s-article-content"]',
            'section.s-article-body',
            'div.s-board-content',
            'article.s-post-content',
            'div[data-content="true"]',
            'main .content-wrapper',
            '.post-content-area',
            '.article-body-content',
            
            # ì¼ë°˜ì ì¸ ê²Œì‹œê¸€ ì„ íƒì
            'div.article-content',
            'div.post-content',
            'div.content-body',
            'main.content',
            'article.content',
            
            # í…ìŠ¤íŠ¸ ì˜ì—­ ì„ íƒì
            'div[class*="text-content"]',
            'div[class*="post-body"]',
            'div[class*="article-body"]',
            'div[class*="content-text"]',
            'section[class*="content"]',
            
            # ë°±ì—… ì„ íƒì
            '.content',
            '[data-role="content"]',
            'main',
            'article'
        ]
        
        extracted_content = ""
        successful_selector = ""
        
        # ê° ì„ íƒìë¡œ ë‚´ìš© ì¶”ì¶œ ì‹œë„
        for selector in content_selectors:
            try:
                elements = driver.find_elements(By.CSS_SELECTOR, selector)
                if elements:
                    for element in elements:
                        text = element.text.strip()
                        if text and len(text) > 30:  # ìµœì†Œ ê¸¸ì´ ê°ì†Œ
                            # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ í•„í„°ë§ í™•ì¥
                            skip_texts = [
                                'install stove', 'ìŠ¤í† ë¸Œë¥¼ ì„¤ì¹˜', 'ë¡œê·¸ì¸ì´ í•„ìš”', 
                                'javascript', 'íšŒì›ê°€ì…', 'ë©”ë‰´', 'ê²€ìƒ‰', 'ê³µì§€ì‚¬í•­',
                                'header', 'footer', 'navigation', 'sidebar'
                            ]
                            if not any(skip_text in text.lower() for skip_text in skip_texts):
                                extracted_content = text
                                successful_selector = selector
                                print(f"[SUCCESS] ì„ íƒì '{selector}'ë¡œ ë‚´ìš© ì¶”ì¶œ ì„±ê³µ")
                                break
                    
                    if extracted_content:
                        break
                        
            except Exception as e:
                print(f"[DEBUG] ì„ íƒì '{selector}' ì‹¤íŒ¨: {e}")
                continue
        
        # ì¶”ì¶œëœ ë‚´ìš©ì´ ìˆìœ¼ë©´ ìš”ì•½ ì²˜ë¦¬
        if extracted_content:
            lines = extracted_content.split('\n')
            meaningful_lines = []
            
            for line in lines:
                line = line.strip()
                if (len(line) > 10 and  # ìµœì†Œ ê¸¸ì´ ê°ì†Œ
                    'ë¡œê·¸ì¸' not in line and 
                    'íšŒì›ê°€ì…' not in line and 
                    'ë©”ë‰´' not in line and 
                    'ê²€ìƒ‰' not in line and
                    'ê³µì§€ì‚¬í•­' not in line and
                    'ì´ë²¤íŠ¸' not in line and
                    'Install STOVE' not in line and
                    'ìŠ¤í† ë¸Œë¥¼ ì„¤ì¹˜' not in line and
                    'header' not in line.lower() and
                    'footer' not in line.lower()):
                    meaningful_lines.append(line)
            
            if meaningful_lines:
                # ê°€ì¥ ê¸´ ì¤„ì„ ê²Œì‹œê¸€ ë‚´ìš©ìœ¼ë¡œ ì„ íƒ
                longest_line = max(meaningful_lines, key=len)
                if len(longest_line) > 15:  # ìµœì†Œ ê¸¸ì´ ê°ì†Œ
                    content_summary = extract_content_summary(longest_line)
                    print(f"[SUCCESS] ì‹¤ì œ ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ ì„±ê³µ: {content_summary[:50]}...")
                else:
                    content_summary = extract_content_summary(meaningful_lines[0])
                    print(f"[SUCCESS] ì²« ë²ˆì§¸ ì˜ë¯¸ìˆëŠ” ë‚´ìš© ì¶”ì¶œ: {content_summary[:50]}...")
        
        # JavaScriptë¥¼ í†µí•œ ëŒ€ì²´ ì¶”ì¶œ ë°©ë²• - ê°œì„ ëœ ìŠ¤í¬ë¦½íŠ¸
        if content_summary == "ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”.":
            try:
                js_content = driver.execute_script("""
                    // ìŠ¤í† ë¸Œ ê²Œì‹œê¸€ ë‚´ìš©ì„ ì •í™•íˆ ì¶”ì¶œí•˜ëŠ” ê°œì„ ëœ JavaScript
                    var contentElements = [
                        document.querySelector('div.s-article-content'),
                        document.querySelector('div[class*="article-content"]'),
                        document.querySelector('div[class*="post-content"]'),
                        document.querySelector('article.s-post-content'),
                        document.querySelector('div[data-content="true"]'),
                        document.querySelector('main .content-wrapper'),
                        document.querySelector('.post-content-area'),
                        document.querySelector('.article-body-content'),
                        document.querySelector('main'),
                        document.querySelector('article')
                    ];
                    
                    for (var i = 0; i < contentElements.length; i++) {
                        var element = contentElements[i];
                        if (element && element.innerText) {
                            var text = element.innerText.trim();
                            if (text.length > 30 && 
                                !text.toLowerCase().includes('install stove') &&
                                !text.includes('ìŠ¤í† ë¸Œë¥¼ ì„¤ì¹˜') &&
                                !text.includes('ë¡œê·¸ì¸ì´ í•„ìš”') &&
                                !text.includes('íšŒì›ê°€ì…') &&
                                !text.toLowerCase().includes('header') &&
                                !text.toLowerCase().includes('footer')) {
                                return text;
                            }
                        }
                    }
                    
                    return '';
                """)
                
                if js_content and len(js_content.strip()) > 30:
                    content_summary = extract_content_summary(js_content)
                    print(f"[SUCCESS] JavaScript ë°©ì‹ìœ¼ë¡œ ë‚´ìš© ì¶”ì¶œ ì„±ê³µ: {content_summary[:50]}...")
                    
            except Exception as e:
                print(f"[ERROR] JavaScript ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        # ìµœí›„ ìˆ˜ë‹¨: BeautifulSoup ì‚¬ìš© - ì„ íƒì í™•ì¥
        if content_summary == "ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”.":
            try:
                html = driver.page_source
                soup = BeautifulSoup(html, 'html.parser')
                
                # ìŠ¤í† ë¸Œ ì „ìš© íƒœê·¸ë“¤ í™•ì¥
                stove_content_tags = [
                    soup.find('div', class_='s-article-content'),
                    soup.find('div', class_='s-article-content-text'),
                    soup.find('section', class_='s-article-body'),
                    soup.find('div', class_='s-board-content'),
                    soup.find('article', class_='s-post-content'),
                    soup.find('div', attrs={'data-content': 'true'}),
                    soup.find('main', class_='content-wrapper'),
                    soup.find('div', class_='post-content-area'),
                    soup.find('div', class_='article-body-content')
                ]
                
                for tag in stove_content_tags:
                    if tag:
                        text = tag.get_text(strip=True)
                        if text and len(text) > 30:
                            if not any(skip in text.lower() for skip in ['install stove', 'ìŠ¤í† ë¸Œë¥¼ ì„¤ì¹˜', 'header', 'footer']):
                                content_summary = extract_content_summary(text)
                                print(f"[SUCCESS] BeautifulSoupìœ¼ë¡œ ë‚´ìš© ì¶”ì¶œ ì„±ê³µ: {content_summary[:50]}...")
                                break
                                
            except Exception as e:
                print(f"[ERROR] BeautifulSoup ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        # ë””ë²„ê·¸ ì •ë³´ ì €ì¥
        debug_filename = f"debug_stove_{datetime.now().strftime('%H%M%S')}.html"
        with open(debug_filename, "w", encoding="utf-8") as f:
            f.write(driver.page_source)
        print(f"[DEBUG] ë””ë²„ê·¸ìš© HTML ì €ì¥: {debug_filename}")
        
        # ìºì‹œ ì €ì¥
        cache[url_hash] = {
            'content': content_summary,
            'timestamp': datetime.now().isoformat(),
            'url': post_url,
            'selector_used': successful_selector
        }
        save_content_cache(cache)
        
    except TimeoutException:
        print(f"[ERROR] í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ: {post_url}")
        content_summary = "â° ê²Œì‹œê¸€ ë¡œë”© ì‹œê°„ ì´ˆê³¼. ë§í¬ë¥¼ í´ë¦­í•˜ì—¬ í™•ì¸í•˜ì„¸ìš”."
    except Exception as e:
        print(f"[ERROR] ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        content_summary = "ğŸ”— ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”."
    finally:
        if driver_created and driver:
            try:
                driver.quit()
            except:
                pass
    
    return content_summary

# ==================== ê¸°ì¡´ í¬ë¡¤ë§ í•¨ìˆ˜ë“¤ ìœ ì§€ ====================

def fetch_ruliweb_epic7_board():
    """ë£¨ë¦¬ì›¹ ì—í”½ì„¸ë¸ ê²Œì‹œíŒ í¬ë¡¤ë§"""
    posts = []
    link_data = load_crawled_links()
    crawled_links = link_data["links"]
    
    print(f"[DEBUG] ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì‹œì‘ - ì €ì¥ëœ ë§í¬ ìˆ˜: {len(crawled_links)}")
    
    driver = None
    try:
        print("[DEBUG] ë£¨ë¦¬ì›¹ìš© Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì¤‘...")
        driver = get_chrome_driver()
        
        driver.set_page_load_timeout(30)
        driver.implicitly_wait(10)
        
        url = "https://bbs.ruliweb.com/game/84834"
        print(f"[DEBUG] ë£¨ë¦¬ì›¹ ì ‘ì† ì¤‘: {url}")
        
        driver.get(url)
        time.sleep(5)
        
        print("[DEBUG] ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ëª©ë¡ ê²€ìƒ‰ ì¤‘...")
        
        selectors = [
            ".subject_link",
            ".table_body .subject a",
            "td.subject a",
            "a[href*='/read/']",
            ".board_list_table .subject_link",
            "table tr td a[href*='read']"
        ]
        
        articles = []
        for selector in selectors:
            try:
                articles = driver.find_elements(By.CSS_SELECTOR, selector)
                if articles:
                    print(f"[DEBUG] ë£¨ë¦¬ì›¹ ì„ íƒì ì„±ê³µ: {selector} ({len(articles)}ê°œ)")
                    break
            except NoSuchElementException:
                continue
        
        if not articles:
            print("[WARNING] ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. HTML ì €ì¥...")
            with open("ruliweb_debug_selenium.html", "w", encoding="utf-8") as f:
                f.write(driver.page_source)
            return posts
        
        for i, article in enumerate(articles[:15]):
            try:
                title = article.text.strip()
                link = article.get_attribute("href")
                
                if not title or not link or len(title) < 3:
                    continue
                
                if any(keyword in title for keyword in ['ê³µì§€', 'í•„ë…', 'ì´ë²¤íŠ¸', 'ì¶”ì²œ', 'ë² ìŠ¤íŠ¸', 'ê³µì§€ì‚¬í•­']):
                    continue
                
                if link.startswith('/'):
                    link = 'https://bbs.ruliweb.com' + link
                
                if link not in crawled_links:
                    post_data = {
                        "title": title,
                        "url": link,
                        "content": "ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”.",
                        "timestamp": datetime.now().isoformat(),
                        "source": "ruliweb_epic7"
                    }
                    posts.append(post_data)
                    crawled_links.append(link)
                    print(f"[NEW] ë£¨ë¦¬ì›¹ ìƒˆ ê²Œì‹œê¸€ ({i+1}): {title[:50]}...")
                
            except Exception as e:
                print(f"[ERROR] ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ {i+1} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
        
        print(f"[DEBUG] ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì™„ë£Œ: {len(posts)}ê°œ ìƒˆ ê²Œì‹œê¸€")
        
    except Exception as e:
        print(f"[ERROR] ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        if driver:
            try:
                with open("ruliweb_error_debug.html", "w", encoding="utf-8") as f:
                    f.write(driver.page_source)
            except:
                pass
    
    finally:
        if driver:
            driver.quit()
    
    link_data["links"] = crawled_links
    save_crawled_links(link_data)
    
    return posts

def fetch_stove_bug_board():
    """ìŠ¤í† ë¸Œ ì—í”½ì„¸ë¸ ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§"""
    posts = []
    link_data = load_crawled_links()
    crawled_links = link_data["links"]
    
    print(f"[DEBUG] ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹œì‘ - ì €ì¥ëœ ë§í¬ ìˆ˜: {len(crawled_links)}")
    
    driver = None
    try:
        print("[DEBUG] ìŠ¤í† ë¸Œ ë²„ê·¸ìš© Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì¤‘...")
        driver = get_chrome_driver()
        
        driver.set_page_load_timeout(45)  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
        driver.implicitly_wait(15)
        
        url = "https://page.onstove.com/epicseven/kr/list/1012?page=1&direction=LATEST"
        print(f"[DEBUG] ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œíŒ ì ‘ì† ì¤‘: {url}")
        
        driver.get(url)
        
        print("[DEBUG] ìŠ¤í† ë¸Œ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘...")
        time.sleep(20)  # ì¦ê°€ëœ ëŒ€ê¸° ì‹œê°„
        
        # í˜ì´ì§€ ì™„ì „ ë¡œë”© í™•ì¸
        WebDriverWait(driver, 20).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        
        # ì¶”ê°€ ìŠ¤í¬ë¡¤ ë° ëŒ€ê¸°
        for i in range(5):
            driver.execute_script(f"window.scrollTo(0, {(i+1)*400});")
            time.sleep(6)
        
        driver.execute_script("window.scrollTo(0, 0);")
        time.sleep(8)
        
        html_content = driver.page_source
        with open("stove_bug_debug_selenium.html", "w", encoding="utf-8") as f:
            f.write(html_content)
        
        # ê°œì„ ëœ JavaScriptë¡œ ê²Œì‹œê¸€ ì¶”ì¶œ
        user_posts = driver.execute_script("""
            var posts = [];
            var items = document.querySelectorAll('section.s-board-item');
            
            console.log('ë°œê²¬ëœ ì „ì²´ ì„¹ì…˜:', items.length);
            
            for (var i = 0; i < Math.min(items.length, 15); i++) {
                var item = items[i];
                var link = item.querySelector('a[href*="/view/"]');
                var title = item.querySelector('.s-board-title-text, .board-title, h3 span, .title');
                
                if (link && title && link.href && title.innerText) {
                    var titleText = title.innerText.trim();
                    if (titleText.length > 3) {
                        // ê³µì§€/ì´ë²¤íŠ¸ ì œì™¸
                        var isNotice = item.querySelector('.notice, [class*="notice"]');
                        var isEvent = item.querySelector('.event, [class*="event"]');
                        
                        if (!isNotice && !isEvent) {
                            posts.push({
                                title: titleText,
                                href: link.href,
                                id: link.href.split('/').pop()
                            });
                            console.log('ê²Œì‹œê¸€ ì¶”ê°€:', titleText.substring(0, 30));
                        }
                    }
                }
            }
            
            console.log('ìµœì¢… ì¶”ì¶œëœ ê²Œì‹œê¸€ ìˆ˜:', posts.length);
            return posts;
        """)
        
        print(f"[DEBUG] JavaScriptë¡œ {len(user_posts)}ê°œ ìœ ì € ê²Œì‹œê¸€ ë°œê²¬")
        
        for i, post_info in enumerate(user_posts, 1):
            try:
                href = post_info['href']
                title = post_info['title']
                
                href = fix_stove_url(href)
                
                print(f"[DEBUG] ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œê¸€ {i}: URL = {href}")
                print(f"[DEBUG] ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œê¸€ {i}: ì œëª© = {title[:50]}...")
                
                if href in crawled_links:
                    print(f"[DEBUG] ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œê¸€ {i}: ì´ë¯¸ í¬ë¡¤ë§ëœ ë§í¬")
                    continue
                
                if title and href and len(title) > 3:
                    # ê°œì„ ëœ ë‚´ìš© ì¶”ì¶œ í•¨ìˆ˜ ì‚¬ìš©
                    content = get_stove_post_content(href, driver)
                    
                    post_data = {
                        "title": title,
                        "url": href,
                        "content": content,
                        "timestamp": datetime.now().isoformat(),
                        "source": "stove_bug"
                    }
                    posts.append(post_data)
                    crawled_links.append(href)
                    print(f"[NEW] ìŠ¤í† ë¸Œ ë²„ê·¸ ìƒˆ ê²Œì‹œê¸€ ë°œê²¬ ({i}): {title[:50]}...")
                    print(f"[CONTENT] ë‚´ìš©: {content[:100]}...")
                else:
                    print(f"[DEBUG] ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œê¸€ {i}: ì¡°ê±´ ë¯¸ì¶©ì¡±")
                
                time.sleep(random.uniform(3, 6))  # ì¦ê°€ëœ ëŒ€ê¸° ì‹œê°„
                
            except Exception as e:
                print(f"[ERROR] ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œê¸€ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        print(f"[DEBUG] ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œíŒ ì²˜ë¦¬ ê²°ê³¼: {len(user_posts)}ê°œ ì¤‘ ìƒˆ ê²Œì‹œê¸€ {len(posts)}ê°œ ë°œê²¬")
        
    except Exception as e:
        print(f"[ERROR] ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
    finally:
        if driver:
            print("[DEBUG] ìŠ¤í† ë¸Œ ë²„ê·¸ Chrome ë“œë¼ì´ë²„ ì¢…ë£Œ ì¤‘...")
            driver.quit()
    
    link_data["links"] = crawled_links
    save_crawled_links(link_data)
    
    print(f"[DEBUG] ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œíŒì—ì„œ {len(posts)}ê°œ ìƒˆ ê²Œì‹œê¸€ ë°œê²¬")
    return posts

def fetch_stove_general_board():
    """ìŠ¤í† ë¸Œ ì—í”½ì„¸ë¸ ììœ ê²Œì‹œíŒ í¬ë¡¤ë§"""
    posts = []
    link_data = load_crawled_links()
    crawled_links = link_data["links"]
    
    print(f"[DEBUG] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹œì‘ - ì €ì¥ëœ ë§í¬ ìˆ˜: {len(crawled_links)}")
    
    driver = None
    try:
        print("[DEBUG] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒìš© Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì¤‘...")
        driver = get_chrome_driver()
        
        driver.set_page_load_timeout(45)  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
        driver.implicitly_wait(15)
        
        url = "https://page.onstove.com/epicseven/kr/list/1005?page=1&direction=LATEST"
        print(f"[DEBUG] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ ì ‘ì† ì¤‘: {url}")
        
        driver.get(url)
        
        print("[DEBUG] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘...")
        time.sleep(20)  # ì¦ê°€ëœ ëŒ€ê¸° ì‹œê°„
        
        # í˜ì´ì§€ ì™„ì „ ë¡œë”© í™•ì¸
        WebDriverWait(driver, 20).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        
        # ì¶”ê°€ ìŠ¤í¬ë¡¤ ë° ëŒ€ê¸°
        for i in range(5):
            driver.execute_script(f"window.scrollTo(0, {(i+1)*400});")
            time.sleep(6)
        
        driver.execute_script("window.scrollTo(0, 0);")
        time.sleep(8)
        
        html_content = driver.page_source
        with open("stove_general_debug_selenium.html", "w", encoding="utf-8") as f:
            f.write(html_content)
        
        # ë™ì¼í•œ ê°œì„ ëœ JavaScript ì‚¬ìš©
        user_posts = driver.execute_script("""
            var posts = [];
            var items = document.querySelectorAll('section.s-board-item');
            
            console.log('ììœ ê²Œì‹œíŒ ì „ì²´ ì„¹ì…˜:', items.length);
            
            for (var i = 0; i < Math.min(items.length, 15); i++) {
                var item = items[i];
                var link = item.querySelector('a[href*="/view/"]');
                var title = item.querySelector('.s-board-title-text, .board-title, h3 span, .title');
                
                if (link && title && link.href && title.innerText) {
                    var titleText = title.innerText.trim();
                    if (titleText.length > 3) {
                        var isNotice = item.querySelector('.notice, [class*="notice"]');
                        var isEvent = item.querySelector('.event, [class*="event"]');
                        
                        if (!isNotice && !isEvent) {
                            posts.push({
                                title: titleText,
                                href: link.href,
                                id: link.href.split('/').pop()
                            });
                            console.log('ììœ ê²Œì‹œíŒ ê²Œì‹œê¸€ ì¶”ê°€:', titleText.substring(0, 30));
                        }
                    }
                }
            }
            
            console.log('ììœ ê²Œì‹œíŒ ìµœì¢… ì¶”ì¶œ ìˆ˜:', posts.length);
            return posts;
        """)
        
        print(f"[DEBUG] JavaScriptë¡œ {len(user_posts)}ê°œ ììœ ê²Œì‹œíŒ ê²Œì‹œê¸€ ë°œê²¬")
        
        for i, post_info in enumerate(user_posts, 1):
            try:
                href = post_info['href']
                title = post_info['title']
                
                href = fix_stove_url(href)
                
                print(f"[DEBUG] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ ê²Œì‹œê¸€ {i}: URL = {href}")
                print(f"[DEBUG] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ ê²Œì‹œê¸€ {i}: ì œëª© = {title[:50]}...")
                
                if href in crawled_links:
                    print(f"[DEBUG] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ ê²Œì‹œê¸€ {i}: ì´ë¯¸ í¬ë¡¤ë§ëœ ë§í¬")
                    continue
                
                if not title or len(title) < 4:
                    print(f"[DEBUG] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ ê²Œì‹œê¸€ {i}: ì¡°ê±´ ë¯¸ì¶©ì¡±")
                    continue
                
                meaningless_patterns = [
                    r'^[.]{3,}$',
                    r'^[ã…‹ã…ã…—ã…œã…‘]{3,}$',
                    r'^[!@#$%^&*()]{3,}$',
                ]
                
                is_meaningless = any(re.match(pattern, title) for pattern in meaningless_patterns)
                if is_meaningless:
                    print(f"[DEBUG] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ ê²Œì‹œê¸€ {i}: ì¡°ê±´ ë¯¸ì¶©ì¡±")
                    continue
                
                # ê°œì„ ëœ ë‚´ìš© ì¶”ì¶œ í•¨ìˆ˜ ì‚¬ìš©
                content = get_stove_post_content(href, driver)
                
                post_data = {
                    "title": title,
                    "url": href,
                    "content": content,
                    "timestamp": datetime.now().isoformat(),
                    "source": "stove_general"
                }
                posts.append(post_data)
                crawled_links.append(href)
                print(f"[NEW] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ ê²Œì‹œê¸€ ë°œê²¬ ({i}): {title[:50]}...")
                print(f"[CONTENT] ë‚´ìš©: {content[:100]}...")
                
                time.sleep(random.uniform(3, 6))  # ì¦ê°€ëœ ëŒ€ê¸° ì‹œê°„
                
            except Exception as e:
                print(f"[ERROR] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ ê²Œì‹œê¸€ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        print(f"[DEBUG] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ ì²˜ë¦¬ ì™„ë£Œ: {len(posts)}ê°œ ìƒˆ ê²Œì‹œê¸€")
        
    except Exception as e:
        print(f"[ERROR] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
    finally:
        if driver:
            print("[DEBUG] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ Chrome ë“œë¼ì´ë²„ ì¢…ë£Œ ì¤‘...")
            driver.quit()
    
    link_data["links"] = crawled_links
    save_crawled_links(link_data)
    
    print(f"[DEBUG] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒì—ì„œ {len(posts)}ê°œ ìƒˆ ê²Œì‹œê¸€ ë°œê²¬")
    return posts

def process_sentiment_posts(posts):
    """ê°ì„± ë¶„ì„ëœ ê²Œì‹œê¸€ ì²˜ë¦¬ ë° Discord ì•Œë¦¼"""
    webhooks = check_discord_webhooks()
    
    if not posts:
        print("[INFO] ì²˜ë¦¬í•  ê°ì„± ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    sentiment_webhook = webhooks.get('sentiment')
    if not sentiment_webhook:
        print(f"[WARNING] ê°ì„± ë™í–¥ {len(posts)}ê°œ ìˆì§€ë§Œ ì›¹í›… ë¯¸ì„¤ì •ìœ¼ë¡œ ì•Œë¦¼ ìƒëµ")
        return
    
    # ê°ì„±ë³„ ë¶„ë¥˜
    positive_posts = [p for p in posts if p.get('sentiment') == 'positive']
    neutral_posts = [p for p in posts if p.get('sentiment') == 'neutral']
    negative_posts = [p for p in posts if p.get('sentiment') == 'negative']
    
    if positive_posts or neutral_posts or negative_posts:
        message = f"ğŸ“Š **ìœ ì € ë™í–¥ ë¶„ì„ ê²°ê³¼**\n\n"
        message += f"ğŸ˜Š ê¸ì •: {len(positive_posts)}ê°œ\n"
        message += f"ğŸ˜ ì¤‘ë¦½: {len(neutral_posts)}ê°œ\n"
        message += f"ğŸ˜  ë¶€ì •: {len(negative_posts)}ê°œ\n\n"
        
        # ìƒ˜í”Œ ê²Œì‹œê¸€ ì¶”ê°€
        if negative_posts:
            message += "**ì£¼ìš” ë¶€ì • ë°˜ì‘:**\n"
            for post in negative_posts[:3]:
                message += f"â€¢ {post['title'][:50]}...\n"
        
        send_discord_message(sentiment_webhook, message, "Epic7 ìœ ì € ë™í–¥")
        print(f"[INFO] ê°ì„± ë™í–¥ {len(posts)}ê°œ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ")

def crawl_korean_sites():
    """í•œêµ­ ì‚¬ì´íŠ¸ë“¤ í¬ë¡¤ë§"""
    all_posts = []
    
    # í™˜ê²½ë³€ìˆ˜ í™•ì¸
    webhooks = check_discord_webhooks()
    
    try:
        print("[INFO] === í•œêµ­ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹œì‘ ===")
        
        print("[INFO] 1/3 ë£¨ë¦¬ì›¹ ì—í”½ì„¸ë¸ ê²Œì‹œíŒ í¬ë¡¤ë§")
        ruliweb_posts = fetch_ruliweb_epic7_board()
        all_posts.extend(ruliweb_posts)
        print(f"[INFO] ë£¨ë¦¬ì›¹: {len(ruliweb_posts)}ê°œ ìƒˆ ê²Œì‹œê¸€")
        
        time.sleep(random.uniform(8, 12))  # ëŒ€ê¸° ì‹œê°„ ì¦ê°€
        
        print("[INFO] 2/3 ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§")
        stove_bug_posts = fetch_stove_bug_board()
        all_posts.extend(stove_bug_posts)
        print(f"[INFO] ìŠ¤í† ë¸Œ ë²„ê·¸: {len(stove_bug_posts)}ê°œ ìƒˆ ê²Œì‹œê¸€")
        
        time.sleep(random.uniform(8, 12))  # ëŒ€ê¸° ì‹œê°„ ì¦ê°€
        
        print("[INFO] 3/3 ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ í¬ë¡¤ë§")
        stove_general_posts = fetch_stove_general_board()
        all_posts.extend(stove_general_posts)
        print(f"[INFO] ìŠ¤í† ë¸Œ ììœ : {len(stove_general_posts)}ê°œ ìƒˆ ê²Œì‹œê¸€")
        
        # ê°ì„± ë¶„ì„ ê²°ê³¼ê°€ ìˆë‹¤ë©´ ì•Œë¦¼ ì „ì†¡
        sentiment_posts = [p for p in all_posts if p.get('sentiment')]
        if sentiment_posts:
            process_sentiment_posts(sentiment_posts)
        
    except Exception as e:
        print(f"[ERROR] í•œêµ­ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    print(f"[INFO] === í•œêµ­ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_posts)}ê°œ ìƒˆ ê²Œì‹œê¸€ ===")
    return all_posts

def crawl_global_sites():
    """ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ë“¤ í¬ë¡¤ë§"""
    print("[DEBUG] ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ì€ ì•„ì§ êµ¬í˜„ë˜ì§€ ì•ŠìŒ")
    return []

def get_all_posts_for_report():
    """ì¼ì¼ ë¦¬í¬íŠ¸ìš© - ìƒˆ ê²Œì‹œê¸€ ìˆ˜ì§‘"""
    print("[INFO] ì¼ì¼ ë¦¬í¬íŠ¸ìš© ê²Œì‹œê¸€ ìˆ˜ì§‘ ì¤‘...")
    return crawl_korean_sites()

def test_korean_crawling():
    """í•œêµ­ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸"""
    print("=== í•œêµ­ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ===")
    
    # í™˜ê²½ë³€ìˆ˜ ì²´í¬
    print("\ní™˜ê²½ë³€ìˆ˜ í™•ì¸:")
    webhooks = check_discord_webhooks()
    
    print("\n1. ë£¨ë¦¬ì›¹ í…ŒìŠ¤íŠ¸:")
    ruliweb_posts = fetch_ruliweb_epic7_board()
    
    print("\n2. ìŠ¤í† ë¸Œ ë²„ê·¸ í…ŒìŠ¤íŠ¸:")
    stove_bug_posts = fetch_stove_bug_board()
    
    print("\n3. ìŠ¤í† ë¸Œ ììœ  í…ŒìŠ¤íŠ¸:")
    stove_general_posts = fetch_stove_general_board()
    
    print(f"\n=== í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===")
    print(f"ë£¨ë¦¬ì›¹: {len(ruliweb_posts)}ê°œ")
    print(f"ìŠ¤í† ë¸Œ ë²„ê·¸: {len(stove_bug_posts)}ê°œ")
    print(f"ìŠ¤í† ë¸Œ ììœ : {len(stove_general_posts)}ê°œ")
    print(f"ì´ í•©ê³„: {len(ruliweb_posts) + len(stove_bug_posts) + len(stove_general_posts)}ê°œ")
    
    print(f"\n=== ë‚´ìš© ì¶”ì¶œ ê²°ê³¼ ìƒ˜í”Œ ===")
    for i, post in enumerate((stove_bug_posts + stove_general_posts)[:3]):
        print(f"{i+1}. {post['title'][:50]}...")
        print(f"   ë‚´ìš©: {post['content'][:100]}...")
        print(f"   ì†ŒìŠ¤: {post['source']}")
        print()
    
    return ruliweb_posts + stove_bug_posts + stove_general_posts

def test_content_extraction():
    """ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ ë‹¨ë… í…ŒìŠ¤íŠ¸"""
    print("=== ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ===")
    
    test_urls = [
        "https://page.onstove.com/epicseven/kr/view/10868434",
        "https://page.onstove.com/epicseven/kr/view/10650067",
        "https://page.onstove.com/epicseven/kr/view/10794251"
    ]
    
    for i, url in enumerate(test_urls, 1):
        print(f"\n{i}. í…ŒìŠ¤íŠ¸ URL: {url}")
        content = get_stove_post_content(url)
        print(f"   ì¶”ì¶œëœ ë‚´ìš©: {content}")
        print(f"   ê¸¸ì´: {len(content)}ì")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "test_content":
        test_content_extraction()
    else:
        test_korean_crawling()