# crawler.py - Epic7 ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì™„ì „ ê°œì„  ë²„ì „ 2.0
# Korean/Global ëª¨ë“œ ë¶„ê¸° ì²˜ë¦¬ì™€ ê¸€ë¡œë²Œ í¬ë¡¤ë§ í•¨ìˆ˜ ì™„ì „ êµ¬í˜„

import json
import os
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.keys import Keys
import time
from datetime import datetime, timedelta
import re
import random
import requests
import hashlib
from typing import Dict, List, Optional, Tuple
import threading
import concurrent.futures
from urllib.parse import urljoin, urlparse

# =============================================================================
# ëª¨ë“œ ê¸°ë°˜ íŒŒì¼ ë¶„ë¦¬ ì‹œìŠ¤í…œ
# =============================================================================

def get_file_paths(mode: str = "korean"):
    """ëª¨ë“œì— ë”°ë¥¸ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
    is_debug = os.environ.get('GITHUB_WORKFLOW', '').lower() in ['debug', 'test']
    
    if mode == "korean":
        links_file = "crawled_links_korean_debug.json" if is_debug else "crawled_links_korean.json"
        cache_file = "content_cache_korean_debug.json" if is_debug else "content_cache_korean.json"
    elif mode == "global":
        links_file = "crawled_links_global_debug.json" if is_debug else "crawled_links_global.json"
        cache_file = "content_cache_global_debug.json" if is_debug else "content_cache_global.json"
    else:  # all mode
        links_file = "crawled_links_debug.json" if is_debug else "crawled_links.json"
        cache_file = "content_cache_debug.json" if is_debug else "content_cache.json"
    
    return links_file, cache_file

def load_crawled_links(mode: str = "korean"):
    """ëª¨ë“œë³„ í¬ë¡¤ë§ëœ ë§í¬ë“¤ì„ ë¡œë“œ"""
    links_file, _ = get_file_paths(mode)
    
    if os.path.exists(links_file):
        try:
            with open(links_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, list):
                    return {"links": data, "last_updated": datetime.now().isoformat()}
                return data
        except (json.JSONDecodeError, FileNotFoundError):
            print(f"[WARNING] {links_file} íŒŒì¼ ì½ê¸° ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±")
    
    return {"links": [], "last_updated": datetime.now().isoformat()}

def save_crawled_links(link_data, mode: str = "korean"):
    """ëª¨ë“œë³„ í¬ë¡¤ë§ëœ ë§í¬ë“¤ì„ ì €ì¥ (ìµœëŒ€ 1000ê°œ ìœ ì§€)"""
    links_file, _ = get_file_paths(mode)
    
    try:
        if len(link_data["links"]) > 1000:
            link_data["links"] = link_data["links"][-1000:]
        link_data["last_updated"] = datetime.now().isoformat()
        
        with open(links_file, 'w', encoding='utf-8') as f:
            json.dump(link_data, f, ensure_ascii=False, indent=2)
        print(f"[INFO] {links_file} ì €ì¥ ì™„ë£Œ: {len(link_data['links'])}ê°œ ë§í¬")
    except Exception as e:
        print(f"[ERROR] {links_file} ì €ì¥ ì‹¤íŒ¨: {e}")

def load_content_cache(mode: str = "korean"):
    """ëª¨ë“œë³„ ê²Œì‹œê¸€ ë‚´ìš© ìºì‹œ ë¡œë“œ"""
    _, cache_file = get_file_paths(mode)
    
    if os.path.exists(cache_file):
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            print(f"[WARNING] {cache_file} íŒŒì¼ ì½ê¸° ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±")
    
    return {}

def save_content_cache(cache_data, mode: str = "korean"):
    """ëª¨ë“œë³„ ê²Œì‹œê¸€ ë‚´ìš© ìºì‹œ ì €ì¥"""
    _, cache_file = get_file_paths(mode)
    
    try:
        if len(cache_data) > 500:
            sorted_items = sorted(cache_data.items(), key=lambda x: x[1].get('timestamp', ''), reverse=True)
            cache_data = dict(sorted_items[:500])
        
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        print(f"[INFO] {cache_file} ì €ì¥ ì™„ë£Œ: {len(cache_data)}ê°œ ìºì‹œ")
    except Exception as e:
        print(f"[ERROR] {cache_file} ì €ì¥ ì‹¤íŒ¨: {e}")

# =============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# =============================================================================

def get_url_hash(url: str) -> str:
    """URLì˜ í•´ì‹œê°’ ìƒì„±"""
    return hashlib.md5(url.encode('utf-8')).hexdigest()

def extract_content_summary(content: str) -> str:
    """ê²Œì‹œê¸€ ë‚´ìš©ì„ í•œ ì¤„ë¡œ ìš”ì•½"""
    if not content or len(content.strip()) < 10:
        return "ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”."
    
    content = re.sub(r'\s+', ' ', content.strip())
    content = re.sub(r'[^\w\sê°€-í£.,!?]', '', content)
    sentences = re.split(r'[.!?]', content)
    first_sentence = sentences[0].strip() if sentences else content
    
    if len(first_sentence) > 100:
        first_sentence = first_sentence[:97] + '...'
    elif len(first_sentence) > 10:
        first_sentence = first_sentence + '...'
    
    return first_sentence if first_sentence else "ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”."

def fix_stove_url(url):
    """ìŠ¤í† ë¸Œ URL ìˆ˜ì • í•¨ìˆ˜"""
    if not url:
        return url
    
    if url.startswith('ttps://'):
        url = 'h' + url
    elif url.startswith('ttp://'):
        url = 'h' + url
    elif url.startswith('/'):
        url = 'https://page.onstove.com' + url
    elif not url.startswith('http'):
        url = 'https://page.onstove.com' + ('/' if not url.startswith('/') else '') + url
    
    return url

def retry_on_failure(max_retries: int = 3, delay: float = 2.0):
    """ì¬ì‹œë„ ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    if attempt < max_retries - 1:
                        print(f"[RETRY] {func.__name__} ì‹œë„ {attempt + 1}/{max_retries} ì‹¤íŒ¨: {e}")
                        time.sleep(delay * (attempt + 1))
                    else:
                        print(f"[ERROR] {func.__name__} ìµœì¢… ì‹¤íŒ¨: {e}")
            
            if last_exception:
                raise last_exception
            
        return wrapper
    return decorator

# =============================================================================
# Chrome ë“œë¼ì´ë²„ ê´€ë¦¬
# =============================================================================

@retry_on_failure(max_retries=3, delay=1.0)
def get_chrome_driver():
    """Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--disable-extensions')
    options.add_argument('--disable-plugins')
    options.add_argument('--disable-images')
    options.add_argument('--window-size=1920,1080')
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument('--disable-web-security')
    options.add_argument('--disable-features=VizDisplayCompositor')
    options.add_argument('--disable-background-timer-throttling')
    options.add_argument('--disable-renderer-backgrounding')
    options.add_argument('--disable-backgrounding-occluded-windows')
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
    ]
    options.add_argument(f'--user-agent={random.choice(user_agents)}')
    
    prefs = {
        'profile.default_content_setting_values': {
            'images': 2, 'plugins': 2, 'popups': 2,
            'geolocation': 2, 'notifications': 2, 'media_stream': 2,
        }
    }
    options.add_experimental_option('prefs', prefs)
    
    possible_paths = [
        '/usr/bin/chromedriver',
        '/usr/local/bin/chromedriver',
        '/snap/bin/chromium.chromedriver',
        '/opt/chrome/chromedriver'
    ]
    
    for path in possible_paths:
        try:
            if os.path.exists(path):
                service = Service(path)
                driver = webdriver.Chrome(service=service, options=options)
                driver.set_page_load_timeout(45)
                driver.implicitly_wait(15)
                print(f"[SUCCESS] ChromeDriver ì´ˆê¸°í™” ì„±ê³µ: {path}")
                return driver
        except Exception as e:
            print(f"[DEBUG] ChromeDriver ê²½ë¡œ {path} ì‹¤íŒ¨: {str(e)[:100]}...")
            continue
    
    try:
        driver = webdriver.Chrome(options=options)
        driver.set_page_load_timeout(45)
        driver.implicitly_wait(15)
        print("[SUCCESS] ì‹œìŠ¤í…œ ê¸°ë³¸ ChromeDriver ì´ˆê¸°í™” ì„±ê³µ")
        return driver
    except Exception as e:
        print(f"[DEBUG] ì‹œìŠ¤í…œ ê¸°ë³¸ ChromeDriver ì‹¤íŒ¨: {str(e)[:100]}...")
    
    try:
        from webdriver_manager.chrome import ChromeDriverManager
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        driver.set_page_load_timeout(45)
        driver.implicitly_wait(15)
        print("[SUCCESS] WebDriver Manager ì´ˆê¸°í™” ì„±ê³µ")
        return driver
    except Exception as e:
        print(f"[DEBUG] WebDriver Manager ì‹¤íŒ¨: {str(e)[:100]}...")
    
    raise Exception("ëª¨ë“  ChromeDriver ì´ˆê¸°í™” ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

# =============================================================================
# ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ í•¨ìˆ˜
# =============================================================================

def get_stove_post_content(post_url: str, driver: webdriver.Chrome = None, mode: str = "korean") -> str:
    """ìŠ¤í† ë¸Œ ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ (ê°•í™”ëœ ë²„ì „)"""
    
    cache = load_content_cache(mode)
    url_hash = get_url_hash(post_url)
    
    if url_hash in cache:
        cached_item = cache[url_hash]
        cache_time = datetime.fromisoformat(cached_item.get('timestamp', '2000-01-01'))
        if datetime.now() - cache_time < timedelta(hours=24):
            return cached_item.get('content', "ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”.")
    
    driver_created = False
    if driver is None:
        try:
            driver = get_chrome_driver()
            driver_created = True
        except Exception as e:
            print(f"[ERROR] Driver ìƒì„± ì‹¤íŒ¨: {e}")
            return "ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”."
    
    content_summary = "ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”."
    
    try:
        driver.get(post_url)
        time.sleep(20)
        
        WebDriverWait(driver, 20).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        
        for i in range(3):
            driver.execute_script(f"window.scrollTo(0, {500 * (i + 1)});")
            time.sleep(5)
        
        driver.execute_script("window.scrollTo(0, 0);")
        time.sleep(5)
        
        content_selectors = [
            'div.s-article-content',
            'div.s-article-content-text',
            'div[class*="s-article-content"]',
            'section.s-article-body',
            'div.s-board-content',
            'div.article-content',
            'div.post-content',
            'div.content-body',
            'main.content',
            'div[class*="text-content"]',
            'div[class*="post-body"]',
            'div[class*="article-body"]'
        ]
        
        extracted_content = ""
        successful_selector = ""
        
        for selector in content_selectors:
            try:
                elements = driver.find_elements(By.CSS_SELECTOR, selector)
                if elements:
                    for element in elements:
                        text = element.text.strip()
                        if text and len(text) > 50:
                            if not any(skip_text in text.lower() for skip_text in 
                                     ['install stove', 'ìŠ¤í† ë¸Œë¥¼ ì„¤ì¹˜', 'ë¡œê·¸ì¸ì´ í•„ìš”', 'javascript']):
                                extracted_content = text
                                successful_selector = selector
                                break
                    if extracted_content:
                        break
            except Exception as e:
                continue
        
        if extracted_content:
            lines = extracted_content.split('\n')
            meaningful_lines = []
            
            for line in lines:
                line = line.strip()
                if (len(line) > 15 and 
                    'ë¡œê·¸ì¸' not in line and 
                    'íšŒì›ê°€ì…' not in line and 
                    'ë©”ë‰´' not in line and 
                    'ê²€ìƒ‰' not in line and
                    'ê³µì§€ì‚¬í•­' not in line and
                    'ì´ë²¤íŠ¸' not in line and
                    'Install STOVE' not in line and
                    'ìŠ¤í† ë¸Œë¥¼ ì„¤ì¹˜' not in line):
                    meaningful_lines.append(line)
            
            if meaningful_lines:
                longest_line = max(meaningful_lines, key=len)
                if len(longest_line) > 20:
                    content_summary = extract_content_summary(longest_line)
                else:
                    content_summary = extract_content_summary(meaningful_lines[0])
        
        if content_summary == "ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”.":
            try:
                js_content = driver.execute_script("""
                    var contentElements = [
                        document.querySelector('div.s-article-content'),
                        document.querySelector('div[class*="article-content"]'),
                        document.querySelector('div[class*="post-content"]'),
                        document.querySelector('main'),
                        document.querySelector('article')
                    ];
                    
                    for (var i = 0; i < contentElements.length; i++) {
                        var element = contentElements[i];
                        if (element && element.innerText) {
                            var text = element.innerText.trim();
                            if (text.length > 50 && 
                                !text.toLowerCase().includes('install stove') &&
                                !text.includes('ìŠ¤í† ë¸Œë¥¼ ì„¤ì¹˜') &&
                                !text.includes('ë¡œê·¸ì¸ì´ í•„ìš”')) {
                                return text;
                            }
                        }
                    }
                    return '';
                """)
                
                if js_content and len(js_content.strip()) > 50:
                    content_summary = extract_content_summary(js_content)
                    
            except Exception as e:
                print(f"[ERROR] JavaScript ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        if content_summary == "ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”.":
            try:
                html = driver.page_source
                soup = BeautifulSoup(html, 'html.parser')
                
                stove_content_tags = [
                    soup.find('div', class_='s-article-content'),
                    soup.find('div', class_='s-article-content-text'),
                    soup.find('section', class_='s-article-body'),
                    soup.find('div', class_='s-board-content')
                ]
                
                for tag in stove_content_tags:
                    if tag:
                        text = tag.get_text(strip=True)
                        if text and len(text) > 50:
                            if not any(skip in text.lower() for skip in ['install stove', 'ìŠ¤í† ë¸Œë¥¼ ì„¤ì¹˜']):
                                content_summary = extract_content_summary(text)
                                break
                                
            except Exception as e:
                print(f"[ERROR] BeautifulSoup ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        debug_filename = f"debug_stove_{mode}_{datetime.now().strftime('%H%M%S')}.html"
        with open(debug_filename, "w", encoding="utf-8") as f:
            f.write(driver.page_source)
        
        cache[url_hash] = {
            'content': content_summary,
            'timestamp': datetime.now().isoformat(),
            'url': post_url,
            'selector_used': successful_selector
        }
        save_content_cache(cache, mode)
        
    except TimeoutException:
        print(f"[ERROR] í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ: {post_url}")
        content_summary = "â° ê²Œì‹œê¸€ ë¡œë”© ì‹œê°„ ì´ˆê³¼. ë§í¬ë¥¼ í´ë¦­í•˜ì—¬ í™•ì¸í•˜ì„¸ìš”."
    except Exception as e:
        print(f"[ERROR] ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        content_summary = "ğŸ”— ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”."
    finally:
        if driver_created and driver:
            try:
                driver.quit()
            except:
                pass
    
    return content_summary

# =============================================================================
# í•œêµ­ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ í•¨ìˆ˜ë“¤
# =============================================================================

@retry_on_failure(max_retries=2, delay=3.0)
def fetch_stove_bug_board(mode: str = "korean"):
    """ìŠ¤í† ë¸Œ ì—í”½ì„¸ë¸ ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§"""
    posts = []
    link_data = load_crawled_links(mode)
    crawled_links = link_data["links"]
    
    print(f"[DEBUG] ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹œì‘ (mode: {mode})")
    
    driver = None
    try:
        driver = get_chrome_driver()
        url = "https://page.onstove.com/epicseven/kr/list/1012?page=1&direction=LATEST"
        driver.get(url)
        
        time.sleep(20)
        WebDriverWait(driver, 20).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        
        for i in range(3):
            driver.execute_script(f"window.scrollTo(0, {500 * (i + 1)});")
            time.sleep(5)
        
        driver.execute_script("window.scrollTo(0, 0);")
        time.sleep(5)
        
        html_content = driver.page_source
        with open(f"stove_bug_debug_{mode}.html", "w", encoding="utf-8") as f:
            f.write(html_content)
        
        user_posts = driver.execute_script("""
            var posts = [];
            var items = document.querySelectorAll('section.s-board-item');
            
            for (var i = 0; i < Math.min(items.length, 15); i++) {
                var item = items[i];
                var link = item.querySelector('a[href*="/view/"]');
                var title = item.querySelector('.s-board-title-text, .board-title, h3 span');
                
                if (link && title && link.href && title.innerText) {
                    var titleText = title.innerText.trim();
                    if (titleText.length > 3) {
                        var isNotice = item.querySelector('.notice, [class*="notice"]');
                        var isEvent = item.querySelector('.event, [class*="event"]');
                        
                        if (!isNotice && !isEvent) {
                            posts.push({
                                title: titleText,
                                href: link.href,
                                id: link.href.split('/').pop()
                            });
                        }
                    }
                }
            }
            return posts;
        """)
        
        for i, post_info in enumerate(user_posts, 1):
            try:
                href = fix_stove_url(post_info['href'])
                title = post_info['title']
                
                if href in crawled_links:
                    continue
                
                if title and href and len(title) > 3:
                    content = get_stove_post_content(href, driver, mode)
                    
                    post_data = {
                        "title": title,
                        "url": href,
                        "content": content,
                        "timestamp": datetime.now().isoformat(),
                        "source": "stove_bug",
                        "site": "STOVE ë²„ê·¸ì‹ ê³ "
                    }
                    posts.append(post_data)
                    crawled_links.append(href)
                    print(f"[NEW] ìŠ¤í† ë¸Œ ë²„ê·¸ ìƒˆ ê²Œì‹œê¸€: {title[:50]}...")
                
                time.sleep(random.uniform(3, 6))
                
            except Exception as e:
                print(f"[ERROR] ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œê¸€ {i} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
        
    except Exception as e:
        print(f"[ERROR] ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        
    finally:
        if driver:
            driver.quit()
    
    link_data["links"] = crawled_links
    save_crawled_links(link_data, mode)
    
    return posts

@retry_on_failure(max_retries=2, delay=3.0)
def fetch_stove_general_board(mode: str = "korean"):
    """ìŠ¤í† ë¸Œ ì—í”½ì„¸ë¸ ììœ ê²Œì‹œíŒ í¬ë¡¤ë§"""
    posts = []
    link_data = load_crawled_links(mode)
    crawled_links = link_data["links"]
    
    print(f"[DEBUG] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹œì‘ (mode: {mode})")
    
    driver = None
    try:
        driver = get_chrome_driver()
        url = "https://page.onstove.com/epicseven/kr/list/1005?page=1&direction=LATEST"
        driver.get(url)
        
        time.sleep(20)
        WebDriverWait(driver, 20).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        
        for i in range(3):
            driver.execute_script(f"window.scrollTo(0, {500 * (i + 1)});")
            time.sleep(5)
        
        user_posts = driver.execute_script("""
            var posts = [];
            var items = document.querySelectorAll('section.s-board-item');
            
            for (var i = 0; i < Math.min(items.length, 15); i++) {
                var item = items[i];
                var link = item.querySelector('a[href*="/view/"]');
                var title = item.querySelector('.s-board-title-text, .board-title, h3 span');
                
                if (link && title && link.href && title.innerText) {
                    var titleText = title.innerText.trim();
                    if (titleText.length > 3) {
                        var isNotice = item.querySelector('.notice, [class*="notice"]');
                        var isEvent = item.querySelector('.event, [class*="event"]');
                        
                        if (!isNotice && !isEvent) {
                            posts.push({
                                title: titleText,
                                href: link.href
                            });
                        }
                    }
                }
            }
            return posts;
        """)
        
        for post_info in user_posts:
            try:
                href = fix_stove_url(post_info['href'])
                title = post_info['title']
                
                if href in crawled_links:
                    continue
                
                if not title or len(title) < 4:
                    continue
                
                meaningless_patterns = [
                    r'^[.]{3,}$',
                    r'^[ã…‹ã…ã…—ã…œã…‘]{3,}$',
                    r'^[!@#$%^&*()]{3,}$',
                ]
                
                is_meaningless = any(re.match(pattern, title) for pattern in meaningless_patterns)
                if is_meaningless:
                    continue
                
                content = get_stove_post_content(href, driver, mode)
                
                post_data = {
                    "title": title,
                    "url": href,
                    "content": content,
                    "timestamp": datetime.now().isoformat(),
                    "source": "stove_general",
                    "site": "STOVE ììœ ê²Œì‹œíŒ"
                }
                posts.append(post_data)
                crawled_links.append(href)
                print(f"[NEW] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ ìƒˆ ê²Œì‹œê¸€: {title[:50]}...")
                
                time.sleep(random.uniform(3, 6))
                
            except Exception as e:
                print(f"[ERROR] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ ê²Œì‹œê¸€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
        
    except Exception as e:
        print(f"[ERROR] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        
    finally:
        if driver:
            driver.quit()
    
    link_data["links"] = crawled_links
    save_crawled_links(link_data, mode)
    
    return posts

@retry_on_failure(max_retries=2, delay=3.0)
def fetch_ruliweb_epic7_board(mode: str = "korean"):
    """ë£¨ë¦¬ì›¹ ì—í”½ì„¸ë¸ ê²Œì‹œíŒ í¬ë¡¤ë§"""
    posts = []
    link_data = load_crawled_links(mode)
    crawled_links = link_data["links"]
    
    print(f"[DEBUG] ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì‹œì‘ (mode: {mode})")
    
    driver = None
    try:
        driver = get_chrome_driver()
        url = "https://bbs.ruliweb.com/game/84834"
        driver.get(url)
        time.sleep(10)
        
        selectors = [
            ".subject_link",
            ".table_body .subject a",
            "td.subject a",
            "a[href*='/read/']",
            ".board_list_table .subject_link",
            "table tr td a[href*='read']"
        ]
        
        articles = []
        for selector in selectors:
            try:
                articles = driver.find_elements(By.CSS_SELECTOR, selector)
                if articles:
                    print(f"[DEBUG] ë£¨ë¦¬ì›¹ ì„ íƒì ì„±ê³µ: {selector} ({len(articles)}ê°œ)")
                    break
            except NoSuchElementException:
                continue
        
        if not articles:
            print("[WARNING] ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            with open(f"ruliweb_debug_{mode}.html", "w", encoding="utf-8") as f:
                f.write(driver.page_source)
            return posts
        
        for i, article in enumerate(articles[:15]):
            try:
                title = article.text.strip()
                link = article.get_attribute("href")
                
                if not title or not link or len(title) < 3:
                    continue
                
                if any(keyword in title for keyword in ['ê³µì§€', 'í•„ë…', 'ì´ë²¤íŠ¸', 'ì¶”ì²œ', 'ë² ìŠ¤íŠ¸', 'ê³µì§€ì‚¬í•­']):
                    continue
                
                if link.startswith('/'):
                    link = 'https://bbs.ruliweb.com' + link
                
                if link not in crawled_links:
                    post_data = {
                        "title": title,
                        "url": link,
                        "content": "ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”.",
                        "timestamp": datetime.now().isoformat(),
                        "source": "ruliweb_epic7",
                        "site": "ë£¨ë¦¬ì›¹"
                    }
                    posts.append(post_data)
                    crawled_links.append(link)
                    print(f"[NEW] ë£¨ë¦¬ì›¹ ìƒˆ ê²Œì‹œê¸€: {title[:50]}...")
                
            except Exception as e:
                print(f"[ERROR] ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ {i+1} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
        
    except Exception as e:
        print(f"[ERROR] ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        if driver:
            try:
                with open(f"ruliweb_error_{mode}.html", "w", encoding="utf-8") as f:
                    f.write(driver.page_source)
            except:
                pass
    
    finally:
        if driver:
            driver.quit()
    
    link_data["links"] = crawled_links
    save_crawled_links(link_data, mode)
    
    return posts

@retry_on_failure(max_retries=2, delay=3.0)
def fetch_arca_epic7_board(mode: str = "korean"):
    """ì•„ì¹´ë¼ì´ë¸Œ ì—í”½ì„¸ë¸ ì±„ë„ í¬ë¡¤ë§"""
    posts = []
    link_data = load_crawled_links(mode)
    crawled_links = link_data["links"]
    
    print(f"[DEBUG] ì•„ì¹´ë¼ì´ë¸Œ í¬ë¡¤ë§ ì‹œì‘ (mode: {mode})")
    
    driver = None
    try:
        driver = get_chrome_driver()
        url = "https://arca.live/b/epic7"
        driver.get(url)
        time.sleep(10)
        
        WebDriverWait(driver, 15).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        
        driver.execute_script("window.scrollTo(0, 800);")
        time.sleep(5)
        
        html_content = driver.page_source
        with open(f"arca_debug_{mode}.html", "w", encoding="utf-8") as f:
            f.write(html_content)
        
        selectors = [
            ".vrow .title a",
            ".vrow-inner .title a",
            "a[href*='/b/epic7/']",
            ".article-title a",
            ".list-table .title a"
        ]
        
        articles = []
        for selector in selectors:
            try:
                articles = driver.find_elements(By.CSS_SELECTOR, selector)
                if articles:
                    print(f"[DEBUG] ì•„ì¹´ë¼ì´ë¸Œ ì„ íƒì ì„±ê³µ: {selector} ({len(articles)}ê°œ)")
                    break
            except NoSuchElementException:
                continue
        
        if not articles:
            print("[WARNING] ì•„ì¹´ë¼ì´ë¸Œ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return posts
        
        for i, article in enumerate(articles[:15]):
            try:
                title = article.text.strip()
                link = article.get_attribute("href")
                
                if not title or not link or len(title) < 3:
                    continue
                
                if any(keyword in title for keyword in ['ê³µì§€', 'í•„ë…', 'ì´ë²¤íŠ¸', 'ì¶”ì²œ', 'ë² ìŠ¤íŠ¸']):
                    continue
                
                if link.startswith('/'):
                    link = 'https://arca.live' + link
                
                if link not in crawled_links:
                    post_data = {
                        "title": title,
                        "url": link,
                        "content": "ì•„ì¹´ë¼ì´ë¸Œ ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”.",
                        "timestamp": datetime.now().isoformat(),
                        "source": "arca_epic7",
                        "site": "ì•„ì¹´ë¼ì´ë¸Œ"
                    }
                    posts.append(post_data)
                    crawled_links.append(link)
                    print(f"[NEW] ì•„ì¹´ë¼ì´ë¸Œ ìƒˆ ê²Œì‹œê¸€: {title[:50]}...")
                
            except Exception as e:
                print(f"[ERROR] ì•„ì¹´ë¼ì´ë¸Œ ê²Œì‹œê¸€ {i+1} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
        
    except Exception as e:
        print(f"[ERROR] ì•„ì¹´ë¼ì´ë¸Œ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        
    finally:
        if driver:
            driver.quit()
    
    link_data["links"] = crawled_links
    save_crawled_links(link_data, mode)
    
    return posts

# =============================================================================
# ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ í•¨ìˆ˜ë“¤
# =============================================================================

@retry_on_failure(max_retries=2, delay=3.0)
def fetch_stove_global_bug_board(mode: str = "global"):
    """STOVE ê¸€ë¡œë²Œ ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§"""
    posts = []
    link_data = load_crawled_links(mode)
    crawled_links = link_data["links"]
    
    print(f"[DEBUG] STOVE ê¸€ë¡œë²Œ ë²„ê·¸ í¬ë¡¤ë§ ì‹œì‘ (mode: {mode})")
    
    driver = None
    try:
        driver = get_chrome_driver()
        url = "https://page.onstove.com/epicseven/global/list/998?page=1&direction=LATEST"
        driver.get(url)
        
        time.sleep(20)
        WebDriverWait(driver, 20).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        
        for i in range(3):
            driver.execute_script(f"window.scrollTo(0, {500 * (i + 1)});")
            time.sleep(5)
        
        html_content = driver.page_source
        with open(f"stove_global_bug_debug_{mode}.html", "w", encoding="utf-8") as f:
            f.write(html_content)
        
        user_posts = driver.execute_script("""
            var posts = [];
            var items = document.querySelectorAll('section.s-board-item');
            
            for (var i = 0; i < Math.min(items.length, 15); i++) {
                var item = items[i];
                var link = item.querySelector('a[href*="/view/"]');
                var title = item.querySelector('.s-board-title-text, .board-title, h3 span');
                
                if (link && title && link.href && title.innerText) {
                    var titleText = title.innerText.trim();
                    if (titleText.length > 3) {
                        var isNotice = item.querySelector('.notice, [class*="notice"]');
                        var isEvent = item.querySelector('.event, [class*="event"]');
                        
                        if (!isNotice && !isEvent) {
                            posts.push({
                                title: titleText,
                                href: link.href
                            });
                        }
                    }
                }
            }
            return posts;
        """)
        
        for post_info in user_posts:
            try:
                href = fix_stove_url(post_info['href'])
                title = post_info['title']
                
                if href in crawled_links:
                    continue
                
                content = get_stove_post_content(href, driver, mode)
                
                post_data = {
                    "title": title,
                    "url": href,
                    "content": content,
                    "timestamp": datetime.now().isoformat(),
                    "source": "stove_global_bug",
                    "site": "STOVE Global Bug"
                }
                posts.append(post_data)
                crawled_links.append(href)
                print(f"[NEW] STOVE ê¸€ë¡œë²Œ ë²„ê·¸ ìƒˆ ê²Œì‹œê¸€: {title[:50]}...")
                
                time.sleep(random.uniform(3, 6))
                
            except Exception as e:
                print(f"[ERROR] STOVE ê¸€ë¡œë²Œ ë²„ê·¸ ê²Œì‹œê¸€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
        
    except Exception as e:
        print(f"[ERROR] STOVE ê¸€ë¡œë²Œ ë²„ê·¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        
    finally:
        if driver:
            driver.quit()
    
    link_data["links"] = crawled_links
    save_crawled_links(link_data, mode)
    
    return posts

@retry_on_failure(max_retries=2, delay=3.0)
def fetch_stove_global_general_board(mode: str = "global"):
    """STOVE ê¸€ë¡œë²Œ ììœ ê²Œì‹œíŒ í¬ë¡¤ë§"""
    posts = []
    link_data = load_crawled_links(mode)
    crawled_links = link_data["links"]
    
    print(f"[DEBUG] STOVE ê¸€ë¡œë²Œ ììœ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹œì‘ (mode: {mode})")
    
    driver = None
    try:
        driver = get_chrome_driver()
        url = "https://page.onstove.com/epicseven/global/list/989?page=1&direction=LATEST"
        driver.get(url)
        
        time.sleep(20)
        WebDriverWait(driver, 20).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        
        for i in range(3):
            driver.execute_script(f"window.scrollTo(0, {500 * (i + 1)});")
            time.sleep(5)
        
        user_posts = driver.execute_script("""
            var posts = [];
            var items = document.querySelectorAll('section.s-board-item');
            
            for (var i = 0; i < Math.min(items.length, 15); i++) {
                var item = items[i];
                var link = item.querySelector('a[href*="/view/"]');
                var title = item.querySelector('.s-board-title-text, .board-title, h3 span');
                
                if (link && title && link.href && title.innerText) {
                    var titleText = title.innerText.trim();
                    if (titleText.length > 3) {
                        var isNotice = item.querySelector('.notice, [class*="notice"]');
                        var isEvent = item.querySelector('.event, [class*="event"]');
                        
                        if (!isNotice && !isEvent) {
                            posts.push({
                                title: titleText,
                                href: link.href
                            });
                        }
                    }
                }
            }
            return posts;
        """)
        
        for post_info in user_posts:
            try:
                href = fix_stove_url(post_info['href'])
                title = post_info['title']
                
                if href in crawled_links:
                    continue
                
                content = get_stove_post_content(href, driver, mode)
                
                post_data = {
                    "title": title,
                    "url": href,
                    "content": content,
                    "timestamp": datetime.now().isoformat(),
                    "source": "stove_global_general",
                    "site": "STOVE Global General"
                }
                posts.append(post_data)
                crawled_links.append(href)
                print(f"[NEW] STOVE ê¸€ë¡œë²Œ ììœ ê²Œì‹œíŒ ìƒˆ ê²Œì‹œê¸€: {title[:50]}...")
                
                time.sleep(random.uniform(3, 6))
                
            except Exception as e:
                print(f"[ERROR] STOVE ê¸€ë¡œë²Œ ììœ ê²Œì‹œíŒ ê²Œì‹œê¸€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
        
    except Exception as e:
        print(f"[ERROR] STOVE ê¸€ë¡œë²Œ ììœ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        
    finally:
        if driver:
            driver.quit()
    
    link_data["links"] = crawled_links
    save_crawled_links(link_data, mode)
    
    return posts

@retry_on_failure(max_retries=2, delay=3.0)
def fetch_reddit_epic7_board(mode: str = "global"):
    """Reddit r/EpicSeven ìµœì‹ ê¸€ í¬ë¡¤ë§"""
    posts = []
    link_data = load_crawled_links(mode)
    crawled_links = link_data["links"]
    
    print(f"[DEBUG] Reddit í¬ë¡¤ë§ ì‹œì‘ (mode: {mode})")
    
    try:
        url = "https://www.reddit.com/r/EpicSeven/new.json?limit=20"
        headers = {
            "User-Agent": "Epic7MonitorBot/2.0",
            "Accept": "application/json"
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        data = response.json()
        
        if 'data' in data and 'children' in data['data']:
            for child in data['data']['children']:
                try:
                    item = child['data']
                    title = item.get('title', '').strip()
                    permalink = "https://www.reddit.com" + item.get('permalink', '')
                    
                    if not title or not permalink or len(title) < 3:
                        continue
                    
                    if permalink in crawled_links:
                        continue
                    
                    post_data = {
                        "title": title,
                        "url": permalink,
                        "content": f"Reddit ê²Œì‹œê¸€: {title[:100]}...",
                        "timestamp": datetime.now().isoformat(),
                        "source": "reddit_epic7",
                        "site": "Reddit"
                    }
                    posts.append(post_data)
                    crawled_links.append(permalink)
                    print(f"[NEW] Reddit ìƒˆ ê²Œì‹œê¸€: {title[:50]}...")
                    
                except Exception as e:
                    print(f"[ERROR] Reddit ê²Œì‹œê¸€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
        
    except requests.RequestException as e:
        print(f"[ERROR] Reddit API ìš”ì²­ ì‹¤íŒ¨: {e}")
    except Exception as e:
        print(f"[ERROR] Reddit í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    link_data["links"] = crawled_links
    save_crawled_links(link_data, mode)
    
    return posts

@retry_on_failure(max_retries=2, delay=3.0)
def fetch_epic7_official_forum(mode: str = "global"):
    """Epic7 ê³µì‹ í¬ëŸ¼ í¬ë¡¤ë§"""
    posts = []
    link_data = load_crawled_links(mode)
    crawled_links = link_data["links"]
    
    print(f"[DEBUG] Epic7 ê³µì‹ í¬ëŸ¼ í¬ë¡¤ë§ ì‹œì‘ (mode: {mode})")
    
    driver = None
    try:
        driver = get_chrome_driver()
        url = "https://epic7.gg/forum"
        driver.get(url)
        
        time.sleep(15)
        WebDriverWait(driver, 15).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        
        driver.execute_script("window.scrollTo(0, 800);")
        time.sleep(5)
        
        html_content = driver.page_source
        with open(f"epic7_forum_debug_{mode}.html", "w", encoding="utf-8") as f:
            f.write(html_content)
        
        selectors = [
            ".topic-title a",
            ".forum-post-title a",
            "a[href*='/topic/']",
            ".post-title a",
            ".thread-title a"
        ]
        
        articles = []
        for selector in selectors:
            try:
                articles = driver.find_elements(By.CSS_SELECTOR, selector)
                if articles:
                    print(f"[DEBUG] ê³µì‹ í¬ëŸ¼ ì„ íƒì ì„±ê³µ: {selector} ({len(articles)}ê°œ)")
                    break
            except NoSuchElementException:
                continue
        
        for i, article in enumerate(articles[:15]):
            try:
                title = article.text.strip()
                link = article.get_attribute("href")
                
                if not title or not link or len(title) < 3:
                    continue
                
                if link.startswith('/'):
                    link = 'https://epic7.gg' + link
                
                if link not in crawled_links:
                    post_data = {
                        "title": title,
                        "url": link,
                        "content": f"Epic7 ê³µì‹ í¬ëŸ¼ ê²Œì‹œê¸€: {title[:100]}...",
                        "timestamp": datetime.now().isoformat(),
                        "source": "epic7_forum",
                        "site": "Epic7 Official Forum"
                    }
                    posts.append(post_data)
                    crawled_links.append(link)
                    print(f"[NEW] Epic7 ê³µì‹ í¬ëŸ¼ ìƒˆ ê²Œì‹œê¸€: {title[:50]}...")
                
            except Exception as e:
                print(f"[ERROR] Epic7 ê³µì‹ í¬ëŸ¼ ê²Œì‹œê¸€ {i+1} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
        
    except Exception as e:
        print(f"[ERROR] Epic7 ê³µì‹ í¬ëŸ¼ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        
    finally:
        if driver:
            driver.quit()
    
    link_data["links"] = crawled_links
    save_crawled_links(link_data, mode)
    
    return posts

# =============================================================================
# ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜ë“¤
# =============================================================================

def crawl_korean_sites(mode: str = "korean"):
    """í•œêµ­ ì‚¬ì´íŠ¸ í†µí•© í¬ë¡¤ë§"""
    print(f"[INFO] í•œêµ­ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹œì‘ (mode: {mode})")
    
    all_posts = []
    
    try:
        # ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œíŒ
        bug_posts = fetch_stove_bug_board(mode)
        all_posts.extend(bug_posts)
        print(f"[INFO] ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œíŒ: {len(bug_posts)}ê°œ ê²Œì‹œê¸€")
        
        # ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ
        general_posts = fetch_stove_general_board(mode)
        all_posts.extend(general_posts)
        print(f"[INFO] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ: {len(general_posts)}ê°œ ê²Œì‹œê¸€")
        
        # ë£¨ë¦¬ì›¹
        ruliweb_posts = fetch_ruliweb_epic7_board(mode)
        all_posts.extend(ruliweb_posts)
        print(f"[INFO] ë£¨ë¦¬ì›¹: {len(ruliweb_posts)}ê°œ ê²Œì‹œê¸€")
        
        # ì•„ì¹´ë¼ì´ë¸Œ
        arca_posts = fetch_arca_epic7_board(mode)
        all_posts.extend(arca_posts)
        print(f"[INFO] ì•„ì¹´ë¼ì´ë¸Œ: {len(arca_posts)}ê°œ ê²Œì‹œê¸€")
        
    except Exception as e:
        print(f"[ERROR] í•œêµ­ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
    
    print(f"[INFO] í•œêµ­ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_posts)}ê°œ ê²Œì‹œê¸€")
    return all_posts

def crawl_global_sites(mode: str = "global"):
    """ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ í†µí•© í¬ë¡¤ë§"""
    print(f"[INFO] ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹œì‘ (mode: {mode})")
    
    all_posts = []
    
    try:
        # STOVE ê¸€ë¡œë²Œ ë²„ê·¸ ê²Œì‹œíŒ
        global_bug_posts = fetch_stove_global_bug_board(mode)
        all_posts.extend(global_bug_posts)
        print(f"[INFO] STOVE ê¸€ë¡œë²Œ ë²„ê·¸ ê²Œì‹œíŒ: {len(global_bug_posts)}ê°œ ê²Œì‹œê¸€")
        
        # STOVE ê¸€ë¡œë²Œ ììœ ê²Œì‹œíŒ
        global_general_posts = fetch_stove_global_general_board(mode)
        all_posts.extend(global_general_posts)
        print(f"[INFO] STOVE ê¸€ë¡œë²Œ ììœ ê²Œì‹œíŒ: {len(global_general_posts)}ê°œ ê²Œì‹œê¸€")
        
        # Reddit
        reddit_posts = fetch_reddit_epic7_board(mode)
        all_posts.extend(reddit_posts)
        print(f"[INFO] Reddit: {len(reddit_posts)}ê°œ ê²Œì‹œê¸€")
        
        # Epic7 ê³µì‹ í¬ëŸ¼
        forum_posts = fetch_epic7_official_forum(mode)
        all_posts.extend(forum_posts)
        print(f"[INFO] Epic7 ê³µì‹ í¬ëŸ¼: {len(forum_posts)}ê°œ ê²Œì‹œê¸€")
        
    except Exception as e:
        print(f"[ERROR] ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
    
    print(f"[INFO] ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_posts)}ê°œ ê²Œì‹œê¸€")
    return all_posts

def crawl_all_sites():
    """ëª¨ë“  ì‚¬ì´íŠ¸ í†µí•© í¬ë¡¤ë§"""
    print("[INFO] ì „ì²´ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹œì‘")
    
    all_posts = []
    
    try:
        # í•œêµ­ ì‚¬ì´íŠ¸
        korean_posts = crawl_korean_sites("korean")
        all_posts.extend(korean_posts)
        
        # ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸
        global_posts = crawl_global_sites("global")
        all_posts.extend(global_posts)
        
    except Exception as e:
        print(f"[ERROR] ì „ì²´ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
    
    print(f"[INFO] ì „ì²´ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_posts)}ê°œ ê²Œì‹œê¸€")
    return all_posts

def get_all_posts_for_report(mode: str = "all"):
    """ë¦¬í¬íŠ¸ìš© ê²Œì‹œê¸€ ìˆ˜ì§‘"""
    print(f"[INFO] ë¦¬í¬íŠ¸ìš© ê²Œì‹œê¸€ ìˆ˜ì§‘ ì‹œì‘ (mode: {mode})")
    
    all_posts = []
    
    try:
        if mode == "korean":
            all_posts = crawl_korean_sites("korean")
        elif mode == "global":
            all_posts = crawl_global_sites("global")
        else:  # all
            all_posts = crawl_all_sites()
        
        # 24ì‹œê°„ ì´ë‚´ ê²Œì‹œê¸€ë§Œ í•„í„°ë§
        recent_posts = []
        cutoff_time = datetime.now() - timedelta(hours=24)
        
        for post in all_posts:
            try:
                post_time = datetime.fromisoformat(post['timestamp'])
                if post_time > cutoff_time:
                    recent_posts.append(post)
            except:
                continue
        
        print(f"[INFO] ë¦¬í¬íŠ¸ìš© ê²Œì‹œê¸€ ìˆ˜ì§‘ ì™„ë£Œ: ìµœê·¼ 24ì‹œê°„ ë‚´ {len(recent_posts)}ê°œ ê²Œì‹œê¸€")
        
    except Exception as e:
        print(f"[ERROR] ë¦¬í¬íŠ¸ìš© ê²Œì‹œê¸€ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
    
    return recent_posts

# =============================================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# =============================================================================

def main_crawl(mode: str = "korean"):
    """ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜"""
    print(f"[INFO] Epic7 ëª¨ë‹ˆí„°ë§ í¬ë¡¤ë§ ì‹œì‘ - ëª¨ë“œ: {mode}")
    
    try:
        if mode == "korean":
            posts = crawl_korean_sites(mode)
        elif mode == "global":
            posts = crawl_global_sites(mode)
        elif mode == "all":
            posts = crawl_all_sites()
        else:
            print(f"[ERROR] ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë“œ: {mode}")
            return []
        
        print(f"[INFO] í¬ë¡¤ë§ ì™„ë£Œ: {len(posts)}ê°œ ê²Œì‹œê¸€")
        return posts
        
    except Exception as e:
        print(f"[ERROR] ë©”ì¸ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
        return []

# =============================================================================
# í…ŒìŠ¤íŠ¸ ë° ë””ë²„ê·¸ í•¨ìˆ˜
# =============================================================================

def test_crawling():
    """í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("[TEST] í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # í•œêµ­ ì‚¬ì´íŠ¸ í…ŒìŠ¤íŠ¸
    print("\n[TEST] í•œêµ­ ì‚¬ì´íŠ¸ í…ŒìŠ¤íŠ¸")
    korean_posts = crawl_korean_sites("korean")
    print(f"í•œêµ­ ì‚¬ì´íŠ¸ ê²°ê³¼: {len(korean_posts)}ê°œ")
    
    # ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ í…ŒìŠ¤íŠ¸
    print("\n[TEST] ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ í…ŒìŠ¤íŠ¸")
    global_posts = crawl_global_sites("global")
    print(f"ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ ê²°ê³¼: {len(global_posts)}ê°œ")
    
    print("\n[TEST] í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_crawling()