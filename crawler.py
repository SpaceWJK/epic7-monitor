import json
import os
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.keys import Keys
import time
from datetime import datetime, timedelta
import re
import random
import requests

# ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ ë§í¬ ì €ì¥ íŒŒì¼
CRAWLED_LINKS_FILE = "crawled_links.json"

def load_crawled_links():
    """ì´ë¯¸ í¬ë¡¤ë§ëœ ë§í¬ë“¤ì„ ë¡œë“œ"""
    if os.path.exists(CRAWLED_LINKS_FILE):
        try:
            with open(CRAWLED_LINKS_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # êµ¬ ë²„ì „ í˜¸í™˜ì„± (ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)
                if isinstance(data, list):
                    return {"links": data, "last_updated": datetime.now().isoformat()}
                return data
        except (json.JSONDecodeError, FileNotFoundError):
            print("[WARNING] crawled_links.json íŒŒì¼ ì½ê¸° ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±")
    
    return {"links": [], "last_updated": datetime.now().isoformat()}

def save_crawled_links(link_data):
    """í¬ë¡¤ë§ëœ ë§í¬ë“¤ì„ ì €ì¥ (ìµœëŒ€ 1000ê°œ ìœ ì§€)"""
    try:
        # ìµœì‹  1000ê°œë§Œ ìœ ì§€ (ë©”ëª¨ë¦¬ ì ˆì•½)
        if len(link_data["links"]) > 1000:
            link_data["links"] = link_data["links"][-1000:]
        
        link_data["last_updated"] = datetime.now().isoformat()
        
        with open(CRAWLED_LINKS_FILE, 'w', encoding='utf-8') as f:
            json.dump(link_data, f, ensure_ascii=False, indent=2)
            
    except Exception as e:
        print(f"[ERROR] ë§í¬ ì €ì¥ ì‹¤íŒ¨: {e}")

def get_chrome_driver():
    """Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” (ë²„ì „ í˜¸í™˜ì„± ìš°ì„ )"""
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--disable-extensions')
    options.add_argument('--disable-plugins')
    options.add_argument('--disable-images')
    options.add_argument('--disable-javascript-harmony-shipping')
    options.add_argument('--disable-background-timer-throttling')
    options.add_argument('--disable-backgrounding-occluded-windows')
    options.add_argument('--disable-renderer-backgrounding')
    options.add_argument('--window-size=1920,1080')
    
    # ğŸ”§ ë´‡ íƒì§€ ìš°íšŒ ì„¤ì • ì¶”ê°€
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    
    # ğŸ”§ ëœë¤ User-Agent
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
    ]
    options.add_argument(f'--user-agent={random.choice(user_agents)}')
    
    # ì¶”ê°€ ì„±ëŠ¥ ìµœì í™”
    prefs = {
        'profile.default_content_setting_values': {
            'images': 2,
            'plugins': 2,
            'popups': 2,
            'geolocation': 2,
            'notifications': 2,
            'media_stream': 2,
        }
    }
    options.add_experimental_option('prefs', prefs)
    
    driver = None
    
    # í˜¸í™˜ ê°€ëŠ¥í•œ ChromeDriver ê²½ë¡œë“¤ (ë²„ì „ë³„ ìš°ì„ ìˆœìœ„)
    possible_paths = [
        '/usr/bin/chromedriver',  # ìš°ë¶„íˆ¬ íŒ¨í‚¤ì§€ (ê°€ì¥ í˜¸í™˜ì„± ì¢‹ìŒ)
        '/usr/local/bin/chromedriver',  # ìˆ˜ë™ ì„¤ì¹˜
        '/snap/bin/chromium.chromedriver'  # Snap íŒ¨í‚¤ì§€
    ]
    
    # ë°©ë²• 1: ìˆ˜ë™ ê²½ë¡œë³„ ì‹œë„ (ë²„ì „ í˜¸í™˜ì„± ìš°ì„ )
    for path in possible_paths:
        try:
            if os.path.exists(path):
                print(f"[DEBUG] ChromeDriver ì‹œë„: {path}")
                service = Service(path)
                driver = webdriver.Chrome(service=service, options=options)
                print(f"[DEBUG] ChromeDriver ì„±ê³µ: {path}")
                return driver
        except Exception as e:
            print(f"[DEBUG] ChromeDriver ì‹¤íŒ¨ {path}: {str(e)[:100]}...")
            continue
    
    # ë°©ë²• 2: ì‹œìŠ¤í…œ ê¸°ë³¸ ChromeDriver
    try:
        print("[DEBUG] ì‹œìŠ¤í…œ ê¸°ë³¸ ChromeDriver ì‹œë„")
        driver = webdriver.Chrome(options=options)
        print("[DEBUG] ì‹œìŠ¤í…œ ê¸°ë³¸ ChromeDriver ì„±ê³µ")
        return driver
    except Exception as e:
        print(f"[DEBUG] ì‹œìŠ¤í…œ ê¸°ë³¸ ChromeDriver ì‹¤íŒ¨: {str(e)[:100]}...")
    
    # ë°©ë²• 3: WebDriver Manager (ìµœí›„ ìˆ˜ë‹¨)
    try:
        print("[DEBUG] WebDriver Manager ì‹œë„")
        from webdriver_manager.chrome import ChromeDriverManager
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        print("[DEBUG] WebDriver Manager ì„±ê³µ")
        return driver
    except Exception as e:
        print(f"[DEBUG] WebDriver Manager ì‹¤íŒ¨: {str(e)[:100]}...")
    
    raise Exception("ëª¨ë“  ChromeDriver ì´ˆê¸°í™” ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

def fetch_arca_epic7_board():
    """ì•„ì¹´ë¼ì´ë¸Œ ì—í”½ì„¸ë¸ ì±„ë„ í¬ë¡¤ë§ (Cloudflare ìš°íšŒ)"""
    posts = []
    link_data = load_crawled_links()
    crawled_links = link_data["links"]
    
    print(f"[DEBUG] ì•„ì¹´ë¼ì´ë¸Œ í¬ë¡¤ë§ ì‹œì‘ - ì €ì¥ëœ ë§í¬ ìˆ˜: {len(crawled_links)}")
    
    driver = None
    try:
        print("[DEBUG] ì•„ì¹´ë¼ì´ë¸Œìš© Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì¤‘...")
        driver = get_chrome_driver()
        
        # ğŸ”§ ë´‡ íƒì§€ ìš°íšŒ ìŠ¤í¬ë¦½íŠ¸
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        driver.execute_script("delete window.chrome")
        
        driver.set_page_load_timeout(30)
        driver.implicitly_wait(15)
        
        url = "https://arca.live/b/epic7"
        print(f"[DEBUG] ì•„ì¹´ë¼ì´ë¸Œ ì ‘ì† ì‹œë„: {url}")
        
        driver.get(url)
        
        # ğŸ”§ Cloudflare ê²€ì¦ ëŒ€ê¸°
        print("[DEBUG] Cloudflare ê²€ì¦ ë° í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘...")
        time.sleep(random.uniform(10, 15))
        
        # í˜ì´ì§€ ìƒíƒœ í™•ì¸
        page_title = driver.title
        page_source_preview = driver.page_source[:500]
        print(f"[DEBUG] í˜ì´ì§€ ì œëª©: {page_title}")
        
        if "Verifying" in page_title or "security" in page_title.lower() or "Ray ID" in page_source_preview:
            print("[WARNING] Cloudflare ê²€ì¦ ê°ì§€, ì¶”ê°€ ëŒ€ê¸°...")
            time.sleep(random.uniform(15, 20))
        
        # ğŸ”§ ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ê²Œì‹œê¸€ ì¶”ì¶œ ì‹œë„
        print("[DEBUG] ê²Œì‹œê¸€ ëª©ë¡ ê²€ìƒ‰ ì¤‘...")
        
        # ë°©ë²• 1: ì¼ë°˜ì ì¸ ì„ íƒìë“¤
        selectors = [
            "a[href*='/b/epic7/'][href*='?p=']",  # ê²Œì‹œê¸€ ë§í¬ íŒ¨í„´
            ".vrow .title a",                     # ê¸°ë³¸ ì„ íƒì
            ".article-wrapper .title a",          # ëŒ€ì²´ ì„ íƒì 1
            ".list-table .title a",               # ëŒ€ì²´ ì„ íƒì 2
            ".article-list .title a",             # ëŒ€ì²´ ì„ íƒì 3
            "a[title][href*='/b/epic7/']"         # title ì†ì„±ì´ ìˆëŠ” ë§í¬
        ]
        
        articles = []
        for selector in selectors:
            try:
                WebDriverWait(driver, 5).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                )
                articles = driver.find_elements(By.CSS_SELECTOR, selector)
                if articles:
                    print(f"[DEBUG] ì„ íƒì ì„±ê³µ: {selector} ({len(articles)}ê°œ)")
                    break
            except TimeoutException:
                continue
        
        # ë°©ë²• 2: JavaScriptë¡œ ë™ì  ê²€ìƒ‰
        if not articles:
            print("[DEBUG] JavaScriptë¡œ ê²Œì‹œê¸€ ë™ì  ê²€ìƒ‰...")
            articles_js = driver.execute_script("""
                var articles = [];
                var links = document.querySelectorAll('a');
                
                for (var i = 0; i < links.length; i++) {
                    var link = links[i];
                    var href = link.href;
                    var text = link.innerText || link.textContent || link.title || '';
                    
                    if (href && href.includes('/b/epic7/') && 
                        text.length > 3 && 
                        !text.includes('ê³µì§€') && 
                        !text.includes('í•„ë…') &&
                        !text.includes('ì´ë²¤íŠ¸')) {
                        articles.push({
                            title: text.trim(),
                            href: href
                        });
                    }
                }
                
                return articles.slice(0, 20);
            """)
            
            if articles_js:
                print(f"[DEBUG] JavaScriptë¡œ {len(articles_js)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")
                for item in articles_js:
                    if item['href'] not in crawled_links:
                        post_data = {
                            "title": item['title'],
                            "url": item['href'],
                            "timestamp": datetime.now().isoformat(),
                            "source": "arca_epic7"
                        }
                        posts.append(post_data)
                        crawled_links.append(item['href'])
                        print(f"[NEW] ì•„ì¹´ë¼ì´ë¸Œ ìƒˆ ê²Œì‹œê¸€: {item['title'][:50]}...")
        
        # ë°©ë²• 3: ì¼ë°˜ì ì¸ ìš”ì†Œ ì²˜ë¦¬
        else:
            for i, article in enumerate(articles[:20]):
                try:
                    title = article.text.strip() if hasattr(article, 'text') else article.get_attribute('title')
                    link = article.get_attribute('href') if hasattr(article, 'get_attribute') else None
                    
                    if not title or not link or len(title) < 3:
                        continue
                    
                    # ê³µì§€ì‚¬í•­ í•„í„°ë§
                    if any(keyword in title for keyword in ['ê³µì§€', 'í•„ë…', 'ì´ë²¤íŠ¸', 'ì•ˆë‚´', 'ê·œì¹™']):
                        continue
                    
                    # URL ì •ê·œí™”
                    if link.startswith('/'):
                        link = 'https://arca.live' + link
                    
                    if link not in crawled_links:
                        post_data = {
                            "title": title,
                            "url": link,
                            "timestamp": datetime.now().isoformat(),
                            "source": "arca_epic7"
                        }
                        posts.append(post_data)
                        crawled_links.append(link)
                        print(f"[NEW] ì•„ì¹´ë¼ì´ë¸Œ ìƒˆ ê²Œì‹œê¸€ ({i+1}): {title[:50]}...")
                        
                        time.sleep(random.uniform(0.5, 1.0))
                
                except Exception as e:
                    print(f"[ERROR] ì•„ì¹´ë¼ì´ë¸Œ ê²Œì‹œê¸€ {i+1} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
        
        # ë””ë²„ê¹…ìš© HTML ì €ì¥
        if not posts:
            with open("arca_debug_selenium.html", "w", encoding="utf-8") as f:
                f.write(driver.page_source)
            print("[DEBUG] ë””ë²„ê¹…ìš© HTML ì €ì¥: arca_debug_selenium.html")
        
        print(f"[DEBUG] ì•„ì¹´ë¼ì´ë¸Œ í¬ë¡¤ë§ ì™„ë£Œ: {len(posts)}ê°œ ìƒˆ ê²Œì‹œê¸€")
        
    except Exception as e:
        print(f"[ERROR] ì•„ì¹´ë¼ì´ë¸Œ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        if driver:
            try:
                with open("arca_error_debug.html", "w", encoding="utf-8") as f:
                    f.write(driver.page_source)
            except:
                pass
    
    finally:
        if driver:
            driver.quit()
    
    # ë§í¬ ì €ì¥
    link_data["links"] = crawled_links
    save_crawled_links(link_data)
    
    return posts

def fetch_ruliweb_epic7_board():
    """ë£¨ë¦¬ì›¹ ì—í”½ì„¸ë¸ ê²Œì‹œíŒ í¬ë¡¤ë§"""
    posts = []
    link_data = load_crawled_links()
    crawled_links = link_data["links"]
    
    print(f"[DEBUG] ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì‹œì‘ - ì €ì¥ëœ ë§í¬ ìˆ˜: {len(crawled_links)}")
    
    driver = None
    try:
        print("[DEBUG] ë£¨ë¦¬ì›¹ìš© Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì¤‘...")
        driver = get_chrome_driver()
        
        driver.set_page_load_timeout(30)
        driver.implicitly_wait(10)
        
        url = "https://bbs.ruliweb.com/game/84834"
        print(f"[DEBUG] ë£¨ë¦¬ì›¹ ì ‘ì† ì¤‘: {url}")
        
        driver.get(url)
        time.sleep(5)
        
        print("[DEBUG] ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ëª©ë¡ ê²€ìƒ‰ ì¤‘...")
        
        # ğŸ”§ ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ì¶”ì¶œ (ìŠ¤í¬ë¦°ìƒ· ê¸°ë°˜)
        selectors = [
            ".subject_link",                  # ì£¼ì œ ë§í¬ (ê¸°ë³¸)
            ".table_body .subject a",         # í…Œì´ë¸” ë‚´ ì£¼ì œ ë§í¬
            "td.subject a",                   # í…Œì´ë¸” ì…€ ë‚´ ë§í¬
            "a[href*='/read/']",              # ê²Œì‹œê¸€ ì½ê¸° ë§í¬ íŒ¨í„´
            ".board_list_table .subject_link", # ê²Œì‹œíŒ ëª©ë¡ í…Œì´ë¸”
            "table tr td a[href*='read']"     # í…Œì´ë¸” ê¸°ë°˜ ë§í¬
        ]
        
        articles = []
        for selector in selectors:
            try:
                articles = driver.find_elements(By.CSS_SELECTOR, selector)
                if articles:
                    print(f"[DEBUG] ë£¨ë¦¬ì›¹ ì„ íƒì ì„±ê³µ: {selector} ({len(articles)}ê°œ)")
                    break
            except NoSuchElementException:
                continue
        
        if not articles:
            print("[WARNING] ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. HTML ì €ì¥...")
            with open("ruliweb_debug_selenium.html", "w", encoding="utf-8") as f:
                f.write(driver.page_source)
            return posts
        
        # ğŸ”§ ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ
        for i, article in enumerate(articles[:15]):
            try:
                title = article.text.strip()
                link = article.get_attribute("href")
                
                if not title or not link or len(title) < 3:
                    continue
                
                # ê³µì§€ì‚¬í•­ ë° ì¶”ì²œê¸€ í•„í„°ë§
                if any(keyword in title for keyword in ['ê³µì§€', 'í•„ë…', 'ì´ë²¤íŠ¸', 'ì¶”ì²œ', 'ë² ìŠ¤íŠ¸', 'ê³µì§€ì‚¬í•­']):
                    continue
                
                # URL ì •ê·œí™”
                if link.startswith('/'):
                    link = 'https://bbs.ruliweb.com' + link
                
                if link not in crawled_links:
                    post_data = {
                        "title": title,
                        "url": link,
                        "timestamp": datetime.now().isoformat(),
                        "source": "ruliweb_epic7"
                    }
                    posts.append(post_data)
                    crawled_links.append(link)
                    print(f"[NEW] ë£¨ë¦¬ì›¹ ìƒˆ ê²Œì‹œê¸€ ({i+1}): {title[:50]}...")
                
            except Exception as e:
                print(f"[ERROR] ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ {i+1} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
        
        print(f"[DEBUG] ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì™„ë£Œ: {len(posts)}ê°œ ìƒˆ ê²Œì‹œê¸€")
        
    except Exception as e:
        print(f"[ERROR] ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        if driver:
            try:
                with open("ruliweb_error_debug.html", "w", encoding="utf-8") as f:
                    f.write(driver.page_source)
            except:
                pass
    
    finally:
        if driver:
            driver.quit()
    
    # ë§í¬ ì €ì¥
    link_data["links"] = crawled_links
    save_crawled_links(link_data)
    
    return posts

def fetch_stove_bug_board():
    """ìŠ¤í† ë¸Œ ì—í”½ì„¸ë¸ ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
    posts = []
    link_data = load_crawled_links()
    crawled_links = link_data["links"]
    
    print(f"[DEBUG] ìŠ¤í† ë¸Œ í¬ë¡¤ë§ ì‹œì‘ - ì €ì¥ëœ ë§í¬ ìˆ˜: {len(crawled_links)}")
    
    driver = None
    try:
        print("[DEBUG] ìŠ¤í† ë¸Œìš© Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì¤‘...")
        driver = get_chrome_driver()
        
        driver.set_page_load_timeout(30)
        driver.implicitly_wait(10)
        
        url = "https://page.onstove.com/epicseven/kr/list/1012?page=1&direction=LATEST"
        print(f"[DEBUG] ìŠ¤í† ë¸Œ ì ‘ì† ì¤‘: {url}")
        
        driver.get(url)
        
        print("[DEBUG] ìŠ¤í† ë¸Œ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘...")
        time.sleep(8)
        
        # ìŠ¤í¬ë¡¤í•˜ì—¬ ì‹¤ì œ ìœ ì € ê²Œì‹œê¸€ ì˜ì—­ê¹Œì§€ ë¡œë”©
        print("[DEBUG] ìœ ì € ê²Œì‹œê¸€ ì˜ì—­ê¹Œì§€ ìŠ¤í¬ë¡¤...")
        driver.execute_script("window.scrollTo(0, 500);")
        time.sleep(3)
        driver.execute_script("window.scrollTo(0, 800);")
        time.sleep(3)
        driver.execute_script("window.scrollTo(0, 1200);")
        time.sleep(3)
        driver.execute_script("window.scrollTo(0, 0);")
        time.sleep(2)
        
        # ë””ë²„ê¹…ìš© HTML ì €ì¥
        html_content = driver.page_source
        with open("stove_debug_selenium.html", "w", encoding="utf-8") as f:
            f.write(html_content)
        
        print("[DEBUG] ì‹¤ì œ ìœ ì € ê²Œì‹œê¸€ ì˜ì—­ íƒìƒ‰ ì¤‘...")
        
        # ğŸ”§ ê¸°ì¡´ JavaScript ë¡œì§ ì‚¬ìš©
        user_posts = driver.execute_script("""
            var userPosts = [];
            
            var sections = document.querySelectorAll('section.s-board-item');
            console.log('ì „ì²´ ê²Œì‹œê¸€ ì„¹ì…˜ ìˆ˜:', sections.length);
            
            var officialIds = ['10518001', '10855687', '10855562', '10855132'];
            
            for (var i = 0; i < sections.length; i++) {
                var section = sections[i];
                
                try {
                    var linkElement = section.querySelector('a.s-board-link');
                    if (!linkElement) continue;
                    
                    var href = linkElement.href;
                    if (!href) continue;
                    
                    var idMatch = href.match(/\/view\/(\d+)/);
                    if (!idMatch) continue;
                    var postId = idMatch[1];
                    
                    if (officialIds.includes(postId)) {
                        console.log('ê³µì§€ ID ì œì™¸:', postId);
                        continue;
                    }
                    
                    var isNotice = section.querySelector('i.element-badge__s.notice');
                    var isEvent = section.querySelector('i.element-badge__s.event');
                    var isOfficial = section.querySelector('span.s-profile-staff-official');
                    
                    if (isNotice || isEvent || isOfficial) {
                        console.log('ê³µì§€/ì´ë²¤íŠ¸ ì œì™¸:', postId);
                        continue;
                    }
                    
                    var titleElement = section.querySelector('h3.s-board-title span.s-board-title-text');
                    if (!titleElement) {
                        console.log('ì œëª© ìš”ì†Œ ì—†ìŒ:', postId);
                        continue;
                    }
                    
                    var title = titleElement.innerText.trim();
                    if (!title || title.length < 3) {
                        console.log('ì œëª© ì—†ìŒ ë˜ëŠ” ë„ˆë¬´ ì§§ìŒ:', postId);
                        continue;
                    }
                    
                    userPosts.push({
                        title: title.substring(0, 200).trim(),
                        href: href,
                        id: postId
                    });
                    
                    console.log('ìœ ì € ê²Œì‹œê¸€ ì¶”ê°€:', title.substring(0, 30));
                    
                } catch (e) {
                    console.log('ê²Œì‹œê¸€ ì²˜ë¦¬ ì˜¤ë¥˜:', e.message);
                    continue;
                }
            }
            
            console.log('ìµœì¢… ë°œê²¬ëœ ìœ ì € ê²Œì‹œê¸€ ìˆ˜:', userPosts.length);
            return userPosts.slice(0, 15);
        """)
        
        print(f"[DEBUG] JavaScriptë¡œ {len(user_posts)}ê°œ ìœ ì € ê²Œì‹œê¸€ ë°œê²¬")
        
        # ìœ ì € ê²Œì‹œê¸€ ì²˜ë¦¬
        for i, post_info in enumerate(user_posts, 1):
            try:
                href = post_info['href']
                title = post_info['title']
                
                # URL ìˆ˜ì •
                if href.startswith('ttps://'):
                    href = 'h' + href
                elif not href.startswith('http'):
                    href = "https://page.onstove.com" + href
                
                print(f"[DEBUG] ìŠ¤í† ë¸Œ ê²Œì‹œê¸€ {i}: URL = {href[-50:]}")
                print(f"[DEBUG] ìŠ¤í† ë¸Œ ê²Œì‹œê¸€ {i}: ì œëª© = {title[:50]}...")
                
                if href in crawled_links:
                    print(f"[DEBUG] ìŠ¤í† ë¸Œ ê²Œì‹œê¸€ {i}: ì´ë¯¸ í¬ë¡¤ë§ëœ ë§í¬")
                    continue
                
                if title and href and len(title) > 3:
                    post_data = {
                        "title": title,
                        "url": href,
                        "timestamp": datetime.now().isoformat(),
                        "source": "stove_bug"
                    }
                    posts.append(post_data)
                    crawled_links.append(href)
                    print(f"[NEW] ìŠ¤í† ë¸Œ ìƒˆ ê²Œì‹œê¸€ ë°œê²¬ ({i}): {title[:50]}...")
                else:
                    print(f"[DEBUG] ìŠ¤í† ë¸Œ ê²Œì‹œê¸€ {i}: ì¡°ê±´ ë¯¸ì¶©ì¡±")
                
            except Exception as e:
                print(f"[ERROR] ìŠ¤í† ë¸Œ ê²Œì‹œê¸€ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        print(f"[DEBUG] ìŠ¤í† ë¸Œ ì²˜ë¦¬ ê²°ê³¼: {len(user_posts)}ê°œ ì¤‘ ìƒˆ ê²Œì‹œê¸€ {len(posts)}ê°œ ë°œê²¬")
        
    except Exception as e:
        print(f"[ERROR] ìŠ¤í† ë¸Œ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
    finally:
        if driver:
            print("[DEBUG] ìŠ¤í† ë¸Œ Chrome ë“œë¼ì´ë²„ ì¢…ë£Œ ì¤‘...")
            driver.quit()
    
    # ì¤‘ë³µ ë°©ì§€ ë§í¬ ì €ì¥
    link_data["links"] = crawled_links
    save_crawled_links(link_data)
    
    print(f"[DEBUG] ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œíŒì—ì„œ {len(posts)}ê°œ ìƒˆ ê²Œì‹œê¸€ ë°œê²¬")
    return posts

def crawl_arca_sites():
    """êµ­ë‚´ ì‚¬ì´íŠ¸ë“¤ í¬ë¡¤ë§ (ì•„ì¹´ë¼ì´ë¸Œ + ë£¨ë¦¬ì›¹ + ìŠ¤í† ë¸Œ)"""
    all_posts = []
    
    try:
        print("[INFO] === êµ­ë‚´ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹œì‘ ===")
        
        # 1. ì•„ì¹´ë¼ì´ë¸Œ í¬ë¡¤ë§
        print("[INFO] 1/3 ì•„ì¹´ë¼ì´ë¸Œ ì—í”½ì„¸ë¸ ì±„ë„ í¬ë¡¤ë§")
        arca_posts = fetch_arca_epic7_board()
        all_posts.extend(arca_posts)
        print(f"[INFO] ì•„ì¹´ë¼ì´ë¸Œ: {len(arca_posts)}ê°œ ìƒˆ ê²Œì‹œê¸€")
        
        # í¬ë¡¤ë§ ê°„ ì§€ì—° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
        time.sleep(random.uniform(5, 8))
        
        # 2. ë£¨ë¦¬ì›¹ í¬ë¡¤ë§
        print("[INFO] 2/3 ë£¨ë¦¬ì›¹ ì—í”½ì„¸ë¸ ê²Œì‹œíŒ í¬ë¡¤ë§")
        ruliweb_posts = fetch_ruliweb_epic7_board()
        all_posts.extend(ruliweb_posts)
        print(f"[INFO] ë£¨ë¦¬ì›¹: {len(ruliweb_posts)}ê°œ ìƒˆ ê²Œì‹œê¸€")
        
        # í¬ë¡¤ë§ ê°„ ì§€ì—°
        time.sleep(random.uniform(5, 8))
        
        # 3. ìŠ¤í† ë¸Œ í¬ë¡¤ë§
        print("[INFO] 3/3 ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§")
        stove_posts = fetch_stove_bug_board()
        all_posts.extend(stove_posts)
        print(f"[INFO] ìŠ¤í† ë¸Œ: {len(stove_posts)}ê°œ ìƒˆ ê²Œì‹œê¸€")
        
    except Exception as e:
        print(f"[ERROR] êµ­ë‚´ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    print(f"[INFO] === êµ­ë‚´ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_posts)}ê°œ ìƒˆ ê²Œì‹œê¸€ ===")
    return all_posts

def crawl_global_sites():
    """ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ë“¤ í¬ë¡¤ë§ (ì¶”í›„ êµ¬í˜„)"""
    print("[DEBUG] ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ì€ ì•„ì§ êµ¬í˜„ë˜ì§€ ì•ŠìŒ")
    return []

def get_all_posts_for_report():
    """ì¼ì¼ ë¦¬í¬íŠ¸ìš© - ìƒˆ ê²Œì‹œê¸€ë§Œì´ ì•„ë‹Œ ìµœê·¼ 24ì‹œê°„ ê²Œì‹œê¸€"""
    print("[INFO] ì¼ì¼ ë¦¬í¬íŠ¸ìš© ê²Œì‹œê¸€ ìˆ˜ì§‘ ì¤‘...")
    return crawl_arca_sites() + crawl_global_sites()

# ğŸ”§ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_all_crawling():
    """ì „ì²´ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸"""
    print("=== ì „ì²´ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ===")
    
    print("\n1. ì•„ì¹´ë¼ì´ë¸Œ í…ŒìŠ¤íŠ¸:")
    arca_posts = fetch_arca_epic7_board()
    
    print("\n2. ë£¨ë¦¬ì›¹ í…ŒìŠ¤íŠ¸:")
    ruliweb_posts = fetch_ruliweb_epic7_board()
    
    print("\n3. ìŠ¤í† ë¸Œ í…ŒìŠ¤íŠ¸:")
    stove_posts = fetch_stove_bug_board()
    
    print(f"\n=== í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===")
    print(f"ì•„ì¹´ë¼ì´ë¸Œ: {len(arca_posts)}ê°œ")
    print(f"ë£¨ë¦¬ì›¹: {len(ruliweb_posts)}ê°œ")
    print(f"ìŠ¤í† ë¸Œ: {len(stove_posts)}ê°œ")
    print(f"ì´ í•©ê³„: {len(arca_posts) + len(ruliweb_posts) + len(stove_posts)}ê°œ")
    
    return arca_posts + ruliweb_posts + stove_posts

if __name__ == "__main__":
    test_all_crawling()
