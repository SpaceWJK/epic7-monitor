#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Epic7 ë‹¤êµ­ê°€ í¬ë¡¤ëŸ¬ v3.6 - ë³¸ë¬¸ ì¶”ì¶œ ìµœì í™” ì™„ë£Œë³¸ 
ğŸ”¥ 3ìˆœìœ„ ë¬¸ì œ ì™„ì „ í•´ê²°: "ì½˜í…ì¸  ì¶”ì¶œ ê°œì„ "

í•µì‹¬ ê°œì„  ì‚¬í•­:
- âœ… ì˜ë¯¸ìˆëŠ” ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ ì•Œê³ ë¦¬ì¦˜ ë„ì…
- âœ… ë©”íƒ€ë°ì´í„° í•„í„°ë§ ì‹œìŠ¤í…œ ê°•í™”  
- âœ… ìµœì†Œ ê¸¸ì´ 50ì ì´ìƒìœ¼ë¡œ ì¦ê°€
- âœ… 2025ë…„ Stove êµ¬ì¡° ìµœì í™” CSS Selector ì¬ë°°ì¹˜
- âœ… ë‹¤ë‹¨ê³„ í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ ì ìš©

Author: Epic7 Monitoring Team
Version: 3.6
Date: 2025-07-22
"""

import time
import random
import re
import requests
import concurrent.futures
import os
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from urllib.parse import urljoin, urlparse

# Selenium ê´€ë ¨ import
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.keys import Keys

# =============================================================================
# í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ ì„¤ì • í´ë˜ìŠ¤
# =============================================================================

class CrawlingSchedule:
    """í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ë³„ ì„¤ì • ê´€ë¦¬"""
    
    FREQUENT_WAIT_TIME = 30      # ë²„ê·¸ ê²Œì‹œíŒ ëŒ€ê¸°ì‹œê°„
    REGULAR_WAIT_TIME = 35       # ì¼ë°˜ ê²Œì‹œíŒ ëŒ€ê¸°ì‹œê°„  
    REDDIT_WAIT_TIME = 20        # Reddit ëŒ€ê¸°ì‹œê°„
    RULIWEB_WAIT_TIME = 22       # ë£¨ë¦¬ì›¹ ëŒ€ê¸°ì‹œê°„
    
    # ìŠ¤í¬ë¡¤ íšŸìˆ˜ ì„¤ì •
    FREQUENT_SCROLL_COUNT = 3
    REGULAR_SCROLL_COUNT = 5
    
    @staticmethod
    def get_wait_time(schedule_type: str) -> int:
        """ìŠ¤ì¼€ì¤„ íƒ€ì…ë³„ ëŒ€ê¸°ì‹œê°„ ë°˜í™˜"""
        if schedule_type == 'frequent':
            return CrawlingSchedule.FREQUENT_WAIT_TIME
        elif schedule_type == 'regular':
            return CrawlingSchedule.REGULAR_WAIT_TIME
        elif schedule_type == 'reddit':
            return CrawlingSchedule.REDDIT_WAIT_TIME
        elif schedule_type == 'ruliweb':
            return CrawlingSchedule.RULIWEB_WAIT_TIME
        else:
            return CrawlingSchedule.REGULAR_WAIT_TIME

    @staticmethod
    def get_scroll_count(schedule_type: str) -> int:
        """ìŠ¤ì¼€ì¤„ íƒ€ì…ë³„ ìŠ¤í¬ë¡¤ íšŸìˆ˜ ë°˜í™˜"""
        if schedule_type == 'frequent':
            return CrawlingSchedule.FREQUENT_SCROLL_COUNT
        else:
            return CrawlingSchedule.REGULAR_SCROLL_COUNT

# =============================================================================
# íŒŒì¼ ê´€ë¦¬ ì‹œìŠ¤í…œ
# =============================================================================

def get_crawled_links_file():
    """ì›Œí¬í”Œë¡œìš°ë³„ ë…ë¦½ì ì¸ í¬ë¡¤ë§ ë§í¬ íŒŒì¼ëª… ìƒì„±"""
    workflow_name = os.environ.get('GITHUB_WORKFLOW', 'default')
    
    if 'debug' in workflow_name.lower() or 'test' in workflow_name.lower():
        return "crawled_links_debug.json"
    elif 'monitor' in workflow_name.lower():
        return "crawled_links_monitor.json"
    else:
        return "crawled_links.json"

def get_content_cache_file():
    """ì›Œí¬í”Œë¡œìš°ë³„ ë…ë¦½ì ì¸ ì½˜í…ì¸  ìºì‹œ íŒŒì¼ëª… ìƒì„±"""
    workflow_name = os.environ.get('GITHUB_WORKFLOW', 'default')
    
    if 'debug' in workflow_name.lower():
        return "content_cache_debug.json"
    else:
        return "content_cache.json"

def load_crawled_links():
    """ì´ë¯¸ í¬ë¡¤ë§ëœ ë§í¬ë“¤ì„ ë¡œë“œ"""
    crawled_links_file = get_crawled_links_file()
    
    if os.path.exists(crawled_links_file):
        try:
            with open(crawled_links_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, list):
                    return {"links": data, "last_updated": datetime.now().isoformat()}
                return data
        except (json.JSONDecodeError, FileNotFoundError):
            print(f"[WARNING] {crawled_links_file} íŒŒì¼ ì½ê¸° ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±")
    
    return {"links": [], "last_updated": datetime.now().isoformat()}

def save_crawled_links(link_data):
    """í¬ë¡¤ë§ëœ ë§í¬ë“¤ì„ ì €ì¥ (ìµœëŒ€ 1000ê°œ ìœ ì§€)"""
    try:
        if len(link_data["links"]) > 1000:
            link_data["links"] = link_data["links"][-1000:]
        
        link_data["last_updated"] = datetime.now().isoformat()
        
        crawled_links_file = get_crawled_links_file()
        with open(crawled_links_file, 'w', encoding='utf-8') as f:
            json.dump(link_data, f, ensure_ascii=False, indent=2)
            
    except Exception as e:
        print(f"[ERROR] ë§í¬ ì €ì¥ ì‹¤íŒ¨: {e}")

def load_content_cache():
    """ê²Œì‹œê¸€ ë‚´ìš© ìºì‹œ ë¡œë“œ"""
    content_cache_file = get_content_cache_file()
    
    if os.path.exists(content_cache_file):
        try:
            with open(content_cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            print(f"[WARNING] {content_cache_file} íŒŒì¼ ì½ê¸° ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±")
    return {}

def save_content_cache(cache_data):
    """ê²Œì‹œê¸€ ë‚´ìš© ìºì‹œ ì €ì¥"""
    try:
        if len(cache_data) > 500:
            sorted_items = sorted(cache_data.items(), 
                                key=lambda x: x[1].get('timestamp', ''), reverse=True)
            cache_data = dict(sorted_items[:500])
        
        content_cache_file = get_content_cache_file()
        with open(content_cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
            
    except Exception as e:
        print(f"[ERROR] ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

# =============================================================================
# Chrome Driver ê´€ë¦¬
# =============================================================================

def get_chrome_driver():
    """Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” (Chrome 138+ í˜¸í™˜)"""
    options = Options()
    
    # ê¸°ë³¸ ì˜µì…˜ë“¤
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--disable-extensions')
    options.add_argument('--disable-plugins')
    options.add_argument('--disable-images')
    options.add_argument('--window-size=1920,1080')
    
    # ë´‡ íƒì§€ ìš°íšŒ
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    
    # ëœë¤ User-Agent
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36'
    ]
    options.add_argument(f'--user-agent={random.choice(user_agents)}')
    
    # ì„±ëŠ¥ ìµœì í™”
    prefs = {
        'profile.default_content_setting_values': {
            'images': 2, 'plugins': 2, 'popups': 2,
            'geolocation': 2, 'notifications': 2, 'media_stream': 2,
        }
    }
    options.add_experimental_option('prefs', prefs)
    
    # 3ë‹¨ê³„ í´ë°± ë©”ì»¤ë‹ˆì¦˜
    possible_paths = [
        '/usr/bin/chromedriver',
        '/usr/local/bin/chromedriver', 
        '/snap/bin/chromium.chromedriver'
    ]
    
    # 1ë‹¨ê³„: Chrome for Testing API
    try:
        response = requests.get('https://googlechromelabs.github.io/chrome-for-testing/LATEST_RELEASE_STABLE')
        if response.status_code == 200:
            version = response.text.strip()
            print(f"[DEBUG] ìµœì‹  Chrome ë²„ì „: {version}")
    except:
        pass
    
    # 2ë‹¨ê³„: ì‹œìŠ¤í…œ ê²½ë¡œë“¤ ì‹œë„
    for path in possible_paths:
        try:
            if os.path.exists(path):
                print(f"[DEBUG] ChromeDriver ì‹œë„: {path}")
                service = Service(path)
                driver = webdriver.Chrome(service=service, options=options)
                print(f"[DEBUG] ChromeDriver ì„±ê³µ: {path}")
                return driver
        except Exception as e:
            print(f"[DEBUG] ChromeDriver ì‹¤íŒ¨ {path}: {str(e)[:100]}...")
            continue
    
    # 3ë‹¨ê³„: WebDriver Manager
    try:
        print("[DEBUG] WebDriver Manager ì‹œë„")
        from webdriver_manager.chrome import ChromeDriverManager
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        print("[DEBUG] WebDriver Manager ì„±ê³µ")
        return driver
    except Exception as e:
        print(f"[DEBUG] WebDriver Manager ì‹¤íŒ¨: {str(e)[:100]}...")
    
    raise Exception("ëª¨ë“  ChromeDriver ì´ˆê¸°í™” ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

# =============================================================================
# URL ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°
# =============================================================================

def fix_url_bug(url):
    """URL ë²„ê·¸ ìˆ˜ì • í•¨ìˆ˜ (998, 989, 1012, 1005 ë“±)"""
    if not url:
        return url
    
    # ttps:// â†’ https:// ìˆ˜ì •
    if url.startswith('ttps://'):
        url = 'h' + url
        print(f"[URL FIX] ttps â†’ https: {url}")
    
    # ìƒëŒ€ ê²½ë¡œ â†’ ì ˆëŒ€ ê²½ë¡œ
    elif url.startswith('/'):
        if 'onstove.com' in url or 'epicseven' in url:
            url = 'https://page.onstove.com' + url
        elif 'ruliweb.com' in url:
            url = 'https://bbs.ruliweb.com' + url
        elif 'reddit.com' in url:
            url = 'https://www.reddit.com' + url
        print(f"[URL FIX] ìƒëŒ€ê²½ë¡œ ìˆ˜ì •: {url}")
    
    # í”„ë¡œí† ì½œ ëˆ„ë½
    elif not url.startswith(('http://', 'https://')):
        url = 'https://' + url
        print(f"[URL FIX] í”„ë¡œí† ì½œ ì¶”ê°€: {url}")
    
    return url

# =============================================================================
# ğŸ”¥ í•µì‹¬ ê°œì„ : ì˜ë¯¸ìˆëŠ” ë³¸ë¬¸ ì¶”ì¶œ í•¨ìˆ˜
# =============================================================================

def extract_meaningful_content(text: str) -> str:
    """ğŸ”¥ ìƒˆë¡œ ì¶”ê°€: ì˜ë¯¸ìˆëŠ” ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ ì•Œê³ ë¦¬ì¦˜"""
    if not text or len(text) < 30:
        return ""
    
    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í•  (ê°œì„ ëœ ì •ê·œì‹)
    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]\s*', text.strip())
    sentences = [s.strip() for s in sentences if s.strip()]
    
    if not sentences:
        return text[:100].strip()
    
    # ğŸ¯ ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ í•„í„°ë§ ì‹œìŠ¤í…œ
    meaningful_sentences = []
    
    for sentence in sentences:
        if len(sentence) < 10:  # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸
            continue
            
        # ì˜ë¯¸ì—†ëŠ” ë¬¸ì¥ íŒ¨í„´ ì œì™¸
        meaningless_patterns = [
            r'^[ã…‹ã…ã„·ã… ã…œã…¡]+$',  # ììŒëª¨ìŒë§Œ
            r'^[!@#$%^&*()_+\-=\[\]{}|;\':",./<>?`~]+$',  # íŠ¹ìˆ˜ë¬¸ìë§Œ
            r'^\d+$',  # ìˆ«ìë§Œ
            r'^(ìŒ|ì–´|ì•„|ë„¤|ì˜ˆ|ì‘|ã…‡ã…‡|ã… ã… |ã…œã…œ)$',  # ë‹¨ìˆœ ê°íƒ„ì‚¬
        ]
        
        if any(re.match(pattern, sentence) for pattern in meaningless_patterns):
            continue
        
        # Epic7 ê´€ë ¨ ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œ ìŠ¤ì½”ì–´ë§
        meaningful_keywords = [
            'ë²„ê·¸', 'ì˜¤ë¥˜', 'ë¬¸ì œ', 'ì—ëŸ¬', 'ì•ˆë¨', 'ì‘ë™', 'ì‹¤í–‰',
            'ìºë¦­í„°', 'ìŠ¤í‚¬', 'ì•„í‹°íŒ©íŠ¸', 'ì¥ë¹„', 'ë˜ì „', 'ì•„ë ˆë‚˜', 
            'ê¸¸ë“œ', 'ì´ë²¤íŠ¸', 'ì—…ë°ì´íŠ¸', 'íŒ¨ì¹˜', 'ë°¸ëŸ°ìŠ¤', 'ë„ˆí”„',
            'ê²Œì„', 'í”Œë ˆì´', 'ìœ ì €', 'ìš´ì˜', 'ê³µì§€', 'í™•ë¥ ',
            'ë½‘ê¸°', 'ì†Œí™˜', '6ì„±', 'ê°ì„±', 'ì´ˆì›”', 'ë£¬', 'ì ¬'
        ]
        
        score = sum(1 for keyword in meaningful_keywords if keyword in sentence)
        
        # ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ìœ¼ë¡œ íŒë³„ (í‚¤ì›Œë“œ ì ìˆ˜ ë˜ëŠ” ì¶©ë¶„í•œ ê¸¸ì´)
        if score > 0 or len(sentence) >= 30:
            meaningful_sentences.append(sentence)
    
    if not meaningful_sentences:
        # í´ë°±: ì²« ë²ˆì§¸ ê¸´ ë¬¸ì¥
        long_sentences = [s for s in sentences if len(s) >= 20]
        if long_sentences:
            return long_sentences[0]
        else:
            return sentences[0] if sentences else text[:100]
    
    # ğŸ¯ ìµœì  ì¡°í•©: 1-3ê°œ ë¬¸ì¥ ì¡°í•©ìœ¼ë¡œ ì˜ë¯¸ìˆëŠ” ë‚´ìš© êµ¬ì„±
    result = meaningful_sentences[0]
    
    # ì²« ë²ˆì§¸ ë¬¸ì¥ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ë‘ ë²ˆì§¸ ë¬¸ì¥ ì¶”ê°€
    if len(result) < 50 and len(meaningful_sentences) > 1:
        result += ' ' + meaningful_sentences[1]
    
    # ì—¬ì „íˆ ë¶€ì¡±í•˜ë©´ ì„¸ ë²ˆì§¸ ë¬¸ì¥ê¹Œì§€ ì¶”ê°€
    if len(result) < 80 and len(meaningful_sentences) > 2:
        result += ' ' + meaningful_sentences[2]
    
    return result.strip()

# =============================================================================
# ğŸ”¥ í•µì‹¬ ìˆ˜ì •: ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ í•¨ìˆ˜ - ë³¸ë¬¸ ì¶”ì¶œ ë¡œì§ ì™„ì „ ê°œì„ 
# =============================================================================

def get_stove_post_content(post_url: str, driver: webdriver.Chrome, 
                          source: str = "stove_korea_bug", 
                          schedule_type: str = "frequent") -> str:
    """ìŠ¤í† ë¸Œ ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ - ë³¸ë¬¸ ì¶”ì¶œ ë¡œì§ ì™„ì „ ê°œì„  v3.6"""
    
    # ìºì‹œ í™•ì¸
    cache = load_content_cache()
    url_hash = hash(post_url) % (10**8)
    
    if str(url_hash) in cache:
        cached_item = cache[str(url_hash)]
        cache_time = datetime.fromisoformat(cached_item.get('timestamp', '2000-01-01'))
        if datetime.now() - cache_time < timedelta(hours=24):
            print(f"[CACHE] ìºì‹œëœ ë‚´ìš© ì‚¬ìš©: {post_url}")
            return cached_item.get('content', "ê²Œì‹œê¸€ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    content_summary = "ê²Œì‹œê¸€ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    try:
        print(f"[DEBUG] ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ ì‹œë„: {post_url}")
        
        wait_time = CrawlingSchedule.get_wait_time(schedule_type)
        driver.set_page_load_timeout(wait_time + 10)
        driver.get(post_url)
        
        print(f"[DEBUG] í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘... ({wait_time}ì´ˆ)")
        time.sleep(wait_time)
        
        # JavaScript ì™„ì „ ë¡œë”© í™•ì¸
        WebDriverWait(driver, 15).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        
        # ë‹¨ê³„ë³„ ìŠ¤í¬ë¡¤ë§
        print("[DEBUG] ë‹¨ê³„ë³„ ìŠ¤í¬ë¡¤ë§ ì‹œì‘...")
        driver.execute_script("window.scrollTo(0, 500);")
        time.sleep(3)
        driver.execute_script("window.scrollTo(0, 800);")
        time.sleep(3)
        driver.execute_script("window.scrollTo(0, 1200);")
        time.sleep(3)
        driver.execute_script("window.scrollTo(0, 0);")
        time.sleep(2)
        print("[DEBUG] ë‹¨ê³„ë³„ ìŠ¤í¬ë¡¤ë§ ì™„ë£Œ")
        
        # ìŠ¤í† ë¸Œ ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œìš© CSS Selector (0714 ì„±ê³µ ë²„ì „)
        content_selectors = [
            # Vue.js ë©”íƒ€ íƒœê·¸ì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ (ìµœìš°ì„ )
            'meta[data-vmid="description"]',
            'meta[name="description"]',
    
            # ë°±ì—… ì„ íƒìë“¤
            'div.s-article-content',
            'div.s-article-content-text',
            'section.s-article-body',
            'div.s-board-content'            
        ]
           
        # ğŸš€ í•µì‹¬ ê°œì„ : ì˜ë¯¸ìˆëŠ” ë³¸ë¬¸ ì¶”ì¶œ ì•Œê³ ë¦¬ì¦˜
        for i, selector in enumerate(content_selectors):
            try:
                elements = driver.find_elements(By.CSS_SELECTOR, selector)
                if elements:
                    for element in elements:
                        # ë©”íƒ€ íƒœê·¸ëŠ” content ì†ì„±ì—ì„œ, ì¼ë°˜ íƒœê·¸ëŠ” textì—ì„œ ì¶”ì¶œ
                        if selector.startswith('meta'):
                            raw_text = element.get_attribute('content').strip()
                        else:
                            raw_text = element.text.strip()
                        if not raw_text or len(raw_text) < 30:
                            continue           
                                            
                        # ğŸ”¥ ê°œì„  1: ë©”íƒ€ë°ì´í„° í•„í„°ë§ ê°•í™”
                        skip_keywords = [
                            'install stove', 'ìŠ¤í† ë¸Œë¥¼ ì„¤ì¹˜', 'ë¡œê·¸ì¸ì´ í•„ìš”', 
                            'javascript', 'ëŒ“ê¸€', 'ê³µìœ ', 'ì¢‹ì•„ìš”', 'ì¶”ì²œ', 'ì‹ ê³ ',
                            'ì‘ì„±ì', 'ì‘ì„±ì¼', 'ì¡°íšŒìˆ˜', 'ì²¨ë¶€íŒŒì¼', 'ë‹¤ìš´ë¡œë“œ',
                            'copyright', 'ì €ì‘ê¶Œ', 'ì´ìš©ì•½ê´€', 'ê°œì¸ì •ë³´', 'ì¿ í‚¤',
                            'ê´‘ê³ ', 'ad', 'advertisement', 'í”„ë¡œëª¨ì…˜', 'ì´ë²¤íŠ¸',
                            'ë¡œê·¸ì¸', 'login', 'sign in', 'íšŒì›ê°€ì…', 'register',
                            'ë©”ë‰´', 'menu', 'navigation', 'ë„¤ë¹„ê²Œì´ì…˜', 'ì‚¬ì´ë“œë°”',
                            'ë°°ë„ˆ', 'banner', 'í‘¸í„°', 'footer', 'í—¤ë”', 'header'
                        ]
                        
                        if any(skip.lower() in raw_text.lower() for skip in skip_keywords):
                            continue
                        
                        # ğŸ”¥ ê°œì„  2: ì˜ë¯¸ìˆëŠ” ë¬¸ë‹¨ ì¶”ì¶œ (ì²« ë¬¸ì¥ â†’ ë¬¸ë‹¨ ì¡°í•©)
                        meaningful_content = extract_meaningful_content(raw_text)
                        
                        # ğŸ”¥ ê°œì„  3: ìµœì†Œ ê¸¸ì´ 50ì ì´ìƒìœ¼ë¡œ ì¦ê°€
                        if len(meaningful_content) >= 50:
                            # 150ì ì´ë‚´ë¡œ ìš”ì•½
                            if len(meaningful_content) > 150:
                                content_summary = meaningful_content[:147] + '...'
                            else:
                                content_summary = meaningful_content
                            
                            print(f"[SUCCESS] ì„ íƒì {i+1}/{len(content_selectors)} '{selector}'ë¡œ ë‚´ìš© ì¶”ì¶œ ì„±ê³µ")
                            print(f"[CONTENT] {content_summary[:80]}...")
                            break
                    
                    if content_summary != "ê²Œì‹œê¸€ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.":
                        break
                        
            except Exception as e:
                print(f"[DEBUG] ì„ íƒì '{selector}' ì‹¤íŒ¨: {e}")
                continue
        
        # ìºì‹œ ì €ì¥
        cache[str(url_hash)] = {
            'content': content_summary,
            'timestamp': datetime.now().isoformat(),
            'url': post_url,
            'source': source
        }
        save_content_cache(cache)
        
    except TimeoutException:
        print(f"[ERROR] í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ: {post_url}")
        content_summary = "â° ê²Œì‹œê¸€ ë¡œë”© ì‹œê°„ ì´ˆê³¼"
    except Exception as e:
        print(f"[ERROR] ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        content_summary = "ğŸ”— ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ ì‹¤íŒ¨"
    
    return content_summary

# =============================================================================
# ìŠ¤í† ë¸Œ ê²Œì‹œíŒ í¬ë¡¤ë§ í•¨ìˆ˜ - CSS Selector ìˆ˜ì •
# =============================================================================

def crawl_stove_board(board_url: str, source: str, force_crawl: bool = False, 
                     schedule_type: str = "frequent", region: str = "korea") -> List[Dict]:
    """ìŠ¤í† ë¸Œ ê²Œì‹œíŒ í¬ë¡¤ë§ - CSS Selector ìˆ˜ì •"""
    
    posts = []
    link_data = load_crawled_links()
    crawled_links = link_data["links"]
    
    print(f"[INFO] {source} í¬ë¡¤ë§ ì‹œì‘ - URL: {board_url}")
    print(f"[DEBUG] ê¸°ì¡´ ë§í¬ ìˆ˜: {len(crawled_links)}, Force Crawl: {force_crawl}")
    
    driver = None
    try:
        driver = get_chrome_driver()
        
        wait_time = CrawlingSchedule.get_wait_time(schedule_type)
        driver.set_page_load_timeout(wait_time + 10)
        driver.implicitly_wait(15)
        
        print(f"[DEBUG] ê²Œì‹œíŒ ì ‘ì† ì¤‘: {board_url}")
        driver.get(board_url)
        
        print(f"[DEBUG] í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘... ({wait_time}ì´ˆ)")
        time.sleep(wait_time)
        
        # JavaScript ì™„ì „ ë¡œë”© í™•ì¸
        WebDriverWait(driver, 15).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        
        # ê²Œì‹œê¸€ ëª©ë¡ ì˜ì—­ê¹Œì§€ ìŠ¤í¬ë¡¤
        driver.execute_script("window.scrollTo(0, 800);")
        time.sleep(5)
        
        # ë””ë²„ê¹…ìš© HTML ì €ì¥
        debug_filename = f"{source}_debug_selenium.html"
        with open(debug_filename, "w", encoding="utf-8") as f:
            f.write(driver.page_source)
        print(f"[DEBUG] HTML ì €ì¥: {debug_filename}")
        
        # JavaScript CSS Selector ìˆ˜ì •
        user_posts = driver.execute_script("""
            var userPosts = [];
            
            // ìˆ˜ì •ëœ ì„ íƒì ìš°ì„ ìˆœìœ„ (section.s-board-item ìµœìš°ì„ )
            const selectors = [
                'section.s-board-item',           // âœ… 0714 ì„±ê³µ ì„ íƒì (ìµœìš°ì„ )
                'h3.s-board-title',               // ê¸°ì¡´ ì„ íƒì (ë°±ì—…)
                '[class*="board-title"]',         // í´ë˜ìŠ¤ëª… í¬í•¨
                '[class*="post-title"]',          // post-title í¬í•¨
                '[class*="article-title"]',       // article-title í¬í•¨
                'h3[class*="title"]',            // h3 íƒœê·¸ title í¬í•¨
                'a[href*="/view/"]'              // view ë§í¬ ì§ì ‘ ì°¾ê¸°
            ];
            
            var elements = [];
            var successful_selector = '';
            
            // ì„ íƒìë³„ ì‹œë„
            for (var i = 0; i < selectors.length; i++) {
                try {
                    elements = document.querySelectorAll(selectors[i]);
                    if (elements && elements.length > 0) {
                        successful_selector = selectors[i];
                        console.log('ì„ íƒì ì„±ê³µ:', selectors[i], 'ê°œìˆ˜:', elements.length);
                        break;
                    }
                } catch (e) {
                    console.log('ì„ íƒì ì‹¤íŒ¨:', selectors[i], e);
                    continue;
                }
            }
            
            if (!elements || elements.length === 0) {
                console.log('ëª¨ë“  ì„ íƒì ì‹¤íŒ¨');
                return [];
            }
            
            console.log('ì´ ë°œê²¬ëœ ìš”ì†Œ ìˆ˜:', elements.length);
            
            // ê³µì§€ì‚¬í•­ IDë“¤ (ì œì™¸ ëŒ€ìƒ)
            const officialIds = ['10518001', '10855687', '10855562', '10855132'];
            
            // ê° ìš”ì†Œì—ì„œ ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ
            for (var i = 0; i < Math.min(elements.length, 20); i++) {
                var element = elements[i];
                
                try {
                    var linkElement, titleElement;
                    var href = '', title = '';
                    
                    // ë§í¬ ìš”ì†Œ ì°¾ê¸°
                    if (successful_selector === 'section.s-board-item') {
                        // section ê¸°ë°˜ ì¶”ì¶œ
                        linkElement = element.querySelector('a[href*="/view/"]');
                        titleElement = element.querySelector('.s-board-title-text, .board-title, h3 span, .title');
                    } else {
                        // ê¸°íƒ€ ì„ íƒì ê¸°ë°˜ ì¶”ì¶œ
                        linkElement = element.closest('a[href*="/view/"]') || element.querySelector('a[href*="/view/"]');
                        titleElement = element;
                    }
                    
                    // ë§í¬ ì¶”ì¶œ
                    if (linkElement && linkElement.href) {
                        href = linkElement.href;
                    }
                    
                    // ì œëª© ì¶”ì¶œ
                    if (titleElement) {
                        title = titleElement.textContent?.trim() || titleElement.innerText?.trim() || '';
                    }
                    
                    // ìœ íš¨ì„± ê²€ì‚¬
                    if (!href || !title || title.length < 3) {
                        continue;
                    }
                    
                    // URLì—ì„œ ê²Œì‹œê¸€ ID ì¶”ì¶œ
                    var idMatch = href.match(/\/view\/(\d+)/);
                    if (!idMatch) {
                        continue;
                    }
                    var id = idMatch[1];
                    
                    // ê³µì§€ì‚¬í•­ ì œì™¸
                    if (officialIds.includes(id)) {
                        console.log('ê³µì§€ì‚¬í•­ ì œì™¸:', id, title.substring(0, 20));
                        continue;
                    }
                    
                    // ê³µì§€/ì´ë²¤íŠ¸ ë°°ì§€ í™•ì¸
                    var isNotice = element.querySelector('i.element-badge__s.notice, .notice, [class*="notice"]');
                    var isEvent = element.querySelector('i.element-badge__s.event, .event, [class*="event"]');
                    var isOfficial = element.querySelector('span.s-profile-staff-official, [class*="official"]');
                    
                    if (isNotice || isEvent || isOfficial) {
                        console.log('ê³µì§€/ì´ë²¤íŠ¸ ì œì™¸:', title.substring(0, 20));
                        continue;
                    }
                    
                    // ì œëª©ì—ì„œ [ê³µì§€], [ì´ë²¤íŠ¸] ë“± í‚¤ì›Œë“œ ì œì™¸  
                    var skipKeywords = ['[ê³µì§€]', '[ì´ë²¤íŠ¸]', '[ì•ˆë‚´]', '[ì ê²€]', '[ê³µì§€ì‚¬í•­]'];
                    var shouldSkip = skipKeywords.some(function(keyword) {
                        return title.includes(keyword);
                    });
                    
                    if (shouldSkip) {
                        console.log('í‚¤ì›Œë“œ ì œì™¸:', title.substring(0, 20));
                        continue;
                    }
                    
                    // URL ì •ê·œí™”
                    var fullUrl = href.startsWith('http') ? href : 'https://page.onstove.com' + href;
                    
                    userPosts.push({
                        href: fullUrl,
                        id: id,
                        title: title.substring(0, 200).trim(),
                        selector_used: successful_selector
                    });
                    
                    console.log('ìœ ì € ê²Œì‹œê¸€ ì¶”ê°€:', title.substring(0, 30));
                    
                } catch (e) {
                    console.log('ê²Œì‹œê¸€ ì²˜ë¦¬ ì˜¤ë¥˜:', e.message);
                    continue;
                }
            }
            
            console.log('ìµœì¢… ì¶”ì¶œëœ ìœ ì € ê²Œì‹œê¸€ ìˆ˜:', userPosts.length);
            return userPosts;
        """)
        
        print(f"[DEBUG] JavaScriptë¡œ {len(user_posts)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")
        
        # ê° ê²Œì‹œê¸€ ì²˜ë¦¬
        for i, post_info in enumerate(user_posts, 1):
            try:
                href = post_info['href']
                title = post_info['title']
                post_id = post_info['id']
                
                # URL ë²„ê·¸ ìˆ˜ì • ì ìš©
                href = fix_url_bug(href)
                
                print(f"[DEBUG] ê²Œì‹œê¸€ {i}/{len(user_posts)}: {title[:40]}...")
                print(f"[DEBUG] URL: {href}")
                
                # ì¤‘ë³µ í™•ì¸ (force_crawlì´ Falseì¸ ê²½ìš°)
                if not force_crawl and href in crawled_links:
                    print(f"[SKIP] ì´ë¯¸ í¬ë¡¤ë§ëœ ë§í¬: {post_id}")
                    continue
                
                # ì œëª© ê¸¸ì´ ê²€ì¦
                if len(title) < 5:
                    print(f"[SKIP] ì œëª©ì´ ë„ˆë¬´ ì§§ìŒ: {title}")
                    continue
                
                # ğŸ”¥ í•µì‹¬: ê°œì„ ëœ ë³¸ë¬¸ ì¶”ì¶œ í•¨ìˆ˜ ì‚¬ìš©
                content = get_stove_post_content(href, driver, source, schedule_type)
                
                # ê²Œì‹œê¸€ ë°ì´í„° êµ¬ì„±
                post_data = {
                    "title": title,
                    "url": href,
                    "content": content,
                    "timestamp": datetime.now().isoformat(),
                    "source": source,
                    "id": post_id,
                    "region": region,
                    "schedule_type": schedule_type
                }
                
                posts.append(post_data)
                crawled_links.append(href)
                
                print(f"[SUCCESS] ìƒˆ ê²Œì‹œê¸€ ì¶”ê°€ ({i}): {title[:30]}...")
                print(f"[CONTENT] {content[:80]}...")
                
                # í¬ë¡¤ë§ ê°„ ëŒ€ê¸° (Rate Limiting)
                time.sleep(random.uniform(2, 5))
                
            except Exception as e:
                print(f"[ERROR] ê²Œì‹œê¸€ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        print(f"[INFO] {source} í¬ë¡¤ë§ ì™„ë£Œ: {len(user_posts)}ê°œ ì¤‘ {len(posts)}ê°œ ìƒˆ ê²Œì‹œê¸€")
        
    except Exception as e:
        print(f"[ERROR] {source} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    finally:
        if driver:
            try:
                driver.quit()
            except:
                pass
    
    # ë§í¬ ë°ì´í„° ì €ì¥
    link_data["links"] = crawled_links
    save_crawled_links(link_data)
    
    return posts

# =============================================================================
# í†µí•© í¬ë¡¤ë§ í•¨ìˆ˜ë“¤
# =============================================================================

def crawl_frequent_sites(force_crawl: bool = False) -> List[Dict]:
    """15ë¶„ ì£¼ê¸° - ë²„ê·¸ ê²Œì‹œíŒ ì „ìš© í¬ë¡¤ë§"""
    all_posts = []
    
    print("[INFO] === 15ë¶„ ì£¼ê¸°: ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹œì‘ ===")
    
    try:
        # í•œêµ­ ë²„ê·¸ ê²Œì‹œíŒ
        stove_kr_bug_posts = crawl_stove_board(
            board_url="https://page.onstove.com/epicseven/kr/list/1012?page=1&direction=LATEST",
            source="stove_korea_bug",
            force_crawl=force_crawl,
            schedule_type="frequent",
            region="korea"
        )
        all_posts.extend(stove_kr_bug_posts)
        print(f"[INFO] í•œêµ­ ë²„ê·¸ ê²Œì‹œíŒ: {len(stove_kr_bug_posts)}ê°œ")
        
        # í¬ë¡¤ë§ ê°„ ëŒ€ê¸°
        time.sleep(random.uniform(8, 12))
        
        # ê¸€ë¡œë²Œ ë²„ê·¸ ê²Œì‹œíŒ
        stove_global_bug_posts = crawl_stove_board(
            board_url="https://page.onstove.com/epicseven/global/list/998?page=1&direction=LATEST",
            source="stove_global_bug",
            force_crawl=force_crawl,
            schedule_type="frequent", 
            region="global"
        )
        all_posts.extend(stove_global_bug_posts)
        print(f"[INFO] ê¸€ë¡œë²Œ ë²„ê·¸ ê²Œì‹œíŒ: {len(stove_global_bug_posts)}ê°œ")
        
    except Exception as e:
        print(f"[ERROR] ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    print(f"[INFO] === 15ë¶„ ì£¼ê¸° ì™„ë£Œ: ì´ {len(all_posts)}ê°œ ìƒˆ ê²Œì‹œê¸€ ===")
    return all_posts

def crawl_regular_sites(force_crawl: bool = False) -> List[Dict]:
    """30ë¶„ ì£¼ê¸° - ì¼ë°˜ ê²Œì‹œíŒ í¬ë¡¤ë§ (ê°ì„± ë¶„ì„ìš©)"""
    all_posts = []
    
    print("[INFO] === 30ë¶„ ì£¼ê¸°: ì¼ë°˜ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹œì‘ ===")
    
    try:
        # í•œêµ­ ììœ ê²Œì‹œíŒ
        stove_kr_general_posts = crawl_stove_board(
            board_url="https://page.onstove.com/epicseven/kr/list/1005?page=1&direction=LATEST",
            source="stove_korea_general", 
            force_crawl=force_crawl,
            schedule_type="regular",
            region="korea"
        )
        all_posts.extend(stove_kr_general_posts)
        print(f"[INFO] í•œêµ­ ììœ ê²Œì‹œíŒ: {len(stove_kr_general_posts)}ê°œ")
        
        time.sleep(random.uniform(8, 12))
        
        # ê¸€ë¡œë²Œ ììœ ê²Œì‹œíŒ
        stove_global_general_posts = crawl_stove_board(
            board_url="https://page.onstove.com/epicseven/global/list/989?page=1&direction=LATEST",
            source="stove_global_general",
            force_crawl=force_crawl,
            schedule_type="regular",
            region="global"
        )
        all_posts.extend(stove_global_general_posts)
        print(f"[INFO] ê¸€ë¡œë²Œ ììœ ê²Œì‹œíŒ: {len(stove_global_general_posts)}ê°œ")
        
        time.sleep(random.uniform(8, 12))
        
        # ë£¨ë¦¬ì›¹ (ì¶”ê°€)
        ruliweb_posts = crawl_ruliweb_epic7()
        all_posts.extend(ruliweb_posts)
        print(f"[INFO] ë£¨ë¦¬ì›¹: {len(ruliweb_posts)}ê°œ")
        
    except Exception as e:
        print(f"[ERROR] ì¼ë°˜ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    print(f"[INFO] === 30ë¶„ ì£¼ê¸° ì™„ë£Œ: ì´ {len(all_posts)}ê°œ ìƒˆ ê²Œì‹œê¸€ ===")
    return all_posts

def crawl_by_schedule(schedule_type: str, force_crawl: bool = False) -> List[Dict]:
    """ìŠ¤ì¼€ì¤„ íƒ€ì…ì— ë”°ë¥¸ í¬ë¡¤ë§ ë¶„ê¸°"""
    
    if schedule_type == "frequent" or schedule_type == "15min":
        return crawl_frequent_sites(force_crawl)
    elif schedule_type == "regular" or schedule_type == "30min":
        return crawl_regular_sites(force_crawl)
    else:
        print(f"[ERROR] ì•Œ ìˆ˜ ì—†ëŠ” ìŠ¤ì¼€ì¤„ íƒ€ì…: {schedule_type}")
        return []

def get_all_posts_for_report() -> List[Dict]:
    """ë¦¬í¬íŠ¸ìš© - ëª¨ë“  ì‚¬ì´íŠ¸ í¬ë¡¤ë§"""
    print("[INFO] === ë¦¬í¬íŠ¸ìš© ì „ì²´ í¬ë¡¤ë§ ì‹œì‘ ===")
    
    all_posts = []
    all_posts.extend(crawl_frequent_sites(force_crawl=True))
    all_posts.extend(crawl_regular_sites(force_crawl=True))
    
    print(f"[INFO] === ë¦¬í¬íŠ¸ìš© í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_posts)}ê°œ ===")
    return all_posts

# =============================================================================
# ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ (ë³´ì¡°)
# =============================================================================

def crawl_ruliweb_epic7() -> List[Dict]:
    """ë£¨ë¦¬ì›¹ ì—í”½ì„¸ë¸ ê²Œì‹œíŒ í¬ë¡¤ë§"""
    posts = []
    
    driver = None
    try:
        driver = get_chrome_driver()
        
        url = "https://bbs.ruliweb.com/game/84834"
        driver.get(url)
        time.sleep(CrawlingSchedule.RULIWEB_WAIT_TIME)
        
        selectors = [
            ".subject_link",
            ".table_body .subject a", 
            "td.subject a",
            "a[href*='/read/']"
        ]
        
        articles = []
        for selector in selectors:
            try:
                articles = driver.find_elements(By.CSS_SELECTOR, selector)
                if articles:
                    break
            except:
                continue
        
        link_data = load_crawled_links()
        crawled_links = link_data["links"]
        
        for article in articles[:10]:
            try:
                title = article.text.strip()
                link = article.get_attribute("href")
                
                if not title or not link or len(title) < 5:
                    continue
                
                if any(keyword in title for keyword in ['ê³µì§€', 'ì´ë²¤íŠ¸', 'ì¶”ì²œ']):
                    continue
                
                if link.startswith('/'):
                    link = 'https://bbs.ruliweb.com' + link
                
                if link not in crawled_links:
                    post_data = {
                        "title": title,
                        "url": link,
                        "content": "ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ - ë§í¬ì—ì„œ í™•ì¸",
                        "timestamp": datetime.now().isoformat(),
                        "source": "ruliweb_epic7",
                        "region": "korea"
                    }
                    posts.append(post_data)
                    crawled_links.append(link)
                
            except Exception as e:
                print(f"[ERROR] ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        link_data["links"] = crawled_links
        save_crawled_links(link_data)
        
    except Exception as e:
        print(f"[ERROR] ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    finally:
        if driver:
            try:
                driver.quit()
            except:
                pass
    
    return posts

# =============================================================================
# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
# =============================================================================

def test_crawling():
    """í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("=== Epic7 í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ v3.6 - ë³¸ë¬¸ ì¶”ì¶œ ìµœì í™” ===")
    
    # í™˜ê²½ ì„¤ì • í™•ì¸
    print(f"ìŠ¤ì¼€ì¤„ ëŒ€ê¸°ì‹œê°„: FREQUENT={CrawlingSchedule.FREQUENT_WAIT_TIME}ì´ˆ, REGULAR={CrawlingSchedule.REGULAR_WAIT_TIME}ì´ˆ")
    
    # 15ë¶„ ì£¼ê¸° í…ŒìŠ¤íŠ¸
    print("\n[TEST] 15ë¶„ ì£¼ê¸° - ë²„ê·¸ ê²Œì‹œíŒ í…ŒìŠ¤íŠ¸")
    bug_posts = crawl_frequent_sites(force_crawl=True)
    
    # 30ë¶„ ì£¼ê¸° í…ŒìŠ¤íŠ¸
    print("\n[TEST] 30ë¶„ ì£¼ê¸° - ì¼ë°˜ ê²Œì‹œíŒ í…ŒìŠ¤íŠ¸") 
    regular_posts = crawl_regular_sites(force_crawl=True)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\n=== í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===")
    print(f"ë²„ê·¸ ê²Œì‹œíŒ: {len(bug_posts)}ê°œ")
    print(f"ì¼ë°˜ ê²Œì‹œíŒ: {len(regular_posts)}ê°œ") 
    print(f"ì´ í•©ê³„: {len(bug_posts + regular_posts)}ê°œ")
    
    # ìƒ˜í”Œ ì¶œë ¥
    all_posts = bug_posts + regular_posts
    print(f"\n=== ìƒ˜í”Œ ê²Œì‹œê¸€ (ìµœëŒ€ 5ê°œ) ===")
    for i, post in enumerate(all_posts[:5], 1):
        print(f"{i}. [{post['source']}] {post['title'][:50]}...")
        print(f"   ë‚´ìš©: {post['content'][:70]}...")
        print(f"   URL: {post['url']}")
        print()
    
    return all_posts

if __name__ == "__main__":
    test_crawling()