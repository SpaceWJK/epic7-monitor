#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Epic7 ë‹¤êµ­ê°€ í¬ë¡¤ëŸ¬ v4.4 - ì •í™•í•œ ë¬¸ì œì  í•´ê²° ì™„ì„±í˜•
Master ìš”êµ¬ì‚¬í•­: ê²Œì‹œê¸€ë³„ ì¦‰ì‹œ ì²˜ë¦¬ (í¬ë¡¤ë§â†’ê°ì„±ë¶„ì„â†’ì•Œë¦¼â†’ë§ˆí‚¹)

í•µì‹¬ ê°œì„ ì‚¬í•­ (v4.4):
- ì¬ì‹œë„ í ì˜ì†ì„± í™•ë³´ (JSON íŒŒì¼ ê¸°ë°˜) âœ¨FIXEDâœ¨
- ì—ëŸ¬ ìœ í˜•ë³„ ë¶„ë¥˜ ë° í†µê³„ ì‹œìŠ¤í…œ âœ¨FIXEDâœ¨ 
- ì•ˆì „í•œ URL í•´ì‹œ ì‹œìŠ¤í…œ (SHA256 ê¸°ë°˜) âœ¨FIXEDâœ¨
- ê°•í™”ëœ Epic7 ëª¨ë“ˆ fallback âœ¨ENHANCEDâœ¨
- ë””ë²„ê·¸ íŒŒì¼ ê´€ë¦¬ ë° ë¦¬ì†ŒìŠ¤ ìµœì í™” âœ¨NEWâœ¨

ê¸°ì¡´ ê¸°ëŠ¥ 100% ë³´ì¡´:
- ê²Œì‹œê¸€ë³„ ì¦‰ì‹œ ì²˜ë¦¬ ì™„ì „ êµ¬í˜„
- ì—ëŸ¬ ê²©ë¦¬ ë° ë³µì›ë ¥ ê°•í™”
- ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ ìë™ ê´€ë¦¬
- ëª¨ë“  API ì¸í„°í˜ì´ìŠ¤ í˜¸í™˜ì„± ìœ ì§€

Author: Epic7 Monitoring Team  
Version: 4.4 (ì •í™•í•œ ë¬¸ì œì  í•´ê²° ì™„ì„±í˜•)
Date: 2025-07-28
Fixed: ì¬ì‹œë„ í, ì—ëŸ¬ ë¶„ë¥˜, í•´ì‹œ ì¶©ëŒ, fallback ê°•í™”
"""

import time
import random
import re
import requests
import concurrent.futures
import os
import sys
import json
import hashlib  # âœ¨ NEW: SHA256 í•´ì‹œìš©
import traceback  # âœ¨ NEW: ì—ëŸ¬ ìƒì„¸ ë¶„ì„ìš©
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Callable, Union
from urllib.parse import urljoin, urlparse
from enum import Enum  # âœ¨ NEW: ì—ëŸ¬ ìœ í˜• ì •ì˜ìš©

# Selenium ê´€ë ¨ import
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.keys import Keys


# =============================================================================
# âœ¨ NEW v4.4: ì—ëŸ¬ ìœ í˜• ì •ì˜ ë° í†µê³„ ì‹œìŠ¤í…œ
# =============================================================================

class ErrorType(Enum):
    """ì—ëŸ¬ ìœ í˜• ë¶„ë¥˜"""
    IMPORT = "import_error"
    NETWORK = "network_error"
    PARSE = "parse_error"
    CLASSIFICATION = "classification_error"
    NOTIFICATION = "notification_error"
    FILE_IO = "file_io_error"
    DRIVER = "driver_error"
    GENERAL = "general_error"

class ErrorManager:
    """ì—ëŸ¬ ê´€ë¦¬ ë° í†µê³„ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.error_stats = {error_type.value: 0 for error_type in ErrorType}
        self.error_log = []
        self.stats_file = "error_stats.json"
        self.load_error_stats()
    
    def record_error(self, error_type: ErrorType, error: Exception, context: Dict = None):
        """ì—ëŸ¬ ê¸°ë¡ ë° í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            self.error_stats[error_type.value] += 1
            
            error_entry = {
                "type": error_type.value,
                "message": str(error),
                "traceback": traceback.format_exc(),
                "context": context or {},
                "timestamp": datetime.now().isoformat()
            }
            
            self.error_log.append(error_entry)
            
            # ë¡œê·¸ í¬ê¸° ì œí•œ (ìµœëŒ€ 1000ê°œ)
            if len(self.error_log) > 1000:
                self.error_log = self.error_log[-500:]
            
            self.save_error_stats()
            
        except Exception as e:
            print(f"[ERROR] ì—ëŸ¬ ê¸°ë¡ ì‹¤íŒ¨: {e}")
    
    def load_error_stats(self):
        """ì—ëŸ¬ í†µê³„ ë¡œë“œ"""
        try:
            if os.path.exists(self.stats_file):
                with open(self.stats_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.error_stats.update(data.get('stats', {}))
                    self.error_log = data.get('log', [])
        except Exception as e:
            print(f"[WARNING] ì—ëŸ¬ í†µê³„ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def save_error_stats(self):
        """ì—ëŸ¬ í†µê³„ ì €ì¥"""
        try:
            data = {
                'stats': self.error_stats,
                'log': self.error_log[-100:],  # ìµœê·¼ 100ê°œë§Œ ì €ì¥
                'last_updated': datetime.now().isoformat()
            }
            with open(self.stats_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"[ERROR] ì—ëŸ¬ í†µê³„ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def get_error_summary(self) -> Dict:
        """ì—ëŸ¬ í†µê³„ ìš”ì•½ ë°˜í™˜"""
        total_errors = sum(self.error_stats.values())
        return {
            'total_errors': total_errors,
            'by_type': self.error_stats,
            'recent_errors': len(self.error_log),
            'most_common': max(self.error_stats, key=self.error_stats.get) if total_errors > 0 else None
        }

# ì „ì—­ ì—ëŸ¬ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
error_manager = ErrorManager()


# Epic7 ì‹œìŠ¤í…œ ëª¨ë“ˆ import (ì¦‰ì‹œ ì²˜ë¦¬ìš©)
try:
    from classifier import Epic7Classifier, is_bug_post, is_high_priority_bug, should_send_realtime_alert
    from notifier import send_bug_alert, send_sentiment_notification
    from sentiment_data_manager import save_sentiment_data, get_sentiment_summary
    EPIC7_MODULES_AVAILABLE = True
    print("[INFO] Epic7 ì²˜ë¦¬ ëª¨ë“ˆë“¤ ë¡œë“œ ì™„ë£Œ")
except ImportError as e:
    

    # âœ¨ ENHANCED v4.4: í–¥ìƒëœ ì„í¬íŠ¸ ì—ëŸ¬ ì§„ë‹¨
    import_error_details = {
        'error_message': str(e),
        'missing_module': getattr(e, 'name', 'unknown'),
        'python_version': sys.version,
        'python_path': sys.path,
        'current_directory': os.getcwd()
    }
    
    error_manager.record_error(ErrorType.IMPORT, e, import_error_details)
    
    print(f"[WARNING] Epic7 ì²˜ë¦¬ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print(f"[WARNING] ëˆ„ë½ ëª¨ë“ˆ: {import_error_details['missing_module']}")
    print("[WARNING] ì¦‰ì‹œ ì²˜ë¦¬ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
    print("[INFO] ìƒì„¸ ì§„ë‹¨ ì •ë³´ê°€ error_stats.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    

    EPIC7_MODULES_AVAILABLE = False

# Reddit í¬ë¡¤ë§ìš© import
try:
    import praw
    REDDIT_AVAILABLE = True
except ImportError as e:
    error_manager.record_error(ErrorType.IMPORT, e, {'module': 'praw'})
    print("[WARNING] PRAW ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Reddit í¬ë¡¤ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    REDDIT_AVAILABLE = False


# =============================================================================
# âœ¨ FIXED v4.4: ì¬ì‹œë„ í ì˜ì†ì„± í™•ë³´ (JSON íŒŒì¼ ê¸°ë°˜)
# =============================================================================

class PersistentRetryQueue:
    """ì˜ì†ì„±ì„ ê°€ì§„ ì¬ì‹œë„ í ê´€ë¦¬ì"""
    
    def __init__(self, queue_file: str = "retry_queue.json"):
        self.queue_file = queue_file
        self.queue = self.load_queue()
        self.max_queue_size = 500  # í í¬ê¸° ì œí•œ
    
    def load_queue(self) -> List[Dict]:
        """ì¬ì‹œë„ í ë¡œë“œ"""
        try:
            if os.path.exists(self.queue_file):
                with open(self.queue_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    queue = data.get('queue', [])
                    print(f"[INFO] ì¬ì‹œë„ í ë¡œë“œ ì™„ë£Œ: {len(queue)}ê°œ í•­ëª©")
                    return queue
        except Exception as e:
            error_manager.record_error(ErrorType.FILE_IO, e, {'file': self.queue_file})
            print(f"[WARNING] ì¬ì‹œë„ í ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return []
    
    def save_queue(self):
        """ì¬ì‹œë„ í ì €ì¥"""
        try:
            # í í¬ê¸° ì œí•œ
            if len(self.queue) > self.max_queue_size:
                # ì˜¤ë˜ëœ í•­ëª©ë¶€í„° ì œê±°
                self.queue = sorted(
                    self.queue, 
                    key=lambda x: x.get('timestamp', ''), 
                    reverse=True
                )[:self.max_queue_size]
                print(f"[INFO] ì¬ì‹œë„ í í¬ê¸° ì œí•œ ì ìš©: {self.max_queue_size}ê°œë¡œ ì¶•ì†Œ")
            
            data = {
                'queue': self.queue,
                'last_updated': datetime.now().isoformat(),
                'queue_size': len(self.queue)
            }
            
            with open(self.queue_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            error_manager.record_error(ErrorType.FILE_IO, e, {'file': self.queue_file})
            print(f"[ERROR] ì¬ì‹œë„ í ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def add(self, post_data: Dict, sentiment_result: Optional[Dict] = None, error_type: str = "general"):
        """ì¬ì‹œë„ íì— í•­ëª© ì¶”ê°€"""
        retry_item = {
            "post_data": post_data,
            "sentiment_result": sentiment_result,
            "error_type": error_type,
            "timestamp": datetime.now().isoformat(),
            "retry_count": 0,
            "priority": self._calculate_priority(error_type)
        }
        
        self.queue.append(retry_item)
        self._sort_by_priority()
        self.save_queue()
        
        print(f"[RETRY] ì¬ì‹œë„ í ì¶”ê°€: {len(self.queue)}ê°œ ëŒ€ê¸°ì¤‘ (ìœ í˜•: {error_type})")
    
    def _calculate_priority(self, error_type: str) -> int:
        """ì—ëŸ¬ ìœ í˜•ì— ë”°ë¥¸ ìš°ì„ ìˆœìœ„ ê³„ì‚°"""
        priority_map = {
            "import_error": 1,      # ë‚®ì€ ìš°ì„ ìˆœìœ„ (ì‹œìŠ¤í…œ ë¬¸ì œ)
            "network_error": 5,    # ë†’ì€ ìš°ì„ ìˆœìœ„ (ì¼ì‹œì )
            "parse_error": 3,      # ì¤‘ê°„ ìš°ì„ ìˆœìœ„
            "classification_error": 4, # ì¤‘ê°„-ë†’ìŒ ìš°ì„ ìˆœìœ„
            "general": 2           # ê¸°ë³¸ ìš°ì„ ìˆœìœ„
        }
        return priority_map.get(error_type, 2)
    
    def _sort_by_priority(self):
        """ìš°ì„ ìˆœìœ„ë³„ ì •ë ¬"""
        self.queue.sort(key=lambda x: x.get('priority', 2), reverse=True)
    
    def remove(self, item: Dict):
        """íì—ì„œ í•­ëª© ì œê±°"""
        try:
            self.queue.remove(item)
            self.save_queue()
        except ValueError:
            pass  # ì´ë¯¸ ì œê±°ëœ í•­ëª©
    
    def get_stats(self) -> Dict:
        """í í†µê³„ ë°˜í™˜"""
        error_types = {}
        for item in self.queue:
            error_type = item.get('error_type', 'unknown')
            error_types[error_type] = error_types.get(error_type, 0) + 1
        
        return {
            'total_items': len(self.queue),
            'by_error_type': error_types,
            'oldest_item': min(self.queue, key=lambda x: x.get('timestamp', '')).get('timestamp') if self.queue else None
        }


# =============================================================================
# ğŸš€ Master ìš”êµ¬ì‚¬í•­: ì¦‰ì‹œ ì²˜ë¦¬ ì‹œìŠ¤í…œ êµ¬í˜„ (v4.4 ê°•í™”)
# =============================================================================

class ImmediateProcessor:
    """ê²Œì‹œê¸€ë³„ ì¦‰ì‹œ ì²˜ë¦¬ ì‹œìŠ¤í…œ v4.4"""
    
    def __init__(self):
        self.processed_count = 0
        self.failed_count = 0
        

        # âœ¨ FIXED v4.4: ì˜ì†ì„±ì„ ê°€ì§„ ì¬ì‹œë„ í
        self.retry_queue = PersistentRetryQueue()
        

        self.classifier = None
        self.error_manager = error_manager
        
        if EPIC7_MODULES_AVAILABLE:
            try:
                self.classifier = Epic7Classifier()
                print("[INFO] ì¦‰ì‹œ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                self.error_manager.record_error(ErrorType.CLASSIFICATION, e)
                print(f"[ERROR] ë¶„ë¥˜ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                
    def process_post_immediately(self, post_data: Dict) -> bool:
        """
        ê²Œì‹œê¸€ë³„ ì¦‰ì‹œ ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜ v4.4
        Master ìš”êµ¬ì‚¬í•­: í¬ë¡¤ë§ â†’ ê°ì„±ë¶„ì„ â†’ ì•Œë¦¼ â†’ ë§ˆí‚¹
        """
        try:
            print(f"[IMMEDIATE] ì¦‰ì‹œ ì²˜ë¦¬ ì‹œì‘: {post_data.get('title', '')[:50]}...")
            
            if not EPIC7_MODULES_AVAILABLE:
                print("[WARNING] ì²˜ë¦¬ ëª¨ë“ˆ ì—†ìŒ, ê°•í™”ëœ ê¸°ë³¸ ì²˜ë¦¬ ìˆ˜í–‰")
                

                # âœ¨ ENHANCED v4.4: ê°•í™”ëœ fallback ì²˜ë¦¬
                return self._enhanced_basic_processing(post_data)
                

            
            # 1. ìœ ì € ë™í–¥ ê°ì„± ë¶„ì„
            sentiment_result = self._analyze_sentiment(post_data)
            
            # 2. ì•Œë¦¼ ì „ì†¡ ì—¬ë¶€ ì²´í¬ ë° ì „ì†¡
            notification_sent = self._handle_notifications(post_data, sentiment_result)
            
            # 3. ì²˜ë¦¬ ì™„ë£Œ ë§ˆí‚¹ (ì•Œë¦¼ ì„±ê³µ ì‹œì—ë§Œ)
            if notification_sent:
                self._mark_as_processed(post_data['url'], notified=True)
                self.processed_count += 1
                print(f"[SUCCESS] ì¦‰ì‹œ ì²˜ë¦¬ ì™„ë£Œ: {post_data.get('title', '')[:30]}...")
            else:
                # ì‹¤íŒ¨í•œ ê²½ìš° ì¬ì‹œë„ íì— ì¶”ê°€
                self.retry_queue.add(post_data, sentiment_result, "notification_error")
                self.failed_count += 1
                
            return notification_sent
            
        

        # âœ¨ ENHANCED v4.4: ì—ëŸ¬ ìœ í˜•ë³„ ë¶„ë¥˜ ì²˜ë¦¬
        except ImportError as e:
            self._handle_import_error(e, post_data)
            return False
        except (requests.RequestException, requests.Timeout, requests.ConnectionError) as e:
            self._handle_network_error(e, post_data)
            return False
        except (ValueError, KeyError, AttributeError) as e:
            self._handle_parse_error(e, post_data)
            return False
        

        except Exception as e:
            self._handle_general_error(e, post_data)
            return False
    
    

    # âœ¨ NEW v4.4: ì—ëŸ¬ ìœ í˜•ë³„ ì²˜ë¦¬ ë©”ì„œë“œë“¤
    def _handle_import_error(self, error: ImportError, post_data: Dict):
        """ì„í¬íŠ¸ ì—ëŸ¬ ì²˜ë¦¬"""
        context = {'post_title': post_data.get('title', '')[:50]}
        self.error_manager.record_error(ErrorType.IMPORT, error, context)
        self.retry_queue.add(post_data, None, "import_error")
        self.failed_count += 1
        print(f"[ERROR] ì„í¬íŠ¸ ì—ëŸ¬ - ë‚®ì€ ìš°ì„ ìˆœìœ„ë¡œ ì¬ì‹œë„ í ì¶”ê°€: {error}")
    
    def _handle_network_error(self, error: Exception, post_data: Dict):
        """ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬ ì²˜ë¦¬"""
        context = {
            'post_url': post_data.get('url', ''),
            'post_title': post_data.get('title', '')[:50]
        }
        self.error_manager.record_error(ErrorType.NETWORK, error, context)
        self.retry_queue.add(post_data, None, "network_error")
        self.failed_count += 1
        print(f"[ERROR] ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬ - ë†’ì€ ìš°ì„ ìˆœìœ„ë¡œ ì¬ì‹œë„: {error}")
    
    def _handle_parse_error(self, error: Exception, post_data: Dict):
        """íŒŒì‹± ì—ëŸ¬ ì²˜ë¦¬"""
        context = {'post_source': post_data.get('source', '')}
        self.error_manager.record_error(ErrorType.PARSE, error, context)
        self.retry_queue.add(post_data, None, "parse_error")
        self.failed_count += 1
        print(f"[ERROR] íŒŒì‹± ì—ëŸ¬: {error}")
    
    def _handle_general_error(self, error: Exception, post_data: Dict):
        """ì¼ë°˜ ì—ëŸ¬ ì²˜ë¦¬"""
        context = {'post_data_keys': list(post_data.keys())}
        self.error_manager.record_error(ErrorType.GENERAL, error, context)
        self.retry_queue.add(post_data, None, "general")
        self.failed_count += 1
        print(f"[ERROR] ì¼ë°˜ ì—ëŸ¬: {error}")
    

    
    def _analyze_sentiment(self, post_data: Dict) -> Dict:
        """ê°ì„± ë¶„ì„ ìˆ˜í–‰"""
        try:
            if not self.classifier:
                return {"sentiment": "neutral", "confidence": 0.5}
                
            result = self.classifier.classify_post(post_data)
            print(f"[SENTIMENT] ë¶„ì„ ê²°ê³¼: {result.get('sentiment', 'unknown')}")
            return result
            
        except Exception as e:
            self.error_manager.record_error(ErrorType.CLASSIFICATION, e)
            print(f"[ERROR] ê°ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"sentiment": "neutral", "confidence": 0.0, "error": str(e)}
    
    

    # âœ¨ ENHANCED v4.4: ê°•í™”ëœ ê¸°ë³¸ ì²˜ë¦¬
    def _enhanced_basic_processing(self, post_data: Dict) -> bool:
        """ê°•í™”ëœ ê¸°ë³¸ ì²˜ë¦¬ (Epic7 ëª¨ë“ˆ ì—†ì„ ë•Œ)"""
        try:
            print(f"[ENHANCED_BASIC] ê°•í™”ëœ ê¸°ë³¸ ì²˜ë¦¬: {post_data.get('title', '')[:50]}...")
            
            # 1. ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ ë° ì €ì¥
            basic_info = {
                'title': post_data.get('title', ''),
                'url': post_data.get('url', ''),
                'source': post_data.get('source', ''),
                'processed_at': datetime.now().isoformat(),
                'processing_method': 'enhanced_basic',
                'epic7_modules_available': False
            }
            
            # 2. ê¸°ë³¸ ë°ì´í„° ì €ì¥
            self._save_basic_data(basic_info)
            
            # 3. ì²˜ë¦¬ ì™„ë£Œ ë§ˆí‚¹
            self._mark_as_processed(post_data['url'], notified=False)
            self.processed_count += 1
            
            print("[SUCCESS] ê°•í™”ëœ ê¸°ë³¸ ì²˜ë¦¬ ì™„ë£Œ - ê¸°ë³¸ ë°ì´í„° ì €ì¥ë¨")
            return True
            
        except Exception as e:
            self.error_manager.record_error(ErrorType.GENERAL, e)
            print(f"[ERROR] ê°•í™”ëœ ê¸°ë³¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return False
    
    def _save_basic_data(self, data: Dict):
        """ê¸°ë³¸ ë°ì´í„° ì €ì¥"""
        try:
            basic_data_file = "basic_processed_data.json"
            
            # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
            existing_data = []
            if os.path.exists(basic_data_file):
                with open(basic_data_file, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
            
            # ìƒˆ ë°ì´í„° ì¶”ê°€
            existing_data.append(data)
            
            # ìµœëŒ€ 1000ê°œë¡œ ì œí•œ
            if len(existing_data) > 1000:
                existing_data = existing_data[-500:]
            
            # ì €ì¥
            with open(basic_data_file, 'w', encoding='utf-8') as f:
                json.dump(existing_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            self.error_manager.record_error(ErrorType.FILE_IO, e)
            print(f"[ERROR] ê¸°ë³¸ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
    

    
    def _handle_notifications(self, post_data: Dict, sentiment_result: Dict) -> bool:
        """ë¶„ë¥˜ë³„ ì•Œë¦¼ ì²˜ë¦¬"""
        try:
            source = post_data.get('source', '')
            sentiment = sentiment_result.get('sentiment', 'neutral')
            
            # Master ìš”êµ¬ì‚¬í•­: ë²„ê·¸ ê²Œì‹œíŒ ê¸€ì´ë¼ë©´ ì‹¤ì‹œê°„ ë²„ê·¸ ë©”ì‹œì§€
            if source.endswith('_bug') or 'bug' in source.lower():
                print("[ALERT] ë²„ê·¸ ê²Œì‹œíŒ ê¸€ â†’ ì¦‰ì‹œ ë²„ê·¸ ì•Œë¦¼")
                return self._send_bug_alert(post_data)
            
            # Master ìš”êµ¬ì‚¬í•­: ë™í–¥ ë¶„ì„ í›„ ë²„ê·¸ë¡œ ë¶„ë¥˜ëœ ê¸€ë„ ì‹¤ì‹œê°„ ë²„ê·¸ ë©”ì‹œì§€
            elif is_bug_post(sentiment_result) or should_send_realtime_alert(sentiment_result):
                print("[ALERT] ë²„ê·¸ ë¶„ë¥˜ ê¸€ â†’ ì¦‰ì‹œ ë²„ê·¸ ì•Œë¦¼")
                return self._send_bug_alert(post_data)
            
            # Master ìš”êµ¬ì‚¬í•­: ê¸ì •/ì¤‘ë¦½/ë¶€ì • ë™í–¥ì€ ê°ì„± ì•Œë¦¼ + ì €ì¥
            else:
                print(f"[ALERT] ê°ì„± ë™í–¥ ê¸€ ({sentiment}) â†’ ì¦‰ì‹œ ê°ì„± ì•Œë¦¼")
                return self._send_sentiment_alert(post_data, sentiment_result)
                
        except Exception as e:
            self.error_manager.record_error(ErrorType.NOTIFICATION, e)
            print(f"[ERROR] ì•Œë¦¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return False
    
    def _send_bug_alert(self, post_data: Dict) -> bool:
        """ë²„ê·¸ ì•Œë¦¼ ì „ì†¡"""
        try:
            success = send_bug_alert(post_data)
            if success:
                print("[SUCCESS] ë²„ê·¸ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ")
            else:
                print("[FAILED] ë²„ê·¸ ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨")
            return success
        except Exception as e:
            self.error_manager.record_error(ErrorType.NOTIFICATION, e)
            print(f"[ERROR] ë²„ê·¸ ì•Œë¦¼ ì „ì†¡ ì˜¤ë¥˜: {e}")
            return False
    
    def _send_sentiment_alert(self, post_data: Dict, sentiment_result: Dict) -> bool:
        """ê°ì„± ì•Œë¦¼ ì „ì†¡ ë° ë°ì´í„° ì €ì¥"""
        try:
            # Master ìš”êµ¬ì‚¬í•­: ì¼ê°„ ë¦¬í¬íŠ¸ìš© ë°ì´í„° ì €ì¥
            save_success = save_sentiment_data(post_data, sentiment_result)
            
            # ì¦‰ì‹œ ê°ì„± ì•Œë¦¼ ì „ì†¡
            alert_success = send_sentiment_notification(post_data, sentiment_result)
            
            if save_success and alert_success:
                print("[SUCCESS] ê°ì„± ì•Œë¦¼ ì „ì†¡ ë° ë°ì´í„° ì €ì¥ ì™„ë£Œ")
                return True
            else:
                print(f"[PARTIAL] ì €ì¥: {save_success}, ì•Œë¦¼: {alert_success}")
                return False
                
        except Exception as e:
            self.error_manager.record_error(ErrorType.NOTIFICATION, e)
            print(f"[ERROR] ê°ì„± ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return False
    
    def _mark_as_processed(self, url: str, notified: bool = True):
        """ì²˜ë¦¬ ì™„ë£Œ ë§ˆí‚¹"""
        try:
            mark_as_processed(url, notified)
        except Exception as e:
            self.error_manager.record_error(ErrorType.FILE_IO, e)
            print(f"[ERROR] ë§ˆí‚¹ ì‹¤íŒ¨: {e}")
    
    def process_retry_queue(self):
        """ì¬ì‹œë„ í ì²˜ë¦¬ v4.4"""
        if not self.retry_queue.queue:
            return
            
        print(f"[RETRY] ì¬ì‹œë„ í ì²˜ë¦¬ ì‹œì‘: {len(self.retry_queue.queue)}ê°œ")
        processed_items = []
        
        for item in self.retry_queue.queue:
            try:
                if item["retry_count"] >= 3:
                    print(f"[SKIP] ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {item.get('error_type', 'unknown')}")
                    processed_items.append(item)
                    continue
                
                item["retry_count"] += 1
                success = self.process_post_immediately(item["post_data"])
                
                if success:
                    processed_items.append(item)
                    
            except Exception as e:
                self.error_manager.record_error(ErrorType.GENERAL, e)
                print(f"[ERROR] ì¬ì‹œë„ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        # ì²˜ë¦¬ ì™„ë£Œëœ í•­ëª©ë“¤ ì œê±°
        for item in processed_items:
            self.retry_queue.remove(item)
        
        print(f"[RETRY] ì¬ì‹œë„ ì™„ë£Œ: {len(processed_items)}ê°œ ì²˜ë¦¬, {len(self.retry_queue.queue)}ê°œ ë‚¨ìŒ")
    
    def get_stats(self) -> Dict:
        """ì²˜ë¦¬ í†µê³„ ë°˜í™˜ v4.4"""
        return {
            "processed": self.processed_count,
            "failed": self.failed_count,
            "retry_queue": self.retry_queue.get_stats(),
            "error_summary": self.error_manager.get_error_summary(),
            "epic7_modules_available": EPIC7_MODULES_AVAILABLE
        }

# ì „ì—­ ì¦‰ì‹œ ì²˜ë¦¬ê¸° ì¸ìŠ¤í„´ìŠ¤
immediate_processor = ImmediateProcessor()


# =============================================================================
# âœ¨ NEW v4.4: ë””ë²„ê·¸ íŒŒì¼ ê´€ë¦¬ ì‹œìŠ¤í…œ
# =============================================================================

class DebugFileManager:
    """ë””ë²„ê·¸ íŒŒì¼ ê´€ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, max_debug_files: int = 10):
        self.max_debug_files = max_debug_files
        self.debug_dir = "debug_files"
        self._ensure_debug_dir()
    
    def _ensure_debug_dir(self):
        """ë””ë²„ê·¸ ë””ë ‰í† ë¦¬ ìƒì„±"""
        try:
            if not os.path.exists(self.debug_dir):
                os.makedirs(self.debug_dir)
        except Exception as e:
            print(f"[WARNING] ë””ë²„ê·¸ ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def save_debug_html(self, filename: str, content: str) -> str:
        """ë””ë²„ê·¸ HTML íŒŒì¼ ì €ì¥ (ê´€ë¦¬ë¨)"""
        try:
            # ê¸°ì¡´ ë””ë²„ê·¸ íŒŒì¼ë“¤ ì •ë¦¬
            self._cleanup_old_files()
            
            filepath = os.path.join(self.debug_dir, filename)
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)
            
            print(f"[DEBUG] ë””ë²„ê·¸ íŒŒì¼ ì €ì¥: {filepath}")
            return filepath
            
        except Exception as e:
            error_manager.record_error(ErrorType.FILE_IO, e)
            print(f"[ERROR] ë””ë²„ê·¸ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""
    
    def _cleanup_old_files(self):
        """ì˜¤ë˜ëœ ë””ë²„ê·¸ íŒŒì¼ ì •ë¦¬"""
        try:
            if not os.path.exists(self.debug_dir):
                return
            
            files = []
            for filename in os.listdir(self.debug_dir):
                filepath = os.path.join(self.debug_dir, filename)
                if os.path.isfile(filepath):
                    mtime = os.path.getmtime(filepath)
                    files.append((filepath, mtime))
            
            # íŒŒì¼ ìˆ˜ê°€ ì œí•œì„ ì´ˆê³¼í•˜ë©´ ì˜¤ë˜ëœ íŒŒì¼ë¶€í„° ì‚­ì œ
            if len(files) >= self.max_debug_files:
                files.sort(key=lambda x: x[1])  # ìˆ˜ì •ì‹œê°„ ê¸°ì¤€ ì •ë ¬
                files_to_delete = files[:len(files) - self.max_debug_files + 1]
                
                for filepath, _ in files_to_delete:
                    os.remove(filepath)
                    print(f"[CLEANUP] ì˜¤ë˜ëœ ë””ë²„ê·¸ íŒŒì¼ ì‚­ì œ: {filepath}")
                    
        except Exception as e:
            print(f"[WARNING] ë””ë²„ê·¸ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ì „ì—­ ë””ë²„ê·¸ íŒŒì¼ ë§¤ë‹ˆì €
debug_file_manager = DebugFileManager()


# =============================================================================
# í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ ì„¤ì • í´ë˜ìŠ¤ (ê¸°ì¡´ ìœ ì§€)
# =============================================================================

class CrawlingSchedule:
    """í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ë³„ ì„¤ì • ê´€ë¦¬"""

    FREQUENT_WAIT_TIME = 25      # 15ë¶„ ì£¼ê¸° ëŒ€ê¸°ì‹œê°„
    REGULAR_WAIT_TIME = 30       # 30ë¶„ ì£¼ê¸° ëŒ€ê¸°ì‹œê°„  
    REDDIT_WAIT_TIME = 15        # Reddit ëŒ€ê¸°ì‹œê°„
    RULIWEB_WAIT_TIME = 20       # ë£¨ë¦¬ì›¹ ëŒ€ê¸°ì‹œê°„

    # ìŠ¤í¬ë¡¤ íšŸìˆ˜ ì„¤ì •
    FREQUENT_SCROLL_COUNT = 2    # 15ë¶„ ì£¼ê¸° ìŠ¤í¬ë¡¤
    REGULAR_SCROLL_COUNT = 3

    @staticmethod
    def get_wait_time(schedule_type: str) -> int:
        """ìŠ¤ì¼€ì¤„ íƒ€ì…ë³„ ëŒ€ê¸°ì‹œê°„ ë°˜í™˜"""
        if schedule_type == 'frequent':
            return CrawlingSchedule.FREQUENT_WAIT_TIME
        elif schedule_type == 'regular':
            return CrawlingSchedule.REGULAR_WAIT_TIME
        elif schedule_type == 'reddit':
            return CrawlingSchedule.REDDIT_WAIT_TIME
        elif schedule_type == 'ruliweb':
            return CrawlingSchedule.RULIWEB_WAIT_TIME
        else:
            return CrawlingSchedule.REGULAR_WAIT_TIME

    @staticmethod
    def get_scroll_count(schedule_type: str) -> int:
        """ìŠ¤ì¼€ì¤„ íƒ€ì…ë³„ ìŠ¤í¬ë¡¤ íšŸìˆ˜ ë°˜í™˜"""
        if schedule_type == 'frequent':
            return CrawlingSchedule.FREQUENT_SCROLL_COUNT
        else:
            return CrawlingSchedule.REGULAR_SCROLL_COUNT

# =============================================================================
# íŒŒì¼ ê´€ë¦¬ ì‹œìŠ¤í…œ - ì‹œê°„ ê¸°ë°˜ ì¤‘ë³µ ê´€ë¦¬ (ê¸°ì¡´ ìœ ì§€)
# =============================================================================

def get_crawled_links_file():
    """ì›Œí¬í”Œë¡œìš°ë³„ ë…ë¦½ì ì¸ í¬ë¡¤ë§ ë§í¬ íŒŒì¼ëª… ìƒì„±"""
    workflow_name = os.environ.get('GITHUB_WORKFLOW', 'default')

    if 'debug' in workflow_name.lower() or 'test' in workflow_name.lower():
        return "crawled_links_debug.json"
    elif 'monitor' in workflow_name.lower():
        return "crawled_links_monitor.json"
    else:
        return "crawled_links.json"

def get_content_cache_file():
    """ì›Œí¬í”Œë¡œìš°ë³„ ë…ë¦½ì ì¸ ì½˜í…ì¸  ìºì‹œ íŒŒì¼ëª… ìƒì„±"""
    workflow_name = os.environ.get('GITHUB_WORKFLOW', 'default')

    if 'debug' in workflow_name.lower():
        return "content_cache_debug.json"
    else:
        return "content_cache.json"

def load_crawled_links():
    """í¬ë¡¤ë§ ë§í¬ ë¡œë“œ - ì‹œê°„ ê¸°ë°˜ êµ¬ì¡° ì ìš©"""
    crawled_links_file = get_crawled_links_file()
    
    if os.path.exists(crawled_links_file):
        try:
            with open(crawled_links_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
                # ê¸°ì¡´ ë‹¨ìˆœ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¥¼ ìƒˆ êµ¬ì¡°ë¡œ ë³€í™˜
                if isinstance(data, dict) and "links" in data:
                    if isinstance(data["links"], list) and len(data["links"]) > 0:
                        if isinstance(data["links"][0], str):
                            converted_links = []
                            for link in data["links"]:
                                converted_links.append({
                                    "url": link,
                                    "processed_at": (datetime.now() - timedelta(hours=25)).isoformat(),
                                    "notified": False
                                })
                            data["links"] = converted_links
                            print(f"[INFO] ê¸°ì¡´ {len(converted_links)}ê°œ ë§í¬ë¥¼ ìƒˆ êµ¬ì¡°ë¡œ ë³€í™˜")
                
                # 24ì‹œê°„ ì§€ë‚œ í•­ëª© ìë™ ì œê±°
                now = datetime.now()
                valid_links = []
                for item in data.get("links", []):
                    try:
                        processed_time = datetime.fromisoformat(item["processed_at"])
                        if now - processed_time < timedelta(hours=24):
                            valid_links.append(item)
                    except:
                        continue
                
                data["links"] = valid_links
                print(f"[INFO] 24ì‹œê°„ ê¸°ì¤€ ìœ íš¨í•œ ë§í¬: {len(valid_links)}ê°œ")
                return data
                        
        except Exception as e:
            print(f"[WARNING] í¬ë¡¤ë§ ë§í¬ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
    
    return {"links": [], "last_updated": datetime.now().isoformat()}

def save_crawled_links(link_data):
    """í¬ë¡¤ë§ ë§í¬ ì €ì¥ - ì ê·¹ì  í¬ê¸° ê´€ë¦¬"""
    try:
        if len(link_data["links"]) > 100:
            link_data["links"] = sorted(
                link_data["links"], 
                key=lambda x: x.get("processed_at", ""), 
                reverse=True
            )[:100]
            print(f"[INFO] ë§í¬ ëª©ë¡ì„ ìµœì‹  100ê°œë¡œ ì •ë¦¬")

        link_data["last_updated"] = datetime.now().isoformat()

        crawled_links_file = get_crawled_links_file()
        with open(crawled_links_file, 'w', encoding='utf-8') as f:
            json.dump(link_data, f, ensure_ascii=False, indent=2)
            
        print(f"[INFO] í¬ë¡¤ë§ ë§í¬ ì €ì¥ ì™„ë£Œ: {len(link_data['links'])}ê°œ")

    except Exception as e:
        print(f"[ERROR] ë§í¬ ì €ì¥ ì‹¤íŒ¨: {e}")

def is_recently_processed(url: str, links_data: List[Dict], hours: int = 24) -> bool:
    """ì‹œê°„ ê¸°ë°˜ ì¤‘ë³µ ì²´í¬"""
    try:
        now = datetime.now()
        for item in links_data:
            if item.get("url") == url:
                processed_time = datetime.fromisoformat(item["processed_at"])
                if now - processed_time < timedelta(hours=hours):
                    return True
        return False
    except Exception as e:
        print(f"[DEBUG] ì¤‘ë³µ ì²´í¬ ì˜¤ë¥˜: {e}")
        return False

def mark_as_processed(url: str, notified: bool = False):
    """ê²Œì‹œê¸€ì„ ì²˜ë¦¬ë¨ìœ¼ë¡œ ë§ˆí‚¹"""
    try:
        link_data = load_crawled_links()
        
        found = False
        for item in link_data["links"]:
            if item.get("url") == url:
                item["processed_at"] = datetime.now().isoformat()
                item["notified"] = notified
                found = True
                break
        
        if not found:
            link_data["links"].append({
                "url": url,
                "processed_at": datetime.now().isoformat(),
                "notified": notified
            })
        
        save_crawled_links(link_data)
        print(f"[INFO] ë§í¬ ì²˜ë¦¬ ì™„ë£Œ ë§ˆí‚¹: {url[:50]}... (ì•Œë¦¼: {notified})")
        
    except Exception as e:
        print(f"[ERROR] ë§í¬ ë§ˆí‚¹ ì‹¤íŒ¨: {e}")

def load_content_cache():
    """ê²Œì‹œê¸€ ë‚´ìš© ìºì‹œ ë¡œë“œ"""
    content_cache_file = get_content_cache_file()

    if os.path.exists(content_cache_file):
        try:
            with open(content_cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            print(f"[WARNING] {content_cache_file} íŒŒì¼ ì½ê¸° ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±")
    return {}

def save_content_cache(cache_data):
    """ê²Œì‹œê¸€ ë‚´ìš© ìºì‹œ ì €ì¥"""
    try:
        if len(cache_data) > 500:
            sorted_items = sorted(cache_data.items(), 
                                key=lambda x: x[1].get('timestamp', ''), reverse=True)
            cache_data = dict(sorted_items[:500])

        content_cache_file = get_content_cache_file()
        with open(content_cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)

    except Exception as e:
        print(f"[ERROR] ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

# =============================================================================
# Chrome Driver ê´€ë¦¬ - ë¦¬ì†ŒìŠ¤ ìµœì í™” ê°•í™” (ê¸°ì¡´ ìœ ì§€)
# =============================================================================

def get_chrome_driver():
    """Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” - ë¦¬ì†ŒìŠ¤ ìµœì í™” ë° ì•ˆì •ì„± ê°•í™”"""
    options = Options()

    # ê¸°ë³¸ ìµœì í™” ì˜µì…˜ë“¤
    basic_options = [
        '--headless', '--no-sandbox', '--disable-dev-shm-usage',
        '--disable-gpu', '--disable-extensions', '--disable-plugins',
        '--disable-images', '--window-size=1920,1080'
    ]
    
    # ì¶”ê°€ ë¦¬ì†ŒìŠ¤ ìµœì í™” ì˜µì…˜
    performance_options = [
        '--memory-pressure-off', '--max_old_space_size=2048',
        '--disable-background-timer-throttling',
        '--disable-backgrounding-occluded-windows',
        '--disable-renderer-backgrounding', '--disable-features=TranslateUI',
        '--disable-default-apps', '--disable-web-security',
        '--disable-features=VizDisplayCompositor'
    ]
    
    # ë´‡ íƒì§€ ìš°íšŒ
    stealth_options = [
        '--disable-blink-features=AutomationControlled'
    ]
    
    for option_list in [basic_options, performance_options, stealth_options]:
        for option in option_list:
            options.add_argument(option)
    
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)

    # ëœë¤ User-Agent
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36'
    ]
    options.add_argument(f'--user-agent={random.choice(user_agents)}')

    # ì„±ëŠ¥ ìµœì í™”
    prefs = {
        'profile.default_content_setting_values': {
            'images': 2, 'plugins': 2, 'popups': 2,
            'geolocation': 2, 'notifications': 2, 'media_stream': 2
        }
    }
    options.add_experimental_option('prefs', prefs)

    # 3ë‹¨ê³„ í´ë°± ë©”ì»¤ë‹ˆì¦˜
    possible_paths = [
        '/usr/bin/chromedriver',
        '/usr/local/bin/chromedriver', 
        '/snap/bin/chromium.chromedriver'
    ]

    for path in possible_paths:
        try:
            if os.path.exists(path):
                print(f"[DEBUG] ChromeDriver ì‹œë„: {path}")
                service = Service(path)
                driver = webdriver.Chrome(service=service, options=options)
                print(f"[DEBUG] ChromeDriver ì„±ê³µ: {path}")
                return driver
        except Exception as e:
            error_manager.record_error(ErrorType.DRIVER, e, {'driver_path': path})
            print(f"[DEBUG] ChromeDriver ì‹¤íŒ¨ {path}: {str(e)[:100]}...")
            continue

    # WebDriver Manager ì‹œë„
    try:
        print("[DEBUG] WebDriver Manager ì‹œë„")
        from webdriver_manager.chrome import ChromeDriverManager
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        print("[DEBUG] WebDriver Manager ì„±ê³µ")
        return driver
    except Exception as e:
        error_manager.record_error(ErrorType.DRIVER, e)
        print(f"[DEBUG] WebDriver Manager ì‹¤íŒ¨: {str(e)[:100]}...")

    raise Exception("ëª¨ë“  ChromeDriver ì´ˆê¸°í™” ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

# =============================================================================
# URL ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹° (ê¸°ì¡´ ìœ ì§€)
# =============================================================================

def fix_url_bug(url):
    """URL ë²„ê·¸ ìˆ˜ì • í•¨ìˆ˜"""
    if not url:
        return url

    if url.startswith('ttps://'):
        url = 'h' + url
        print(f"[URL FIX] ttps â†’ https: {url}")
    elif url.startswith('/'):
        if 'onstove.com' in url or 'epicseven' in url:
            url = 'https://page.onstove.com' + url
        elif 'ruliweb.com' in url:
            url = 'https://bbs.ruliweb.com' + url
        elif 'reddit.com' in url:
            url = 'https://www.reddit.com' + url
        print(f"[URL FIX] ìƒëŒ€ê²½ë¡œ ìˆ˜ì •: {url}")
    elif not url.startswith(('http://', 'https://')):
        url = 'https://' + url
        print(f"[URL FIX] í”„ë¡œí† ì½œ ì¶”ê°€: {url}")

    return url

# =============================================================================
# ì˜ë¯¸ìˆëŠ” ë³¸ë¬¸ ì¶”ì¶œ í•¨ìˆ˜ (ê¸°ì¡´ ìœ ì§€)
# =============================================================================

def extract_meaningful_content(text: str) -> str:
    """ì˜ë¯¸ìˆëŠ” ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ ì•Œê³ ë¦¬ì¦˜"""
    if not text or len(text) < 30:
        return ""

    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]\s*', text.strip())
    sentences = [s.strip() for s in sentences if s.strip()]

    if not sentences:
        return text[:100].strip()

    meaningful_sentences = []
    for sentence in sentences:
        if len(sentence) < 10:
            continue

        meaningless_patterns = [
            r'^[ã…‹ã…ã„·ã… ã…œã…¡]+$',
            r'^[!@#$%^&*()_+\-=\[\]{}|;\':",./<>?`~]+$',
            r'^\d+$',
            r'^(ìŒ|ì–´|ì•„|ë„¤|ì˜ˆ|ì‘|ã…‡ã…‡|ã… ã… |ã…œã…œ)$'
        ]

        if any(re.match(pattern, sentence) for pattern in meaningless_patterns):
            continue

        meaningful_keywords = [
            'ë²„ê·¸', 'ì˜¤ë¥˜', 'ë¬¸ì œ', 'ì—ëŸ¬', 'ì•ˆë¨', 'ì‘ë™', 'ì‹¤í–‰',
            'ìºë¦­í„°', 'ìŠ¤í‚¬', 'ì•„í‹°íŒ©íŠ¸', 'ì¥ë¹„', 'ë˜ì „', 'ì•„ë ˆë‚˜', 
            'ê¸¸ë“œ', 'ì´ë²¤íŠ¸', 'ì—…ë°ì´íŠ¸', 'íŒ¨ì¹˜', 'ë°¸ëŸ°ìŠ¤', 'ë„ˆí”„',
            'ê²Œì„', 'í”Œë ˆì´', 'ìœ ì €', 'ìš´ì˜', 'ê³µì§€', 'í™•ë¥ ',
            'ë½‘ê¸°', 'ì†Œí™˜', '6ì„±', 'ê°ì„±', 'ì´ˆì›”', 'ë£¬', 'ì ¬'
        ]

        score = sum(1 for keyword in meaningful_keywords if keyword in sentence)

        if score > 0 or len(sentence) >= 30:
            meaningful_sentences.append(sentence)

    if not meaningful_sentences:
        long_sentences = [s for s in sentences if len(s) >= 20]
        if long_sentences:
            return long_sentences[0]
        else:
            return sentences[0] if sentences else text[:100]

    result = meaningful_sentences[0]
    if len(result) < 50 and len(meaningful_sentences) > 1:
        result += ' ' + meaningful_sentences[1]
    if len(result) < 80 and len(meaningful_sentences) > 2:
        result += ' ' + meaningful_sentences[2]

    return result.strip()


# =============================================================================
# âœ¨ FIXED v4.4: ì•ˆì „í•œ URL í•´ì‹œ ì‹œìŠ¤í…œ (SHA256 ê¸°ë°˜)
# =============================================================================

def get_safe_url_hash(url: str) -> str:
    """ì•ˆì „í•œ URL í•´ì‹œ ìƒì„± (SHA256 ê¸°ë°˜, ì¶©ëŒ ë°©ì§€)"""
    try:
        # SHA256 í•´ì‹œ ìƒì„±
        url_bytes = url.encode('utf-8')
        hash_object = hashlib.sha256(url_bytes)
        
        # 16ìë¦¬ í•´ì‹œ (ì¶©ëŒ í™•ë¥  ê·¹ì†Œ)
        safe_hash = hash_object.hexdigest()[:16]
        
        return safe_hash
        
    except Exception as e:
        error_manager.record_error(ErrorType.GENERAL, e, {'url': url[:100]})
        # í´ë°±: ê¸°ì¡´ ë°©ì‹ (í˜¸í™˜ì„±)
        return str(hash(url) % (10**8))


# =============================================================================
# Stove ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ í•¨ìˆ˜ (v4.4 ì•ˆì „í•œ í•´ì‹œ ì ìš©)
# =============================================================================

def get_stove_post_content(post_url: str, driver: webdriver.Chrome, 
                          source: str = "stove_korea_bug", 
                          schedule_type: str = "frequent") -> str:
    """Stove ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ - v4.4 ì•ˆì „í•œ í•´ì‹œ ì ìš©"""

    # ìºì‹œ í™•ì¸
    cache = load_content_cache()
    

    # âœ¨ FIXED v4.4: ì•ˆì „í•œ í•´ì‹œ ì‚¬ìš©
    url_hash = get_safe_url_hash(post_url)
    


    if url_hash in cache:
        cached_item = cache[url_hash]
        cache_time = datetime.fromisoformat(cached_item.get('timestamp', '2000-01-01'))
        if datetime.now() - cache_time < timedelta(hours=24):
            print(f"[CACHE] ìºì‹œëœ ë‚´ìš© ì‚¬ìš©: {post_url}")
            return cached_item.get('content', "ê²Œì‹œê¸€ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    content_summary = "ê²Œì‹œê¸€ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    try:
        print(f"[DEBUG] ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ ì‹œë„: {post_url}")

        wait_time = CrawlingSchedule.get_wait_time(schedule_type)
        driver.set_page_load_timeout(wait_time + 10)
        driver.get(post_url)

        print(f"[DEBUG] í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘... ({wait_time}ì´ˆ)")
        time.sleep(wait_time)

        WebDriverWait(driver, 15).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )

        # ìµœì í™”ëœ ìŠ¤í¬ë¡¤ë§
        print("[DEBUG] ìµœì í™”ëœ ìŠ¤í¬ë¡¤ë§ ì‹œì‘...")
        driver.execute_script("window.scrollTo(0, 500);")
        time.sleep(2)
        driver.execute_script("window.scrollTo(0, 1000);")
        time.sleep(2)
        driver.execute_script("window.scrollTo(0, 0);")
        time.sleep(1)
        print("[DEBUG] ìµœì í™”ëœ ìŠ¤í¬ë¡¤ë§ ì™„ë£Œ")

        content_selectors = [
            'meta[data-vmid="description"]',
            'meta[name="description"]',
            'div.s-article-content',
            'div.s-article-content-text',
            'section.s-article-body',
            'div.s-board-content',
            '.article-content',
            '.post-content',
            '[class*="content"]'
        ]

        for i, selector in enumerate(content_selectors):
            try:
                elements = driver.find_elements(By.CSS_SELECTOR, selector)
                if elements:
                    for element in elements:
                        if selector.startswith('meta'):
                            raw_text = element.get_attribute('content').strip()
                        else:
                            raw_text = element.text.strip()

                        if not raw_text or len(raw_text) < 30:
                            continue

                        skip_keywords = [
                            'install stove', 'ìŠ¤í† ë¸Œë¥¼ ì„¤ì¹˜', 'ë¡œê·¸ì¸ì´ í•„ìš”', 
                            'javascript', 'ëŒ“ê¸€', 'ê³µìœ ', 'ì¢‹ì•„ìš”', 'ì¶”ì²œ', 'ì‹ ê³ ',
                            'ì‘ì„±ì', 'ì‘ì„±ì¼', 'ì¡°íšŒìˆ˜', 'ì²¨ë¶€íŒŒì¼', 'ë‹¤ìš´ë¡œë“œ'
                        ]

                        if any(skip.lower() in raw_text.lower() for skip in skip_keywords):
                            continue

                        meaningful_content = extract_meaningful_content(raw_text)

                        if len(meaningful_content) >= 50:
                            if len(meaningful_content) > 150:
                                content_summary = meaningful_content[:147] + '...'
                            else:
                                content_summary = meaningful_content

                            print(f"[SUCCESS] ì„ íƒì {i+1}/{len(content_selectors)} '{selector}'ë¡œ ë‚´ìš© ì¶”ì¶œ ì„±ê³µ")
                            print(f"[CONTENT] {content_summary[:80]}...")
                            break

                    if content_summary != "ê²Œì‹œê¸€ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.":
                        break

            except Exception as e:
                error_manager.record_error(ErrorType.PARSE, e, {'selector': selector})
                print(f"[DEBUG] ì„ íƒì '{selector}' ì‹¤íŒ¨: {e}")
                continue

        # ìºì‹œ ì €ì¥
        cache[url_hash] = {
            'content': content_summary,
            'timestamp': datetime.now().isoformat(),
            'url': post_url,
            'source': source
        }
        save_content_cache(cache)

    except TimeoutException as e:
        error_manager.record_error(ErrorType.NETWORK, e, {'url': post_url})
        print(f"[ERROR] í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ: {post_url}")
        content_summary = "â° ê²Œì‹œê¸€ ë¡œë”© ì‹œê°„ ì´ˆê³¼"
    except Exception as e:
        error_manager.record_error(ErrorType.GENERAL, e, {'url': post_url})
        print(f"[ERROR] ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        content_summary = "ğŸ”— ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ ì‹¤íŒ¨"

    return content_summary

# =============================================================================
# ğŸš€ Stove ê²Œì‹œíŒ í¬ë¡¤ë§ + ì¦‰ì‹œ ì²˜ë¦¬ í†µí•© (v4.4 ê°•í™”)
# =============================================================================

def crawl_stove_board(board_url: str, source: str, force_crawl: bool = False, 
                     schedule_type: str = "frequent", region: str = "korea",
                     on_post_process: Optional[Callable[[Dict], None]] = None) -> List[Dict]:
    """
    Stove ê²Œì‹œíŒ í¬ë¡¤ë§ + ì¦‰ì‹œ ì²˜ë¦¬ í†µí•© v4.4
    Master ìš”êµ¬ì‚¬í•­: ê²Œì‹œê¸€ë³„ ì¦‰ì‹œ ì²˜ë¦¬ (í¬ë¡¤ë§â†’ê°ì„±ë¶„ì„â†’ì•Œë¦¼â†’ë§ˆí‚¹)
    """

    posts = []
    link_data = load_crawled_links()

    print(f"[INFO] {source} í¬ë¡¤ë§ ì‹œì‘ - URL: {board_url}")
    print(f"[DEBUG] ê¸°ì¡´ ë§í¬ ìˆ˜: {len(link_data['links'])}, Force Crawl: {force_crawl}")

    driver = None
    try:
        driver = get_chrome_driver()

        wait_time = CrawlingSchedule.get_wait_time(schedule_type)
        driver.set_page_load_timeout(wait_time + 10)
        driver.implicitly_wait(15)

        print(f"[DEBUG] ê²Œì‹œíŒ ì ‘ì† ì¤‘: {board_url}")
        driver.get(board_url)

        print(f"[DEBUG] í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘... ({wait_time}ì´ˆ)")
        time.sleep(wait_time)

        WebDriverWait(driver, 15).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )

        driver.execute_script("window.scrollTo(0, 800);")
        time.sleep(3)

        

        # âœ¨ ENHANCED v4.4: ê´€ë¦¬ëœ ë””ë²„ê·¸ íŒŒì¼ ì €ì¥
        debug_filename = f"{source}_debug_selenium.html"
        debug_file_manager.save_debug_html(debug_filename, driver.page_source)
        


        # JavaScriptë¡œ ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ
        user_posts = driver.execute_script("""
            var userPosts = [];
            const selectors = [
                'section.s-board-item',
                'h3.s-board-title',
                '[class*="board-title"]',
                '[class*="post-title"]',
                'a[href*="/view/"]'
            ];

            var elements = [];
            var successful_selector = '';

            for (var i = 0; i < selectors.length; i++) {
                try {
                    elements = document.querySelectorAll(selectors[i]);
                    if (elements && elements.length > 0) {
                        successful_selector = selectors[i];
                        break;
                    }
                } catch (e) {
                    continue;
                }
            }

            if (!elements || elements.length === 0) {
                return [];
            }

            const officialIds = ['10518001', '10855687', '10855562', '10855132'];

            for (var i = 0; i < Math.min(elements.length, 20); i++) {
                var element = elements[i];