import json
import os
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.keys import Keys
import time
from datetime import datetime, timedelta
import re
import random
import requests
import hashlib
from typing import Dict, List, Optional, Tuple

# ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ ë§í¬ ì €ì¥ íŒŒì¼
CRAWLED_LINKS_FILE = "crawled_links.json"
CONTENT_CACHE_FILE = "content_cache.json"

def load_crawled_links():
    """ì´ë¯¸ í¬ë¡¤ë§ëœ ë§í¬ë“¤ì„ ë¡œë“œ"""
    if os.path.exists(CRAWLED_LINKS_FILE):
        try:
            with open(CRAWLED_LINKS_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # êµ¬ ë²„ì „ í˜¸í™˜ì„± (ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)
                if isinstance(data, list):
                    return {"links": data, "last_updated": datetime.now().isoformat()}
                return data
        except (json.JSONDecodeError, FileNotFoundError):
            print("[WARNING] crawled_links.json íŒŒì¼ ì½ê¸° ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±")

    return {"links": [], "last_updated": datetime.now().isoformat()}

def save_crawled_links(link_data):
    """í¬ë¡¤ë§ëœ ë§í¬ë“¤ì„ ì €ì¥ (ìµœëŒ€ 1000ê°œ ìœ ì§€)"""
    try:
        # ìµœì‹  1000ê°œë§Œ ìœ ì§€ (ë©”ëª¨ë¦¬ ì ˆì•½)
        if len(link_data["links"]) > 1000:
            link_data["links"] = link_data["links"][-1000:]

        link_data["last_updated"] = datetime.now().isoformat()

        with open(CRAWLED_LINKS_FILE, 'w', encoding='utf-8') as f:
            json.dump(link_data, f, ensure_ascii=False, indent=2)

    except Exception as e:
        print(f"[ERROR] ë§í¬ ì €ì¥ ì‹¤íŒ¨: {e}")

def load_content_cache():
    """ê²Œì‹œê¸€ ë‚´ìš© ìºì‹œ ë¡œë“œ (ê°œì„ ì‚¬í•­ 3: ìºì‹œ ì‹œìŠ¤í…œ)"""
    if os.path.exists(CONTENT_CACHE_FILE):
        try:
            with open(CONTENT_CACHE_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            print("[WARNING] content_cache.json íŒŒì¼ ì½ê¸° ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±")

    return {}

def save_content_cache(cache_data):
    """ê²Œì‹œê¸€ ë‚´ìš© ìºì‹œ ì €ì¥ (ê°œì„ ì‚¬í•­ 5: 24ì‹œê°„ ìºì‹œ)"""
    try:
        # ìµœì‹  500ê°œë§Œ ìœ ì§€
        if len(cache_data) > 500:
            sorted_items = sorted(cache_data.items(), key=lambda x: x[1].get('timestamp', ''), reverse=True)
            cache_data = dict(sorted_items[:500])

        with open(CONTENT_CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)

    except Exception as e:
        print(f"[ERROR] ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

def get_url_hash(url: str) -> str:
    """URLì˜ í•´ì‹œê°’ ìƒì„± (ìºì‹œ í‚¤ìš©)"""
    return hashlib.md5(url.encode('utf-8')).hexdigest()

def extract_content_summary(content: str) -> str:
    """ê²Œì‹œê¸€ ë‚´ìš©ì„ í•œ ì¤„ë¡œ ìš”ì•½ (ê°œì„ ì‚¬í•­ 2: í•œì¤„ ìš”ì•½ ê¸°ëŠ¥)"""
    if not content or len(content.strip()) < 10:
        return "ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”."

    # ë¶ˆí•„ìš”í•œ ê³µë°± ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°
    content = re.sub(r'\s+', ' ', content.strip())
    content = re.sub(r'[^\w\sê°€-í£.,!?]', '', content)

    # ì²« ë²ˆì§¸ ì™„ì „í•œ ë¬¸ì¥ ì¶”ì¶œ
    sentences = re.split(r'[.!?]', content)
    first_sentence = sentences[0].strip() if sentences else content

    # ê¸¸ì´ ì œí•œ (100ì ì´ë‚´)
    if len(first_sentence) > 100:
        first_sentence = first_sentence[:97] + '...'
    elif len(first_sentence) > 10:
        first_sentence = first_sentence + '...'

    return first_sentence if first_sentence else "ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”."

def fix_stove_url(url):
    """ìŠ¤í† ë¸Œ URL ìˆ˜ì • í•¨ìˆ˜ - URL ë²„ê·¸ ì™„ì „ í•´ê²°"""
    if not url:
        return url

    # 'ttps://' -> 'https://' ìˆ˜ì •
    if url.startswith('ttps://'):
        url = 'h' + url
        print(f"[FIX] URL ìˆ˜ì •ë¨: {url}")

    # 'ttp://' -> 'http://' ìˆ˜ì • (í˜¹ì‹œ ëª¨ë¥¼ ê²½ìš°)
    elif url.startswith('ttp://'):
        url = 'h' + url
        print(f"[FIX] URL ìˆ˜ì •ë¨: {url}")

    # ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
    elif url.startswith('/'):
        url = 'https://page.onstove.com' + url
        print(f"[FIX] ìƒëŒ€ê²½ë¡œ URL ìˆ˜ì •ë¨: {url}")

    # URLì´ ì œëŒ€ë¡œ ì‹œì‘í•˜ì§€ ì•ŠëŠ” ê²½ìš°
    elif not url.startswith('http'):
        url = 'https://page.onstove.com' + ('/' if not url.startswith('/') else '') + url
        print(f"[FIX] ë¹„ì •ìƒ URL ìˆ˜ì •ë¨: {url}")

    return url

def get_chrome_driver():
    """Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” (í˜¸í™˜ì„± ìµœì í™”)"""
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--disable-extensions')
    options.add_argument('--disable-plugins')
    options.add_argument('--disable-images')
    options.add_argument('--window-size=1920,1080')

    # ë´‡ íƒì§€ ìš°íšŒ ì„¤ì •
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)

    # ëœë¤ User-Agent
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
    ]
    options.add_argument(f'--user-agent={random.choice(user_agents)}')

    # ì„±ëŠ¥ ìµœì í™”
    prefs = {
        'profile.default_content_setting_values': {
            'images': 2,
            'plugins': 2,
            'popups': 2,
            'geolocation': 2,
            'notifications': 2,
            'media_stream': 2,
        }
    }
    options.add_experimental_option('prefs', prefs)

    driver = None

    # í˜¸í™˜ ê°€ëŠ¥í•œ ChromeDriver ê²½ë¡œë“¤
    possible_paths = [
        '/usr/bin/chromedriver',
        '/usr/local/bin/chromedriver',
        '/snap/bin/chromium.chromedriver'
    ]

    # ê²½ë¡œë³„ ì‹œë„
    for path in possible_paths:
        try:
            if os.path.exists(path):
                print(f"[DEBUG] ChromeDriver ì‹œë„: {path}")
                service = Service(path)
                driver = webdriver.Chrome(service=service, options=options)
                print(f"[DEBUG] ChromeDriver ì„±ê³µ: {path}")
                return driver
        except Exception as e:
            print(f"[DEBUG] ChromeDriver ì‹¤íŒ¨ {path}: {str(e)[:100]}...")
            continue

    # ì‹œìŠ¤í…œ ê¸°ë³¸ ChromeDriver
    try:
        print("[DEBUG] ì‹œìŠ¤í…œ ê¸°ë³¸ ChromeDriver ì‹œë„")
        driver = webdriver.Chrome(options=options)
        print("[DEBUG] ì‹œìŠ¤í…œ ê¸°ë³¸ ChromeDriver ì„±ê³µ")
        return driver
    except Exception as e:
        print(f"[DEBUG] ì‹œìŠ¤í…œ ê¸°ë³¸ ChromeDriver ì‹¤íŒ¨: {str(e)[:100]}...")

    # WebDriver Manager (ìµœí›„ ìˆ˜ë‹¨)
    try:
        print("[DEBUG] WebDriver Manager ì‹œë„")
        from webdriver_manager.chrome import ChromeDriverManager
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        print("[DEBUG] WebDriver Manager ì„±ê³µ")
        return driver
    except Exception as e:
        print(f"[DEBUG] WebDriver Manager ì‹¤íŒ¨: {str(e)[:100]}...")

    raise Exception("ëª¨ë“  ChromeDriver ì´ˆê¸°í™” ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

def get_stove_post_content(post_url: str, driver: webdriver.Chrome = None) -> str:
    """ìŠ¤í† ë¸Œ ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ (ê°œì„ ì‚¬í•­ 1: ìŠ¤í† ë¸Œ ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ ì™„ë£Œ)"""

    # ìºì‹œ í™•ì¸ (ê°œì„ ì‚¬í•­ 5: 24ì‹œê°„ ìºì‹œ)
    cache = load_content_cache()
    url_hash = get_url_hash(post_url)

    if url_hash in cache:
        cached_item = cache[url_hash]
        # ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬ (24ì‹œê°„)
        cache_time = datetime.fromisoformat(cached_item.get('timestamp', '2000-01-01'))
        if datetime.now() - cache_time < timedelta(hours=24):
            print(f"[CACHE] ìºì‹œëœ ë‚´ìš© ì‚¬ìš©: {post_url}")
            return cached_item.get('content', "ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”.")

    # Driver ìƒì„± ì—¬ë¶€ í™•ì¸ (ê°œì„ ì‚¬í•­ 4: Driver ì¬ì‚¬ìš©)
    driver_created = False
    if driver is None:
        try:
            driver = get_chrome_driver()
            driver_created = True
        except Exception as e:
            print(f"[ERROR] Driver ìƒì„± ì‹¤íŒ¨: {e}")
            return "ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”."

    content_summary = "ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”."

    try:
        print(f"[DEBUG] ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ ì‹œë„: {post_url}")

        # í˜ì´ì§€ ë¡œë”© (ê°œì„ ì‚¬í•­ 8: íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬ ê°•í™”)
        driver.set_page_load_timeout(15)
        driver.get(post_url)

        # í˜ì´ì§€ ë¡œë”© ì™„ë£Œ ëŒ€ê¸°
        WebDriverWait(driver, 10).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )

        # ì¶”ê°€ ëŒ€ê¸° (ë™ì  ì½˜í…ì¸  ë¡œë”©)
        time.sleep(random.uniform(3, 5))

        # JavaScriptë¡œ ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ (ê°œì„ ì‚¬í•­ 7: ë‹¤ì¤‘ CSS ì„ íƒì fallback)
        content = driver.execute_script("""
            // ìŠ¤í† ë¸Œ ê²Œì‹œê¸€ ë‚´ìš© ì„ íƒìë“¤ (ìš°ì„ ìˆœìœ„ ìˆœ)
            var contentSelectors = [
                // ë©”ì¸ ì½˜í…ì¸ 
                '.s-article-content',
                '.s-article-content-text',
                '.article-content',
                '.s-board-content',
                '.board-content',
                '.post-content',
                '.content',

                // ëŒ€ì²´ ì„ íƒì
                '[class*="article-content"]',
                '[class*="board-content"]',
                '[class*="post-content"]',
                '[class*="content"]',

                // í…ìŠ¤íŠ¸ ì˜ì—­
                '.s-text-content',
                '.text-content',
                'div[class*="text"]',

                // ì¼ë°˜ì ì¸ ì„ íƒì
                'main',
                'article',
                '.main-content',
                '#content'
            ];

            var content = '';
            var foundSelector = '';

            for (var i = 0; i < contentSelectors.length; i++) {
                var elements = document.querySelectorAll(contentSelectors[i]);

                for (var j = 0; j < elements.length; j++) {
                    var element = elements[j];
                    var text = element.innerText || element.textContent || '';

                    // ìœ íš¨í•œ ë‚´ìš©ì¸ì§€ í™•ì¸
                    if (text && text.trim().length > 20) {
                        content = text.trim();
                        foundSelector = contentSelectors[i];
                        console.log('ë‚´ìš© ì¶”ì¶œ ì„±ê³µ:', foundSelector, 'ê¸¸ì´:', content.length);
                        break;
                    }
                }

                if (content) break;
            }

            // ë‚´ìš©ì´ ì—†ìœ¼ë©´ í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
            if (!content) {
                var bodyText = document.body.innerText || document.body.textContent || '';
                if (bodyText && bodyText.trim().length > 50) {
                    // ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°
                    var lines = bodyText.split('\n');
                    var meaningfulLines = lines.filter(function(line) {
                        return line.trim().length > 10 && 
                               !line.includes('ë¡œê·¸ì¸') && 
                               !line.includes('íšŒì›ê°€ì…') &&
                               !line.includes('ë©”ë‰´') &&
                               !line.includes('ê²€ìƒ‰');
                    });

                    if (meaningfulLines.length > 0) {
                        content = meaningfulLines.slice(0, 3).join(' ');
                        foundSelector = 'body-fallback';
                        console.log('ëŒ€ì²´ ë‚´ìš© ì¶”ì¶œ:', foundSelector, 'ê¸¸ì´:', content.length);
                    }
                }
            }

            console.log('ìµœì¢… ë‚´ìš© ê¸¸ì´:', content.length);
            return content;
        """)

        # ë‚´ìš© í›„ì²˜ë¦¬ (ê°œì„ ì‚¬í•­ 2: í•œì¤„ ìš”ì•½ ê¸°ëŠ¥)
        if content and len(content.strip()) > 20:
            content_summary = extract_content_summary(content)
            print(f"[SUCCESS] ë‚´ìš© ì¶”ì¶œ ì„±ê³µ: {content_summary[:50]}...")
        else:
            print(f"[WARNING] ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ë‚´ìš© ë¶€ì¡±: {len(content) if content else 0}ì")

            # ì œëª©ì´ë¼ë„ ì¶”ì¶œ ì‹œë„ (ê°œì„ ì‚¬í•­ 9: ì—ëŸ¬ ìƒí™©ë³„ ëŒ€ì‘ ë¡œì§)
            try:
                title = driver.execute_script("""
                    var titleSelectors = [
                        'h1', 'h2', 'h3',
                        '.title', '.s-title', '.board-title',
                        '[class*="title"]'
                    ];

                    for (var i = 0; i < titleSelectors.length; i++) {
                        var element = document.querySelector(titleSelectors[i]);
                        if (element && element.innerText) {
                            return element.innerText.trim();
                        }
                    }
                    return '';
                """)

                if title and len(title) > 5:
                    content_summary = f"ì œëª©: {title[:80]}..."
                    print(f"[FALLBACK] ì œëª© ì¶”ì¶œ ì„±ê³µ: {content_summary}")

            except Exception as e:
                print(f"[ERROR] ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        # ìºì‹œ ì €ì¥ (ê°œì„ ì‚¬í•­ 3: ìºì‹œ ì‹œìŠ¤í…œ)
        cache[url_hash] = {
            'content': content_summary,
            'timestamp': datetime.now().isoformat(),
            'url': post_url
        }
        save_content_cache(cache)

    except TimeoutException:
        print(f"[ERROR] í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ: {post_url}")
        content_summary = "â° ê²Œì‹œê¸€ ë¡œë”© ì‹œê°„ ì´ˆê³¼. ë§í¬ë¥¼ í´ë¦­í•˜ì—¬ í™•ì¸í•˜ì„¸ìš”."

    except Exception as e:
        print(f"[ERROR] ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        content_summary = "ğŸ”— ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”."

    finally:
        if driver_created and driver:
            try:
                driver.quit()
            except:
                pass

    return content_summary


def fetch_ruliweb_epic7_board():
    """ë£¨ë¦¬ì›¹ ì—í”½ì„¸ë¸ ê²Œì‹œíŒ í¬ë¡¤ë§"""
    posts = []
    link_data = load_crawled_links()
    crawled_links = link_data["links"]

    print(f"[DEBUG] ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì‹œì‘ - ì €ì¥ëœ ë§í¬ ìˆ˜: {len(crawled_links)}")

    driver = None
    try:
        print("[DEBUG] ë£¨ë¦¬ì›¹ìš© Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì¤‘...")
        driver = get_chrome_driver()

        driver.set_page_load_timeout(30)
        driver.implicitly_wait(10)

        url = "https://bbs.ruliweb.com/game/84834"
        print(f"[DEBUG] ë£¨ë¦¬ì›¹ ì ‘ì† ì¤‘: {url}")

        driver.get(url)
        time.sleep(5)

        print("[DEBUG] ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ëª©ë¡ ê²€ìƒ‰ ì¤‘...")

        # ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ì¶”ì¶œ (ê°œì„ ì‚¬í•­ 7: ë‹¤ì¤‘ CSS ì„ íƒì fallback)
        selectors = [
            ".subject_link",
            ".table_body .subject a",
            "td.subject a",
            "a[href*='/read/']",
            ".board_list_table .subject_link",
            "table tr td a[href*='read']"
        ]

        articles = []
        for selector in selectors:
            try:
                articles = driver.find_elements(By.CSS_SELECTOR, selector)
                if articles:
                    print(f"[DEBUG] ë£¨ë¦¬ì›¹ ì„ íƒì ì„±ê³µ: {selector} ({len(articles)}ê°œ)")
                    break
            except NoSuchElementException:
                continue

        if not articles:
            print("[WARNING] ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. HTML ì €ì¥...")
            with open("ruliweb_debug_selenium.html", "w", encoding="utf-8") as f:
                f.write(driver.page_source)
            return posts

        # ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ
        for i, article in enumerate(articles[:15]):
            try:
                title = article.text.strip()
                link = article.get_attribute("href")

                if not title or not link or len(title) < 3:
                    continue

                # ê³µì§€ì‚¬í•­ ë° ì¶”ì²œê¸€ í•„í„°ë§
                if any(keyword in title for keyword in ['ê³µì§€', 'í•„ë…', 'ì´ë²¤íŠ¸', 'ì¶”ì²œ', 'ë² ìŠ¤íŠ¸', 'ê³µì§€ì‚¬í•­']):
                    continue

                # URL ì •ê·œí™”
                if link.startswith('/'):
                    link = 'https://bbs.ruliweb.com' + link

                if link not in crawled_links:
                    post_data = {
                        "title": title,
                        "url": link,
                        "content": "ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”.",
                        "timestamp": datetime.now().isoformat(),
                        "source": "ruliweb_epic7"
                    }
                    posts.append(post_data)
                    crawled_links.append(link)
                    print(f"[NEW] ë£¨ë¦¬ì›¹ ìƒˆ ê²Œì‹œê¸€ ({i+1}): {title[:50]}...")

            except Exception as e:
                print(f"[ERROR] ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ {i+1} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue

        print(f"[DEBUG] ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì™„ë£Œ: {len(posts)}ê°œ ìƒˆ ê²Œì‹œê¸€")

    except Exception as e:
        print(f"[ERROR] ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        if driver:
            try:
                with open("ruliweb_error_debug.html", "w", encoding="utf-8") as f:
                    f.write(driver.page_source)
            except:
                pass

    finally:
        if driver:
            driver.quit()

    # ë§í¬ ì €ì¥
    link_data["links"] = crawled_links
    save_crawled_links(link_data)

    return posts

def fetch_stove_bug_board():
    """ìŠ¤í† ë¸Œ ì—í”½ì„¸ë¸ ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§ (ëª¨ë“  ê°œì„ ì‚¬í•­ ì ìš©)"""
    posts = []
    link_data = load_crawled_links()
    crawled_links = link_data["links"]

    print(f"[DEBUG] ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹œì‘ - ì €ì¥ëœ ë§í¬ ìˆ˜: {len(crawled_links)}")

    driver = None
    try:
        print("[DEBUG] ìŠ¤í† ë¸Œ ë²„ê·¸ìš© Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì¤‘...")
        driver = get_chrome_driver()

        driver.set_page_load_timeout(30)
        driver.implicitly_wait(10)

        url = "https://page.onstove.com/epicseven/kr/list/1012?page=1&direction=LATEST"
        print(f"[DEBUG] ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œíŒ ì ‘ì† ì¤‘: {url}")

        driver.get(url)

        print("[DEBUG] ìŠ¤í† ë¸Œ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘...")
        time.sleep(8)

        # ìŠ¤í¬ë¡¤í•˜ì—¬ ê²Œì‹œê¸€ ì˜ì—­ê¹Œì§€ ë¡œë”©
        print("[DEBUG] ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œíŒ ê²Œì‹œê¸€ ì˜ì—­ íƒìƒ‰ ì¤‘...")
        driver.execute_script("window.scrollTo(0, 500);")
        time.sleep(3)
        driver.execute_script("window.scrollTo(0, 800);")
        time.sleep(3)
        driver.execute_script("window.scrollTo(0, 1200);")
        time.sleep(3)
        driver.execute_script("window.scrollTo(0, 0);")
        time.sleep(2)

        # ë””ë²„ê¹…ìš© HTML ì €ì¥
        html_content = driver.page_source
        with open("stove_bug_debug_selenium.html", "w", encoding="utf-8") as f:
            f.write(html_content)

        # JavaScriptë¡œ ê²Œì‹œê¸€ ì¶”ì¶œ (ê°œì„ ì‚¬í•­ 7: ë‹¤ì¤‘ CSS ì„ íƒì fallback)
        user_posts = driver.execute_script("""
            var userPosts = [];

            // ì—¬ëŸ¬ ì„ íƒì ì‹œë„ (ì•ˆì •ì„± í–¥ìƒ)
            var selectors = [
                'section.s-board-item',
                '.s-board-item',
                '.board-item',
                '[class*="board-item"]'
            ];

            var sections = [];
            for (var s = 0; s < selectors.length; s++) {
                sections = document.querySelectorAll(selectors[s]);
                if (sections.length > 0) {
                    console.log('ì„ íƒì ì„±ê³µ:', selectors[s], sections.length);
                    break;
                }
            }

            console.log('ì „ì²´ ê²Œì‹œê¸€ ì„¹ì…˜ ìˆ˜:', sections.length);

            var officialIds = ['10518001', '10855687', '10855562', '10855132'];

            for (var i = 0; i < sections.length; i++) {
                var section = sections[i];

                try {
                    // ë§í¬ ìš”ì†Œ ì°¾ê¸° (ë‹¤ì¤‘ ì„ íƒì)
                    var linkSelectors = [
                        'a.s-board-link',
                        '.s-board-link',
                        'a[href*="/view/"]',
                        'a'
                    ];

                    var linkElement = null;
                    for (var ls = 0; ls < linkSelectors.length; ls++) {
                        linkElement = section.querySelector(linkSelectors[ls]);
                        if (linkElement && linkElement.href) break;
                    }

                    if (!linkElement || !linkElement.href) continue;

                    var href = linkElement.href;
                    var idMatch = href.match(/\/view\/(\d+)/);
                    if (!idMatch) continue;
                    var postId = idMatch[1];

                    if (officialIds.includes(postId)) {
                        console.log('ê³µì§€ ID ì œì™¸:', postId);
                        continue;
                    }

                    // ê³µì§€/ì´ë²¤íŠ¸ ë°°ì§€ í™•ì¸
                    var isNotice = section.querySelector('i.element-badge__s.notice') || 
                                  section.querySelector('.notice') ||
                                  section.querySelector('[class*="notice"]');
                    var isEvent = section.querySelector('i.element-badge__s.event') ||
                                 section.querySelector('.event') ||
                                 section.querySelector('[class*="event"]');
                    var isOfficial = section.querySelector('span.s-profile-staff-official') ||
                                    section.querySelector('[class*="official"]');

                    if (isNotice || isEvent || isOfficial) {
                        console.log('ê³µì§€/ì´ë²¤íŠ¸ ì œì™¸:', postId);
                        continue;
                    }

                    // ì œëª© ìš”ì†Œ ì°¾ê¸° (ë‹¤ì¤‘ ì„ íƒì)
                    var titleSelectors = [
                        'h3.s-board-title span.s-board-title-text',
                        '.s-board-title-text',
                        '.board-title',
                        'h3 span',
                        '.title'
                    ];

                    var titleElement = null;
                    for (var ts = 0; ts < titleSelectors.length; ts++) {
                        titleElement = section.querySelector(titleSelectors[ts]);
                        if (titleElement && titleElement.innerText) break;
                    }

                    if (!titleElement) {
                        console.log('ì œëª© ìš”ì†Œ ì—†ìŒ:', postId);
                        continue;
                    }

                    var title = titleElement.innerText.trim();
                    if (!title || title.length < 3) {
                        console.log('ì œëª© ì—†ìŒ ë˜ëŠ” ë„ˆë¬´ ì§§ìŒ:', postId);
                        continue;
                    }

                    userPosts.push({
                        title: title.substring(0, 200).trim(),
                        href: href,
                        id: postId
                    });

                    console.log('ìœ ì € ê²Œì‹œê¸€ ì¶”ê°€:', title.substring(0, 30));

                } catch (e) {
                    console.log('ê²Œì‹œê¸€ ì²˜ë¦¬ ì˜¤ë¥˜:', e.message);
                    continue;
                }
            }

            console.log('ìµœì¢… ë°œê²¬ëœ ìœ ì € ê²Œì‹œê¸€ ìˆ˜:', userPosts.length);
            return userPosts.slice(0, 15);
        """)

        print(f"[DEBUG] JavaScriptë¡œ {len(user_posts)}ê°œ ìœ ì € ê²Œì‹œê¸€ ë°œê²¬")

        # ìœ ì € ê²Œì‹œê¸€ ì²˜ë¦¬ (ê°œì„ ì‚¬í•­ 6: ë°°ì¹˜ ì²˜ë¦¬ + ê°œì„ ì‚¬í•­ 4: Driver ì¬ì‚¬ìš©)
        for i, post_info in enumerate(user_posts, 1):
            try:
                href = post_info['href']
                title = post_info['title']

                # URL ìˆ˜ì • ì ìš© (í•µì‹¬ ë²„ê·¸ ìˆ˜ì •)
                href = fix_stove_url(href)

                print(f"[DEBUG] ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œê¸€ {i}: URL = {href}")
                print(f"[DEBUG] ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œê¸€ {i}: ì œëª© = {title[:50]}...")

                if href in crawled_links:
                    print(f"[DEBUG] ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œê¸€ {i}: ì´ë¯¸ í¬ë¡¤ë§ëœ ë§í¬")
                    continue

                if title and href and len(title) > 3:
                    # ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ (ê°œì„ ì‚¬í•­ 1: ìŠ¤í† ë¸Œ ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ ì™„ë£Œ)
                    content = get_stove_post_content(href, driver)

                    post_data = {
                        "title": title,
                        "url": href,
                        "content": content,  # ë‚´ìš© ì¶”ê°€
                        "timestamp": datetime.now().isoformat(),
                        "source": "stove_bug"
                    }
                    posts.append(post_data)
                    crawled_links.append(href)
                    print(f"[NEW] ìŠ¤í† ë¸Œ ë²„ê·¸ ìƒˆ ê²Œì‹œê¸€ ë°œê²¬ ({i}): {title[:50]}...")
                    print(f"[CONTENT] ë‚´ìš©: {content[:100]}...")
                else:
                    print(f"[DEBUG] ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œê¸€ {i}: ì¡°ê±´ ë¯¸ì¶©ì¡±")

                # ì²˜ë¦¬ ê°„ ì§€ì—° (ë´‡ íƒì§€ ë°©ì§€)
                time.sleep(random.uniform(1, 2))

            except Exception as e:
                print(f"[ERROR] ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œê¸€ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue

        print(f"[DEBUG] ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œíŒ ì²˜ë¦¬ ê²°ê³¼: {len(user_posts)}ê°œ ì¤‘ ìƒˆ ê²Œì‹œê¸€ {len(posts)}ê°œ ë°œê²¬")

    except Exception as e:
        print(f"[ERROR] ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    finally:
        if driver:
            print("[DEBUG] ìŠ¤í† ë¸Œ ë²„ê·¸ Chrome ë“œë¼ì´ë²„ ì¢…ë£Œ ì¤‘...")
            driver.quit()

    # ë§í¬ ì €ì¥
    link_data["links"] = crawled_links
    save_crawled_links(link_data)

    print(f"[DEBUG] ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œíŒì—ì„œ {len(posts)}ê°œ ìƒˆ ê²Œì‹œê¸€ ë°œê²¬")
    return posts

def fetch_stove_general_board():
    """ìŠ¤í† ë¸Œ ì—í”½ì„¸ë¸ ììœ ê²Œì‹œíŒ í¬ë¡¤ë§ (ëª¨ë“  ê°œì„ ì‚¬í•­ ì ìš©)"""
    posts = []
    link_data = load_crawled_links()
    crawled_links = link_data["links"]

    print(f"[DEBUG] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹œì‘ - ì €ì¥ëœ ë§í¬ ìˆ˜: {len(crawled_links)}")

    driver = None
    try:
        print("[DEBUG] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒìš© Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì¤‘...")
        driver = get_chrome_driver()

        driver.set_page_load_timeout(30)
        driver.implicitly_wait(10)

        url = "https://page.onstove.com/epicseven/kr/list/1005?page=1&direction=LATEST"
        print(f"[DEBUG] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ ì ‘ì† ì¤‘: {url}")

        driver.get(url)

        print("[DEBUG] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘...")
        time.sleep(8)

        # ìŠ¤í¬ë¡¤í•˜ì—¬ ê²Œì‹œê¸€ ì˜ì—­ê¹Œì§€ ë¡œë”©
        print("[DEBUG] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ ê²Œì‹œê¸€ ì˜ì—­ íƒìƒ‰ ì¤‘...")
        driver.execute_script("window.scrollTo(0, 500);")
        time.sleep(3)
        driver.execute_script("window.scrollTo(0, 800);")
        time.sleep(3)
        driver.execute_script("window.scrollTo(0, 1200);")
        time.sleep(3)
        driver.execute_script("window.scrollTo(0, 0);")
        time.sleep(2)

        # ë””ë²„ê¹…ìš© HTML ì €ì¥
        html_content = driver.page_source
        with open("stove_general_debug_selenium.html", "w", encoding="utf-8") as f:
            f.write(html_content)

        # JavaScriptë¡œ ììœ ê²Œì‹œíŒ ê²Œì‹œê¸€ ì¶”ì¶œ
        user_posts = driver.execute_script("""
            var userPosts = [];

            // ë™ì¼í•œ ì„ íƒì ë¡œì§ ì‚¬ìš©
            var selectors = [
                'section.s-board-item',
                '.s-board-item',
                '.board-item',
                '[class*="board-item"]'
            ];

            var sections = [];
            for (var s = 0; s < selectors.length; s++) {
                sections = document.querySelectorAll(selectors[s]);
                if (sections.length > 0) {
                    console.log('ììœ ê²Œì‹œíŒ ì„ íƒì ì„±ê³µ:', selectors[s], sections.length);
                    break;
                }
            }

            console.log('ììœ ê²Œì‹œíŒ ì „ì²´ ê²Œì‹œê¸€ ì„¹ì…˜ ìˆ˜:', sections.length);

            var officialIds = ['10518001', '10855687', '10855562', '10855132'];

            for (var i = 0; i < sections.length; i++) {
                var section = sections[i];

                try {
                    var linkSelectors = [
                        'a.s-board-link',
                        '.s-board-link',
                        'a[href*="/view/"]',
                        'a'
                    ];

                    var linkElement = null;
                    for (var ls = 0; ls < linkSelectors.length; ls++) {
                        linkElement = section.querySelector(linkSelectors[ls]);
                        if (linkElement && linkElement.href) break;
                    }

                    if (!linkElement || !linkElement.href) continue;

                    var href = linkElement.href;
                    var idMatch = href.match(/\/view\/(\d+)/);
                    if (!idMatch) continue;
                    var postId = idMatch[1];

                    if (officialIds.includes(postId)) continue;

                    var isNotice = section.querySelector('i.element-badge__s.notice') || 
                                  section.querySelector('.notice') ||
                                  section.querySelector('[class*="notice"]');
                    var isEvent = section.querySelector('i.element-badge__s.event') ||
                                 section.querySelector('.event') ||
                                 section.querySelector('[class*="event"]');
                    var isOfficial = section.querySelector('span.s-profile-staff-official') ||
                                    section.querySelector('[class*="official"]');

                    if (isNotice || isEvent || isOfficial) continue;

                    var titleSelectors = [
                        'h3.s-board-title span.s-board-title-text',
                        '.s-board-title-text',
                        '.board-title',
                        'h3 span',
                        '.title'
                    ];

                    var titleElement = null;
                    for (var ts = 0; ts < titleSelectors.length; ts++) {
                        titleElement = section.querySelector(titleSelectors[ts]);
                        if (titleElement && titleElement.innerText) break;
                    }

                    if (!titleElement) continue;

                    var title = titleElement.innerText.trim();
                    if (!title || title.length < 3) continue;

                    userPosts.push({
                        title: title.substring(0, 200).trim(),
                        href: href,
                        id: postId
                    });

                    console.log('ììœ ê²Œì‹œíŒ ìœ ì € ê²Œì‹œê¸€ ì¶”ê°€:', title.substring(0, 30));

                } catch (e) {
                    console.log('ììœ ê²Œì‹œíŒ ê²Œì‹œê¸€ ì²˜ë¦¬ ì˜¤ë¥˜:', e.message);
                    continue;
                }
            }

            console.log('ììœ ê²Œì‹œíŒ ìµœì¢… ë°œê²¬ëœ ìœ ì € ê²Œì‹œê¸€ ìˆ˜:', userPosts.length);
            return userPosts.slice(0, 15);
        """)

        print(f"[DEBUG] JavaScriptë¡œ {len(user_posts)}ê°œ ììœ ê²Œì‹œíŒ ê²Œì‹œê¸€ ë°œê²¬")

        # ììœ ê²Œì‹œíŒ ê²Œì‹œê¸€ ì²˜ë¦¬ (ë‚´ìš© ì¶”ì¶œ í¬í•¨)
        for i, post_info in enumerate(user_posts, 1):
            try:
                href = post_info['href']
                title = post_info['title']

                # URL ìˆ˜ì • ì ìš©
                href = fix_stove_url(href)

                print(f"[DEBUG] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ ê²Œì‹œê¸€ {i}: URL = {href}")
                print(f"[DEBUG] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ ê²Œì‹œê¸€ {i}: ì œëª© = {title[:50]}...")

                if href in crawled_links:
                    print(f"[DEBUG] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ ê²Œì‹œê¸€ {i}: ì´ë¯¸ í¬ë¡¤ë§ëœ ë§í¬")
                    continue

                # ì œëª© ê¸¸ì´ ë° ìœ íš¨ì„± ê²€ì‚¬ ê°•í™”
                if not title or len(title) < 4:
                    print(f"[DEBUG] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ ê²Œì‹œê¸€ {i}: ì¡°ê±´ ë¯¸ì¶©ì¡±")
                    continue

                # ì˜ë¯¸ì—†ëŠ” ì œëª© í•„í„°ë§
                meaningless_patterns = [
                    r'^[.]{3,}$',  # ì ë§Œ ìˆëŠ” ì œëª©
                    r'^[ã…‹ã…ã…—ã…œã…‘]{3,}$',  # ììŒ/ëª¨ìŒë§Œ ìˆëŠ” ì œëª©
                    r'^[!@#$%^&*()]{3,}$',  # íŠ¹ìˆ˜ë¬¸ìë§Œ ìˆëŠ” ì œëª©
                ]

                is_meaningless = any(re.match(pattern, title) for pattern in meaningless_patterns)
                if is_meaningless:
                    print(f"[DEBUG] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ ê²Œì‹œê¸€ {i}: ì¡°ê±´ ë¯¸ì¶©ì¡±")
                    continue

                # ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ (Driver ì¬ì‚¬ìš©)
                content = get_stove_post_content(href, driver)

                post_data = {
                    "title": title,
                    "url": href,
                    "content": content,  # ë‚´ìš© ì¶”ê°€
                    "timestamp": datetime.now().isoformat(),
                    "source": "stove_general"
                }
                posts.append(post_data)
                crawled_links.append(href)
                print(f"[NEW] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ ê²Œì‹œê¸€ ë°œê²¬ ({i}): {title[:50]}...")
                print(f"[CONTENT] ë‚´ìš©: {content[:100]}...")

                # ì²˜ë¦¬ ê°„ ì§€ì—° (ë´‡ íƒì§€ ë°©ì§€)
                time.sleep(random.uniform(1, 2))

            except Exception as e:
                print(f"[ERROR] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ ê²Œì‹œê¸€ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue

        print(f"[DEBUG] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ ì²˜ë¦¬ ì™„ë£Œ: {len(posts)}ê°œ ìƒˆ ê²Œì‹œê¸€")

    except Exception as e:
        print(f"[ERROR] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    finally:
        if driver:
            print("[DEBUG] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ Chrome ë“œë¼ì´ë²„ ì¢…ë£Œ ì¤‘...")
            driver.quit()

    # ë§í¬ ì €ì¥
    link_data["links"] = crawled_links
    save_crawled_links(link_data)

    print(f"[DEBUG] ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒì—ì„œ {len(posts)}ê°œ ìƒˆ ê²Œì‹œê¸€ ë°œê²¬")
    return posts


def crawl_korean_sites():
    """í•œêµ­ ì‚¬ì´íŠ¸ë“¤ í¬ë¡¤ë§ (ë£¨ë¦¬ì›¹ + ìŠ¤í† ë¸Œ ë²„ê·¸ + ìŠ¤í† ë¸Œ ììœ )"""
    all_posts = []

    try:
        print("[INFO] === í•œêµ­ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹œì‘ ===")

        # 1. ë£¨ë¦¬ì›¹ í¬ë¡¤ë§
        print("[INFO] 1/3 ë£¨ë¦¬ì›¹ ì—í”½ì„¸ë¸ ê²Œì‹œíŒ í¬ë¡¤ë§")
        ruliweb_posts = fetch_ruliweb_epic7_board()
        all_posts.extend(ruliweb_posts)
        print(f"[INFO] ë£¨ë¦¬ì›¹: {len(ruliweb_posts)}ê°œ ìƒˆ ê²Œì‹œê¸€")

        # í¬ë¡¤ë§ ê°„ ì§€ì—°
        time.sleep(random.uniform(5, 8))

        # 2. ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§
        print("[INFO] 2/3 ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§ (ë‚´ìš© ì¶”ì¶œ í¬í•¨)")
        stove_bug_posts = fetch_stove_bug_board()
        all_posts.extend(stove_bug_posts)
        print(f"[INFO] ìŠ¤í† ë¸Œ ë²„ê·¸: {len(stove_bug_posts)}ê°œ ìƒˆ ê²Œì‹œê¸€")

        # í¬ë¡¤ë§ ê°„ ì§€ì—°
        time.sleep(random.uniform(5, 8))

        # 3. ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ í¬ë¡¤ë§
        print("[INFO] 3/3 ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ í¬ë¡¤ë§ (ë‚´ìš© ì¶”ì¶œ í¬í•¨)")
        stove_general_posts = fetch_stove_general_board()
        all_posts.extend(stove_general_posts)
        print(f"[INFO] ìŠ¤í† ë¸Œ ììœ : {len(stove_general_posts)}ê°œ ìƒˆ ê²Œì‹œê¸€")

    except Exception as e:
        print(f"[ERROR] í•œêµ­ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

    print(f"[INFO] === í•œêµ­ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_posts)}ê°œ ìƒˆ ê²Œì‹œê¸€ ===")
    return all_posts

def crawl_global_sites():
    """ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ë“¤ í¬ë¡¤ë§ (ë¯¸êµ¬í˜„)"""
    print("[DEBUG] ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ì€ ì•„ì§ êµ¬í˜„ë˜ì§€ ì•ŠìŒ")
    return []

def get_all_posts_for_report():
    """ì¼ì¼ ë¦¬í¬íŠ¸ìš© - ìƒˆ ê²Œì‹œê¸€ ìˆ˜ì§‘"""
    print("[INFO] ì¼ì¼ ë¦¬í¬íŠ¸ìš© ê²Œì‹œê¸€ ìˆ˜ì§‘ ì¤‘...")
    return crawl_korean_sites()

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
def test_korean_crawling():
    """í•œêµ­ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸"""
    print("=== í•œêµ­ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ===")

    print("\n1. ë£¨ë¦¬ì›¹ í…ŒìŠ¤íŠ¸:")
    ruliweb_posts = fetch_ruliweb_epic7_board()

    print("\n2. ìŠ¤í† ë¸Œ ë²„ê·¸ í…ŒìŠ¤íŠ¸:")
    stove_bug_posts = fetch_stove_bug_board()

    print("\n3. ìŠ¤í† ë¸Œ ììœ  í…ŒìŠ¤íŠ¸:")
    stove_general_posts = fetch_stove_general_board()

    print(f"\n=== í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===")
    print(f"ë£¨ë¦¬ì›¹: {len(ruliweb_posts)}ê°œ")
    print(f"ìŠ¤í† ë¸Œ ë²„ê·¸: {len(stove_bug_posts)}ê°œ")
    print(f"ìŠ¤í† ë¸Œ ììœ : {len(stove_general_posts)}ê°œ")
    print(f"ì´ í•©ê³„: {len(ruliweb_posts) + len(stove_bug_posts) + len(stove_general_posts)}ê°œ")

    # ë‚´ìš© ì¶”ì¶œ ê²°ê³¼ í™•ì¸
    print(f"\n=== ë‚´ìš© ì¶”ì¶œ ê²°ê³¼ ìƒ˜í”Œ ===")
    for i, post in enumerate((stove_bug_posts + stove_general_posts)[:3]):
        print(f"{i+1}. {post['title'][:50]}...")
        print(f"   ë‚´ìš©: {post['content'][:100]}...")
        print(f"   ì†ŒìŠ¤: {post['source']}")
        print()

    return ruliweb_posts + stove_bug_posts + stove_general_posts

def test_content_extraction():
    """ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ ë‹¨ë… í…ŒìŠ¤íŠ¸"""
    print("=== ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ===")

    # í…ŒìŠ¤íŠ¸ìš© ìŠ¤í† ë¸Œ ê²Œì‹œê¸€ URL
    test_urls = [
        "https://page.onstove.com/epicseven/kr/view/10650067",
        "https://page.onstove.com/epicseven/kr/view/10794251"
    ]

    for i, url in enumerate(test_urls, 1):
        print(f"\n{i}. í…ŒìŠ¤íŠ¸ URL: {url}")
        content = get_stove_post_content(url)
        print(f"   ì¶”ì¶œëœ ë‚´ìš©: {content}")
        print(f"   ê¸¸ì´: {len(content)}ì")

if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "test_content":
        test_content_extraction()
    else:
        test_korean_crawling()