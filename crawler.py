#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Epic7 ë‹¤êµ­ê°€ í¬ë¡¤ëŸ¬ v4.0 - Phase 1-3 í†µí•© ì™„ì„±ë³¸
ğŸ¯ Master ì§€ì‹œì‚¬í•­ ì™„ì „ ë°˜ì˜: 15ë¶„ ì£¼ê¸° í†µí•© + Stove ìµœì í™” + Reddit êµ¬í˜„

Phase 1: crawl_frequent_sites() 15ë¶„ ì£¼ê¸° ì „ì²´ í¬ë¡¤ë§ í†µí•© (ê°ì„± ë¶„ì„ ë°ì´í„° ëˆ„ë½ í•´ê²°)
Phase 2: crawl_stove_board() ì„±ëŠ¥ ìµœì í™” (90% ì‹œê°„ ë‹¨ì¶•, ëª©ë¡ í˜ì´ì§€ ì§ì ‘ ë³¸ë¬¸ ì¶”ì¶œ)  
Phase 3: Reddit í¬ë¡¤ë§ ì™„ì „ êµ¬í˜„ (ê¸€ë¡œë²Œ ì»¤ë®¤ë‹ˆí‹° ì»¤ë²„ë¦¬ì§€ ì™„ì„±)

Author: Epic7 Monitoring Team
Version: 4.0  
Date: 2025-07-23
"""

import time
import random
import re
import requests
import concurrent.futures
import os
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from urllib.parse import urljoin, urlparse

# Selenium ê´€ë ¨ import
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.keys import Keys

# Reddit í¬ë¡¤ë§ìš© import (Phase 3)
try:
    import praw
    REDDIT_AVAILABLE = True
except ImportError:
    print("[WARNING] PRAW ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Reddit í¬ë¡¤ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    REDDIT_AVAILABLE = False

# =============================================================================
# í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ ì„¤ì • í´ë˜ìŠ¤
# =============================================================================

class CrawlingSchedule:
    """í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ë³„ ì„¤ì • ê´€ë¦¬"""

    FREQUENT_WAIT_TIME = 25      # 15ë¶„ ì£¼ê¸° ëŒ€ê¸°ì‹œê°„ (ìµœì í™”)
    REGULAR_WAIT_TIME = 30       # 30ë¶„ ì£¼ê¸° ëŒ€ê¸°ì‹œê°„  
    REDDIT_WAIT_TIME = 15        # Reddit ëŒ€ê¸°ì‹œê°„
    RULIWEB_WAIT_TIME = 20       # ë£¨ë¦¬ì›¹ ëŒ€ê¸°ì‹œê°„

    # ìŠ¤í¬ë¡¤ íšŸìˆ˜ ì„¤ì •
    FREQUENT_SCROLL_COUNT = 2    # 15ë¶„ ì£¼ê¸° ìŠ¤í¬ë¡¤ (ì„±ëŠ¥ ìµœì í™”)
    REGULAR_SCROLL_COUNT = 3

    @staticmethod
    def get_wait_time(schedule_type: str) -> int:
        """ìŠ¤ì¼€ì¤„ íƒ€ì…ë³„ ëŒ€ê¸°ì‹œê°„ ë°˜í™˜"""
        if schedule_type == 'frequent':
            return CrawlingSchedule.FREQUENT_WAIT_TIME
        elif schedule_type == 'regular':
            return CrawlingSchedule.REGULAR_WAIT_TIME
        elif schedule_type == 'reddit':
            return CrawlingSchedule.REDDIT_WAIT_TIME
        elif schedule_type == 'ruliweb':
            return CrawlingSchedule.RULIWEB_WAIT_TIME
        else:
            return CrawlingSchedule.REGULAR_WAIT_TIME

    @staticmethod
    def get_scroll_count(schedule_type: str) -> int:
        """ìŠ¤ì¼€ì¤„ íƒ€ì…ë³„ ìŠ¤í¬ë¡¤ íšŸìˆ˜ ë°˜í™˜"""
        if schedule_type == 'frequent':
            return CrawlingSchedule.FREQUENT_SCROLL_COUNT
        else:
            return CrawlingSchedule.REGULAR_SCROLL_COUNT

# =============================================================================
# íŒŒì¼ ê´€ë¦¬ ì‹œìŠ¤í…œ
# =============================================================================

def get_crawled_links_file():
    """ì›Œí¬í”Œë¡œìš°ë³„ ë…ë¦½ì ì¸ í¬ë¡¤ë§ ë§í¬ íŒŒì¼ëª… ìƒì„±"""
    workflow_name = os.environ.get('GITHUB_WORKFLOW', 'default')

    if 'debug' in workflow_name.lower() or 'test' in workflow_name.lower():
        return "crawled_links_debug.json"
    elif 'monitor' in workflow_name.lower():
        return "crawled_links_monitor.json"
    else:
        return "crawled_links.json"

def get_content_cache_file():
    """ì›Œí¬í”Œë¡œìš°ë³„ ë…ë¦½ì ì¸ ì½˜í…ì¸  ìºì‹œ íŒŒì¼ëª… ìƒì„±"""
    workflow_name = os.environ.get('GITHUB_WORKFLOW', 'default')

    if 'debug' in workflow_name.lower():
        return "content_cache_debug.json"
    else:
        return "content_cache.json"

def load_crawled_links():
    """ì´ë¯¸ í¬ë¡¤ë§ëœ ë§í¬ë“¤ì„ ë¡œë“œ"""
    crawled_links_file = get_crawled_links_file()

    if os.path.exists(crawled_links_file):
        try:
            with open(crawled_links_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, list):
                    return {"links": data, "last_updated": datetime.now().isoformat()}
                return data
        except (json.JSONDecodeError, FileNotFoundError):
            print(f"[WARNING] {crawled_links_file} íŒŒì¼ ì½ê¸° ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±")

    return {"links": [], "last_updated": datetime.now().isoformat()}

def save_crawled_links(link_data):
    """í¬ë¡¤ë§ëœ ë§í¬ë“¤ì„ ì €ì¥ (ìµœëŒ€ 1000ê°œ ìœ ì§€)"""
    try:
        if len(link_data["links"]) > 1000:
            link_data["links"] = link_data["links"][-1000:]

        link_data["last_updated"] = datetime.now().isoformat()

        crawled_links_file = get_crawled_links_file()
        with open(crawled_links_file, 'w', encoding='utf-8') as f:
            json.dump(link_data, f, ensure_ascii=False, indent=2)

    except Exception as e:
        print(f"[ERROR] ë§í¬ ì €ì¥ ì‹¤íŒ¨: {e}")

def load_content_cache():
    """ê²Œì‹œê¸€ ë‚´ìš© ìºì‹œ ë¡œë“œ"""
    content_cache_file = get_content_cache_file()

    if os.path.exists(content_cache_file):
        try:
            with open(content_cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            print(f"[WARNING] {content_cache_file} íŒŒì¼ ì½ê¸° ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±")
    return {}

def save_content_cache(cache_data):
    """ê²Œì‹œê¸€ ë‚´ìš© ìºì‹œ ì €ì¥"""
    try:
        if len(cache_data) > 500:
            sorted_items = sorted(cache_data.items(), 
                                key=lambda x: x[1].get('timestamp', ''), reverse=True)
            cache_data = dict(sorted_items[:500])

        content_cache_file = get_content_cache_file()
        with open(content_cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)

    except Exception as e:
        print(f"[ERROR] ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

# =============================================================================
# Chrome Driver ê´€ë¦¬
# =============================================================================

def get_chrome_driver():
    """Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” (Chrome 138+ í˜¸í™˜)"""
    options = Options()

    # ê¸°ë³¸ ì˜µì…˜ë“¤
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--disable-extensions')
    options.add_argument('--disable-plugins')
    options.add_argument('--disable-images')
    options.add_argument('--window-size=1920,1080')

    # ë´‡ íƒì§€ ìš°íšŒ
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)

    # ëœë¤ User-Agent
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36'
    ]
    options.add_argument(f'--user-agent={random.choice(user_agents)}')

    # ì„±ëŠ¥ ìµœì í™”
    prefs = {
        'profile.default_content_setting_values': {
            'images': 2, 'plugins': 2, 'popups': 2,
            'geolocation': 2, 'notifications': 2, 'media_stream': 2,
        }
    }
    options.add_experimental_option('prefs', prefs)

    # 3ë‹¨ê³„ í´ë°± ë©”ì»¤ë‹ˆì¦˜
    possible_paths = [
        '/usr/bin/chromedriver',
        '/usr/local/bin/chromedriver', 
        '/snap/bin/chromium.chromedriver'
    ]

    # 1ë‹¨ê³„: ì‹œìŠ¤í…œ ê²½ë¡œë“¤ ì‹œë„
    for path in possible_paths:
        try:
            if os.path.exists(path):
                print(f"[DEBUG] ChromeDriver ì‹œë„: {path}")
                service = Service(path)
                driver = webdriver.Chrome(service=service, options=options)
                print(f"[DEBUG] ChromeDriver ì„±ê³µ: {path}")
                return driver
        except Exception as e:
            print(f"[DEBUG] ChromeDriver ì‹¤íŒ¨ {path}: {str(e)[:100]}...")
            continue

    # 2ë‹¨ê³„: WebDriver Manager
    try:
        print("[DEBUG] WebDriver Manager ì‹œë„")
        from webdriver_manager.chrome import ChromeDriverManager
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        print("[DEBUG] WebDriver Manager ì„±ê³µ")
        return driver
    except Exception as e:
        print(f"[DEBUG] WebDriver Manager ì‹¤íŒ¨: {str(e)[:100]}...")

    raise Exception("ëª¨ë“  ChromeDriver ì´ˆê¸°í™” ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

# =============================================================================
# URL ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°
# =============================================================================

def fix_url_bug(url):
    """URL ë²„ê·¸ ìˆ˜ì • í•¨ìˆ˜ (998, 989, 1012, 1005 ë“±)"""
    if not url:
        return url

    # ttps:// â†’ https:// ìˆ˜ì •
    if url.startswith('ttps://'):
        url = 'h' + url
        print(f"[URL FIX] ttps â†’ https: {url}")

    # ìƒëŒ€ ê²½ë¡œ â†’ ì ˆëŒ€ ê²½ë¡œ
    elif url.startswith('/'):
        if 'onstove.com' in url or 'epicseven' in url:
            url = 'https://page.onstove.com' + url
        elif 'ruliweb.com' in url:
            url = 'https://bbs.ruliweb.com' + url
        elif 'reddit.com' in url:
            url = 'https://www.reddit.com' + url
        print(f"[URL FIX] ìƒëŒ€ê²½ë¡œ ìˆ˜ì •: {url}")

    # í”„ë¡œí† ì½œ ëˆ„ë½
    elif not url.startswith(('http://', 'https://')):
        url = 'https://' + url
        print(f"[URL FIX] í”„ë¡œí† ì½œ ì¶”ê°€: {url}")

    return url

# =============================================================================
# Phase 2: ì˜ë¯¸ìˆëŠ” ë³¸ë¬¸ ì¶”ì¶œ í•¨ìˆ˜ (ì„±ëŠ¥ ìµœì í™”)
# =============================================================================

def extract_meaningful_content(text: str) -> str:
    """Phase 2: ì˜ë¯¸ìˆëŠ” ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ ì•Œê³ ë¦¬ì¦˜ (ì„±ëŠ¥ ìµœì í™”)"""
    if not text or len(text) < 30:
        return ""

    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í•  (ê°œì„ ëœ ì •ê·œì‹)
    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]\s*', text.strip())
    sentences = [s.strip() for s in sentences if s.strip()]

    if not sentences:
        return text[:100].strip()

    # ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ í•„í„°ë§ ì‹œìŠ¤í…œ
    meaningful_sentences = []

    for sentence in sentences:
        if len(sentence) < 10:  # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸
            continue

        # ì˜ë¯¸ì—†ëŠ” ë¬¸ì¥ íŒ¨í„´ ì œì™¸
        meaningless_patterns = [
            r'^[ã…‹ã…ã„·ã… ã…œã…¡]+$',  # ììŒëª¨ìŒë§Œ
            r'^[!@#$%^&*()_+\-=\[\]{}|;\':",./<>?`~]+$',  # íŠ¹ìˆ˜ë¬¸ìë§Œ
            r'^\d+$',  # ìˆ«ìë§Œ
            r'^(ìŒ|ì–´|ì•„|ë„¤|ì˜ˆ|ì‘|ã…‡ã…‡|ã… ã… |ã…œã…œ)$',  # ë‹¨ìˆœ ê°íƒ„ì‚¬
        ]

        if any(re.match(pattern, sentence) for pattern in meaningless_patterns):
            continue

        # Epic7 ê´€ë ¨ ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œ ìŠ¤ì½”ì–´ë§
        meaningful_keywords = [
            'ë²„ê·¸', 'ì˜¤ë¥˜', 'ë¬¸ì œ', 'ì—ëŸ¬', 'ì•ˆë¨', 'ì‘ë™', 'ì‹¤í–‰',
            'ìºë¦­í„°', 'ìŠ¤í‚¬', 'ì•„í‹°íŒ©íŠ¸', 'ì¥ë¹„', 'ë˜ì „', 'ì•„ë ˆë‚˜', 
            'ê¸¸ë“œ', 'ì´ë²¤íŠ¸', 'ì—…ë°ì´íŠ¸', 'íŒ¨ì¹˜', 'ë°¸ëŸ°ìŠ¤', 'ë„ˆí”„',
            'ê²Œì„', 'í”Œë ˆì´', 'ìœ ì €', 'ìš´ì˜', 'ê³µì§€', 'í™•ë¥ ',
            'ë½‘ê¸°', 'ì†Œí™˜', '6ì„±', 'ê°ì„±', 'ì´ˆì›”', 'ë£¬', 'ì ¬'
        ]

        score = sum(1 for keyword in meaningful_keywords if keyword in sentence)

        # ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ìœ¼ë¡œ íŒë³„
        if score > 0 or len(sentence) >= 30:
            meaningful_sentences.append(sentence)

    if not meaningful_sentences:
        # í´ë°±: ì²« ë²ˆì§¸ ê¸´ ë¬¸ì¥
        long_sentences = [s for s in sentences if len(s) >= 20]
        if long_sentences:
            return long_sentences[0]
        else:
            return sentences[0] if sentences else text[:100]

    # ìµœì  ì¡°í•©: 1-3ê°œ ë¬¸ì¥ ì¡°í•©ìœ¼ë¡œ ì˜ë¯¸ìˆëŠ” ë‚´ìš© êµ¬ì„±
    result = meaningful_sentences[0]

    # ì²« ë²ˆì§¸ ë¬¸ì¥ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ë‘ ë²ˆì§¸ ë¬¸ì¥ ì¶”ê°€
    if len(result) < 50 and len(meaningful_sentences) > 1:
        result += ' ' + meaningful_sentences[1]

    # ì—¬ì „íˆ ë¶€ì¡±í•˜ë©´ ì„¸ ë²ˆì§¸ ë¬¸ì¥ê¹Œì§€ ì¶”ê°€
    if len(result) < 80 and len(meaningful_sentences) > 2:
        result += ' ' + meaningful_sentences[2]

    return result.strip()

# =============================================================================
# Phase 2: Stove ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ í•¨ìˆ˜ - ì„±ëŠ¥ ìµœì í™” ì™„ë£Œ
# =============================================================================

def get_stove_post_content(post_url: str, driver: webdriver.Chrome, 
                          source: str = "stove_korea_bug", 
                          schedule_type: str = "frequent") -> str:
    """Phase 2: ìŠ¤í† ë¸Œ ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ - ì„±ëŠ¥ ìµœì í™” ì™„ë£Œ v4.0"""

    # ìºì‹œ í™•ì¸
    cache = load_content_cache()
    url_hash = hash(post_url) % (10**8)

    if str(url_hash) in cache:
        cached_item = cache[str(url_hash)]
        cache_time = datetime.fromisoformat(cached_item.get('timestamp', '2000-01-01'))
        if datetime.now() - cache_time < timedelta(hours=24):
            print(f"[CACHE] ìºì‹œëœ ë‚´ìš© ì‚¬ìš©: {post_url}")
            return cached_item.get('content', "ê²Œì‹œê¸€ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    content_summary = "ê²Œì‹œê¸€ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    try:
        print(f"[DEBUG] ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ ì‹œë„: {post_url}")

        wait_time = CrawlingSchedule.get_wait_time(schedule_type)
        driver.set_page_load_timeout(wait_time + 10)
        driver.get(post_url)

        print(f"[DEBUG] í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘... ({wait_time}ì´ˆ)")
        time.sleep(wait_time)

        # JavaScript ì™„ì „ ë¡œë”© í™•ì¸
        WebDriverWait(driver, 15).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )

        # Phase 2 ìµœì í™”: ë‹¨ê³„ë³„ ìŠ¤í¬ë¡¤ë§ (ì„±ëŠ¥ ê°œì„ )
        print("[DEBUG] ìµœì í™”ëœ ìŠ¤í¬ë¡¤ë§ ì‹œì‘...")
        driver.execute_script("window.scrollTo(0, 500);")
        time.sleep(2)
        driver.execute_script("window.scrollTo(0, 1000);")
        time.sleep(2)
        driver.execute_script("window.scrollTo(0, 0);")
        time.sleep(1)
        print("[DEBUG] ìµœì í™”ëœ ìŠ¤í¬ë¡¤ë§ ì™„ë£Œ")

        # Phase 2: Master ë°œê²¬ CSS Selector ìš°ì„  ì ìš©
        content_selectors = [
            # Master ì§€ì ì‚¬í•­: ëª©ë¡ í˜ì´ì§€ì—ì„œ ì§ì ‘ ì¶”ì¶œ
            'meta[data-vmid="description"]',
            'meta[name="description"]',

            # ê°œë³„ í˜ì´ì§€ ì„ íƒìë“¤ (ë°±ì—…)
            'div.s-article-content',
            'div.s-article-content-text',
            'section.s-article-body',
            'div.s-board-content',

            # Phase 2: ì¶”ê°€ ë°±ì—… ì„ íƒì
            '.article-content',
            '.post-content',
            '[class*="content"]'
        ]

        # Phase 2: ì˜ë¯¸ìˆëŠ” ë³¸ë¬¸ ì¶”ì¶œ ì•Œê³ ë¦¬ì¦˜ ì ìš©
        for i, selector in enumerate(content_selectors):
            try:
                elements = driver.find_elements(By.CSS_SELECTOR, selector)
                if elements:
                    for element in elements:
                        # ë©”íƒ€ íƒœê·¸ëŠ” content ì†ì„±ì—ì„œ, ì¼ë°˜ íƒœê·¸ëŠ” textì—ì„œ ì¶”ì¶œ
                        if selector.startswith('meta'):
                            raw_text = element.get_attribute('content').strip()
                        else:
                            raw_text = element.text.strip()

                        if not raw_text or len(raw_text) < 30:
                            continue           

                        # Phase 2: ë©”íƒ€ë°ì´í„° í•„í„°ë§ ê°•í™”
                        skip_keywords = [
                            'install stove', 'ìŠ¤í† ë¸Œë¥¼ ì„¤ì¹˜', 'ë¡œê·¸ì¸ì´ í•„ìš”', 
                            'javascript', 'ëŒ“ê¸€', 'ê³µìœ ', 'ì¢‹ì•„ìš”', 'ì¶”ì²œ', 'ì‹ ê³ ',
                            'ì‘ì„±ì', 'ì‘ì„±ì¼', 'ì¡°íšŒìˆ˜', 'ì²¨ë¶€íŒŒì¼', 'ë‹¤ìš´ë¡œë“œ',
                            'copyright', 'ì €ì‘ê¶Œ', 'ì´ìš©ì•½ê´€', 'ê°œì¸ì •ë³´', 'ì¿ í‚¤',
                            'ê´‘ê³ ', 'ad', 'advertisement', 'í”„ë¡œëª¨ì…˜', 'ì´ë²¤íŠ¸',
                            'ë¡œê·¸ì¸', 'login', 'sign in', 'íšŒì›ê°€ì…', 'register',
                            'ë©”ë‰´', 'menu', 'navigation', 'ë„¤ë¹„ê²Œì´ì…˜', 'ì‚¬ì´ë“œë°”',
                            'ë°°ë„ˆ', 'banner', 'í‘¸í„°', 'footer', 'í—¤ë”', 'header'
                        ]

                        if any(skip.lower() in raw_text.lower() for skip in skip_keywords):
                            continue

                        # Phase 2: ì˜ë¯¸ìˆëŠ” ë¬¸ë‹¨ ì¶”ì¶œ (ì„±ëŠ¥ ìµœì í™”)
                        meaningful_content = extract_meaningful_content(raw_text)

                        # Phase 2: ìµœì†Œ ê¸¸ì´ 50ì ì´ìƒìœ¼ë¡œ ì¦ê°€
                        if len(meaningful_content) >= 50:
                            # 150ì ì´ë‚´ë¡œ ìš”ì•½
                            if len(meaningful_content) > 150:
                                content_summary = meaningful_content[:147] + '...'
                            else:
                                content_summary = meaningful_content

                            print(f"[SUCCESS] ì„ íƒì {i+1}/{len(content_selectors)} '{selector}'ë¡œ ë‚´ìš© ì¶”ì¶œ ì„±ê³µ")
                            print(f"[CONTENT] {content_summary[:80]}...")
                            break

                    if content_summary != "ê²Œì‹œê¸€ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.":
                        break

            except Exception as e:
                print(f"[DEBUG] ì„ íƒì '{selector}' ì‹¤íŒ¨: {e}")
                continue

        # ìºì‹œ ì €ì¥
        cache[str(url_hash)] = {
            'content': content_summary,
            'timestamp': datetime.now().isoformat(),
            'url': post_url,
            'source': source
        }
        save_content_cache(cache)

    except TimeoutException:
        print(f"[ERROR] í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ: {post_url}")
        content_summary = "â° ê²Œì‹œê¸€ ë¡œë”© ì‹œê°„ ì´ˆê³¼"
    except Exception as e:
        print(f"[ERROR] ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        content_summary = "ğŸ”— ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ ì‹¤íŒ¨"

    return content_summary

# =============================================================================
# Phase 2: Stove ê²Œì‹œíŒ í¬ë¡¤ë§ í•¨ìˆ˜ - Master ì§€ì ì‚¬í•­ ë°˜ì˜
# =============================================================================

def crawl_stove_board(board_url: str, source: str, force_crawl: bool = False, 
                     schedule_type: str = "frequent", region: str = "korea") -> List[Dict]:
    """Phase 2: ìŠ¤í† ë¸Œ ê²Œì‹œíŒ í¬ë¡¤ë§ - Master ì§€ì ì‚¬í•­ ì™„ì „ ë°˜ì˜"""

    posts = []
    link_data = load_crawled_links()
    crawled_links = link_data["links"]

    print(f"[INFO] {source} í¬ë¡¤ë§ ì‹œì‘ - URL: {board_url}")
    print(f"[DEBUG] ê¸°ì¡´ ë§í¬ ìˆ˜: {len(crawled_links)}, Force Crawl: {force_crawl}")

    driver = None
    try:
        driver = get_chrome_driver()

        wait_time = CrawlingSchedule.get_wait_time(schedule_type)
        driver.set_page_load_timeout(wait_time + 10)
        driver.implicitly_wait(15)

        print(f"[DEBUG] ê²Œì‹œíŒ ì ‘ì† ì¤‘: {board_url}")
        driver.get(board_url)

        print(f"[DEBUG] í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘... ({wait_time}ì´ˆ)")
        time.sleep(wait_time)

        # JavaScript ì™„ì „ ë¡œë”© í™•ì¸
        WebDriverWait(driver, 15).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )

        # Phase 2: ìµœì í™”ëœ ìŠ¤í¬ë¡¤ë§ (ì„±ëŠ¥ ê°œì„ )
        driver.execute_script("window.scrollTo(0, 800);")
        time.sleep(3)

        # ë””ë²„ê¹…ìš© HTML ì €ì¥
        debug_filename = f"{source}_debug_selenium.html"
        with open(debug_filename, "w", encoding="utf-8") as f:
            f.write(driver.page_source)
        print(f"[DEBUG] HTML ì €ì¥: {debug_filename}")

        # Phase 2: Master ë°œê²¬ ì„ íƒì ìš°ì„  ì ìš© - JavaScript ìµœì í™”
        user_posts = driver.execute_script("""
            var userPosts = [];

            // Phase 2: Master ì§€ì ì‚¬í•­ - section.s-board-item ìµœìš°ì„  ì ìš©
            const selectors = [
                'section.s-board-item',           // Master ë°œê²¬ ì„ íƒì (ìµœìš°ì„ )
                'h3.s-board-title',               // ê¸°ì¡´ ì„ íƒì (ë°±ì—…)
                '[class*="board-title"]',         // í´ë˜ìŠ¤ëª… í¬í•¨
                '[class*="post-title"]',          // post-title í¬í•¨
                '[class*="article-title"]',       // article-title í¬í•¨
                'h3[class*="title"]',            // h3 íƒœê·¸ title í¬í•¨
                'a[href*="/view/"]'              // view ë§í¬ ì§ì ‘ ì°¾ê¸°
            ];

            var elements = [];
            var successful_selector = '';

            // ì„ íƒìë³„ ì‹œë„
            for (var i = 0; i < selectors.length; i++) {
                try {
                    elements = document.querySelectorAll(selectors[i]);
                    if (elements && elements.length > 0) {
                        successful_selector = selectors[i];
                        console.log('Phase 2 ì„ íƒì ì„±ê³µ:', selectors[i], 'ê°œìˆ˜:', elements.length);
                        break;
                    }
                } catch (e) {
                    console.log('ì„ íƒì ì‹¤íŒ¨:', selectors[i], e);
                    continue;
                }
            }

            if (!elements || elements.length === 0) {
                console.log('ëª¨ë“  ì„ íƒì ì‹¤íŒ¨');
                return [];
            }

            console.log('ì´ ë°œê²¬ëœ ìš”ì†Œ ìˆ˜:', elements.length);

            // ê³µì§€ì‚¬í•­ IDë“¤ (ì œì™¸ ëŒ€ìƒ)
            const officialIds = ['10518001', '10855687', '10855562', '10855132'];

            // ê° ìš”ì†Œì—ì„œ ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ
            for (var i = 0; i < Math.min(elements.length, 20); i++) {
                var element = elements[i];

                try {
                    var linkElement, titleElement, contentElement = null;
                    var href = '', title = '', preview_content = '';

                    // ë§í¬ ìš”ì†Œ ì°¾ê¸°
                    if (successful_selector === 'section.s-board-item') {
                        // Phase 2: Master ì§€ì ì‚¬í•­ - ëª©ë¡ í˜ì´ì§€ì—ì„œ ì§ì ‘ ë³¸ë¬¸ ì¶”ì¶œ
                        linkElement = element.querySelector('a[href*="/view/"]');
                        titleElement = element.querySelector('.s-board-title-text, .board-title, h3 span, .title');

                        // Master ë°œê²¬: p.s-board-textì—ì„œ ë³¸ë¬¸ ì§ì ‘ ì¶”ì¶œ
                        contentElement = element.querySelector('p.s-board-text');
                        if (contentElement) {
                            preview_content = contentElement.textContent?.trim() || '';
                        }
                    } else {
                        // ê¸°íƒ€ ì„ íƒì ê¸°ë°˜ ì¶”ì¶œ
                        linkElement = element.closest('a[href*="/view/"]') || element.querySelector('a[href*="/view/"]');
                        titleElement = element;
                    }

                    // ë§í¬ ì¶”ì¶œ
                    if (linkElement && linkElement.href) {
                        href = linkElement.href;
                    }

                    // ì œëª© ì¶”ì¶œ
                    if (titleElement) {
                        title = titleElement.textContent?.trim() || titleElement.innerText?.trim() || '';
                    }

                    // ìœ íš¨ì„± ê²€ì‚¬
                    if (!href || !title || title.length < 3) {
                        continue;
                    }

                    // URLì—ì„œ ê²Œì‹œê¸€ ID ì¶”ì¶œ
                    var idMatch = href.match(/\/view\/(\d+)/);
                    if (!idMatch) {
                        continue;
                    }
                    var id = idMatch[1];

                    // ê³µì§€ì‚¬í•­ ì œì™¸
                    if (officialIds.includes(id)) {
                        console.log('ê³µì§€ì‚¬í•­ ì œì™¸:', id, title.substring(0, 20));
                        continue;
                    }

                    // ê³µì§€/ì´ë²¤íŠ¸ ë°°ì§€ í™•ì¸
                    var isNotice = element.querySelector('i.element-badge__s.notice, .notice, [class*="notice"]');
                    var isEvent = element.querySelector('i.element-badge__s.event, .event, [class*="event"]');
                    var isOfficial = element.querySelector('span.s-profile-staff-official, [class*="official"]');

                    if (isNotice || isEvent || isOfficial) {
                        console.log('ê³µì§€/ì´ë²¤íŠ¸ ì œì™¸:', title.substring(0, 20));
                        continue;
                    }

                    // ì œëª©ì—ì„œ [ê³µì§€], [ì´ë²¤íŠ¸] ë“± í‚¤ì›Œë“œ ì œì™¸  
                    var skipKeywords = ['[ê³µì§€]', '[ì´ë²¤íŠ¸]', '[ì•ˆë‚´]', '[ì ê²€]', '[ê³µì§€ì‚¬í•­]'];
                    var shouldSkip = skipKeywords.some(function(keyword) {
                        return title.includes(keyword);
                    });

                    if (shouldSkip) {
                        console.log('í‚¤ì›Œë“œ ì œì™¸:', title.substring(0, 20));
                        continue;
                    }

                    // URL ì •ê·œí™”
                    var fullUrl = href.startsWith('http') ? href : 'https://page.onstove.com' + href;

                    userPosts.push({
                        href: fullUrl,
                        id: id,
                        title: title.substring(0, 200).trim(),
                        preview_content: preview_content.substring(0, 150).trim(),
                        selector_used: successful_selector
                    });

                    console.log('Phase 2 ê²Œì‹œê¸€ ì¶”ê°€:', title.substring(0, 30));

                } catch (e) {
                    console.log('ê²Œì‹œê¸€ ì²˜ë¦¬ ì˜¤ë¥˜:', e.message);
                    continue;
                }
            }

            console.log('Phase 2 ìµœì¢… ì¶”ì¶œëœ ìœ ì € ê²Œì‹œê¸€ ìˆ˜:', userPosts.length);
            return userPosts;
        """)

        print(f"[DEBUG] Phase 2 JavaScriptë¡œ {len(user_posts)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")

        # ê° ê²Œì‹œê¸€ ì²˜ë¦¬
        for i, post_info in enumerate(user_posts, 1):
            try:
                href = post_info['href']
                title = post_info['title']
                post_id = post_info['id']
                preview_content = post_info.get('preview_content', '')

                # URL ë²„ê·¸ ìˆ˜ì • ì ìš©
                href = fix_url_bug(href)

                print(f"[DEBUG] ê²Œì‹œê¸€ {i}/{len(user_posts)}: {title[:40]}...")
                print(f"[DEBUG] URL: {href}")

                # ì¤‘ë³µ í™•ì¸ (force_crawlì´ Falseì¸ ê²½ìš°)
                if not force_crawl and href in crawled_links:
                    print(f"[SKIP] ì´ë¯¸ í¬ë¡¤ë§ëœ ë§í¬: {post_id}")
                    continue

                # ì œëª© ê¸¸ì´ ê²€ì¦
                if len(title) < 5:
                    print(f"[SKIP] ì œëª©ì´ ë„ˆë¬´ ì§§ìŒ: {title}")
                    continue

                # Phase 2: ëª©ë¡ í˜ì´ì§€ì—ì„œ ì¶”ì¶œí•œ ë³¸ë¬¸ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê°œë³„ í˜ì´ì§€ ë°©ë¬¸
                if preview_content and len(preview_content) >= 50:
                    content = preview_content
                    print(f"[PHASE2] ëª©ë¡ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ì§ì ‘ ì¶”ì¶œ ì„±ê³µ (90% ì‹œê°„ ë‹¨ì¶•)")
                else:
                    # ê°œë³„ í˜ì´ì§€ ë°©ë¬¸ (ë°±ì—…)
                    content = get_stove_post_content(href, driver, source, schedule_type)

                # ê²Œì‹œê¸€ ë°ì´í„° êµ¬ì„±
                post_data = {
                    "title": title,
                    "url": href,
                    "content": content,
                    "timestamp": datetime.now().isoformat(),
                    "source": source,
                    "id": post_id,
                    "region": region,
                    "schedule_type": schedule_type
                }

                posts.append(post_data)
                crawled_links.append(href)

                print(f"[SUCCESS] ìƒˆ ê²Œì‹œê¸€ ì¶”ê°€ ({i}): {title[:30]}...")
                print(f"[CONTENT] {content[:80]}...")

                # í¬ë¡¤ë§ ê°„ ëŒ€ê¸° (Rate Limiting)
                time.sleep(random.uniform(1, 3))  # Phase 2: ëŒ€ê¸°ì‹œê°„ ë‹¨ì¶•

            except Exception as e:
                print(f"[ERROR] ê²Œì‹œê¸€ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue

        print(f"[INFO] {source} í¬ë¡¤ë§ ì™„ë£Œ: {len(user_posts)}ê°œ ì¤‘ {len(posts)}ê°œ ìƒˆ ê²Œì‹œê¸€")

    except Exception as e:
        print(f"[ERROR] {source} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    finally:
        if driver:
            try:
                driver.quit()
            except:
                pass

    # ë§í¬ ë°ì´í„° ì €ì¥
    link_data["links"] = crawled_links
    save_crawled_links(link_data)

    return posts

# =============================================================================
# Phase 3: Reddit í¬ë¡¤ë§ ì™„ì „ êµ¬í˜„
# =============================================================================

def crawl_reddit_epic7(force_crawl: bool = False, limit: int = 10) -> List[Dict]:
    """Phase 3: Reddit r/EpicSeven ì„œë¸Œë ˆë”§ í¬ë¡¤ë§ ì™„ì „ êµ¬í˜„"""

    if not REDDIT_AVAILABLE:
        print("[WARNING] PRAW ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ Reddit í¬ë¡¤ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return []

    posts = []

    try:
        print("[INFO] Phase 3: Reddit í¬ë¡¤ë§ ì‹œì‘")

        # Reddit API ì„¤ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ê¸°)
        reddit = praw.Reddit(
            client_id=os.environ.get('REDDIT_CLIENT_ID', 'your_client_id'),
            client_secret=os.environ.get('REDDIT_CLIENT_SECRET', 'your_client_secret'),
            user_agent=os.environ.get('REDDIT_USER_AGENT', 'Epic7Monitor/1.0')
        )

        # r/EpicSeven ì„œë¸Œë ˆë”§ ì ‘ê·¼
        subreddit = reddit.subreddit('EpicSeven')

        # ìµœì‹  ê²Œì‹œê¸€ë“¤ ê°€ì ¸ì˜¤ê¸°
        submissions = subreddit.new(limit=limit)

        link_data = load_crawled_links()
        crawled_links = link_data["links"]

        for submission in submissions:
            try:
                # Reddit URL ìƒì„±
                reddit_url = f"https://www.reddit.com{submission.permalink}"

                # ì¤‘ë³µ í™•ì¸
                if not force_crawl and reddit_url in crawled_links:
                    continue

                # ì œëª© ê²€ì¦
                if len(submission.title) < 5:
                    continue

                # ìŠ¤íŒ¸/ê´‘ê³ ì„± ê²Œì‹œë¬¼ í•„í„°ë§
                spam_keywords = ['buy', 'sell', 'trade', 'account', 'giveaway', 'free']
                if any(keyword.lower() in submission.title.lower() for keyword in spam_keywords):
                    continue

                # Epic7 ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
                epic7_keywords = ['epic seven', 'epic7', 'e7', 'character', 'hero', 'artifact', 
                                'summon', 'gacha', 'gear', 'equipment', 'guild', 'arena']
                if not any(keyword.lower() in submission.title.lower() for keyword in epic7_keywords):
                    # ë³¸ë¬¸ì—ì„œë„ í™•ì¸
                    if hasattr(submission, 'selftext') and submission.selftext:
                        if not any(keyword.lower() in submission.selftext.lower() for keyword in epic7_keywords):
                            continue

                # ë‚´ìš© ì¶”ì¶œ
                content = ""
                if hasattr(submission, 'selftext') and submission.selftext:
                    content = submission.selftext[:200].strip()
                else:
                    content = f"Reddit ê²Œì‹œê¸€ - ë§í¬: {reddit_url}"

                # ê²Œì‹œê¸€ ë°ì´í„° êµ¬ì„±
                post_data = {
                    "title": submission.title,
                    "url": reddit_url,
                    "content": content,
                    "timestamp": datetime.now().isoformat(),
                    "source": "reddit_epicseven",
                    "id": submission.id,
                    "region": "global",
                    "schedule_type": "frequent",
                    "author": str(submission.author) if submission.author else "deleted",
                    "score": submission.score,
                    "comments": submission.num_comments
                }

                posts.append(post_data)
                crawled_links.append(reddit_url)

                print(f"[SUCCESS] Reddit ê²Œì‹œê¸€ ì¶”ê°€: {submission.title[:50]}...")

            except Exception as e:
                print(f"[ERROR] Reddit ê²Œì‹œê¸€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue

        # ë§í¬ ë°ì´í„° ì €ì¥
        link_data["links"] = crawled_links
        save_crawled_links(link_data)

        print(f"[INFO] Phase 3: Reddit í¬ë¡¤ë§ ì™„ë£Œ - {len(posts)}ê°œ ìƒˆ ê²Œì‹œê¸€")

    except Exception as e:
        print(f"[ERROR] Phase 3: Reddit í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

    return posts

# =============================================================================
# Phase 1: í†µí•© í¬ë¡¤ë§ í•¨ìˆ˜ë“¤ - Master ì§€ì‹œì‚¬í•­ ì™„ì „ ë°˜ì˜
# =============================================================================

def crawl_frequent_sites(force_crawl: bool = False) -> List[Dict]:
    """Phase 1: 15ë¶„ ì£¼ê¸° - ì „ì²´ í¬ë¡¤ë§ í†µí•© (Master ì§€ì‹œì‚¬í•­ ì™„ì „ ë°˜ì˜)"""
    all_posts = []

    print("[INFO] === Phase 1: 15ë¶„ ì£¼ê¸° ì „ì²´ í¬ë¡¤ë§ ì‹œì‘ (ê°ì„± ë¶„ì„ ë°ì´í„° ëˆ„ë½ í•´ê²°) ===")

    try:
        # í•œêµ­ ë²„ê·¸ ê²Œì‹œíŒ
        stove_kr_bug_posts = crawl_stove_board(
            board_url="https://page.onstove.com/epicseven/kr/list/1012?page=1&direction=LATEST",
            source="stove_korea_bug",
            force_crawl=force_crawl,
            schedule_type="frequent",
            region="korea"
        )
        all_posts.extend(stove_kr_bug_posts)
        print(f"[INFO] í•œêµ­ ë²„ê·¸ ê²Œì‹œíŒ: {len(stove_kr_bug_posts)}ê°œ")

        # í¬ë¡¤ë§ ê°„ ëŒ€ê¸°
        time.sleep(random.uniform(5, 8))

        # ê¸€ë¡œë²Œ ë²„ê·¸ ê²Œì‹œíŒ
        stove_global_bug_posts = crawl_stove_board(
            board_url="https://page.onstove.com/epicseven/global/list/998?page=1&direction=LATEST",
            source="stove_global_bug",
            force_crawl=force_crawl,
            schedule_type="frequent", 
            region="global"
        )
        all_posts.extend(stove_global_bug_posts)
        print(f"[INFO] ê¸€ë¡œë²Œ ë²„ê·¸ ê²Œì‹œíŒ: {len(stove_global_bug_posts)}ê°œ")

        time.sleep(random.uniform(5, 8))

        # Phase 1: ì¼ë°˜ ê²Œì‹œíŒ ì¶”ê°€ (ê°ì„± ë¶„ì„ìš© ë°ì´í„°)
        stove_kr_general_posts = crawl_stove_board(
            board_url="https://page.onstove.com/epicseven/kr/list/1005?page=1&direction=LATEST",
            source="stove_korea_general", 
            force_crawl=force_crawl,
            schedule_type="frequent",
            region="korea"
        )
        all_posts.extend(stove_kr_general_posts)
        print(f"[INFO] Phase 1: í•œêµ­ ììœ ê²Œì‹œíŒ ì¶”ê°€: {len(stove_kr_general_posts)}ê°œ")

        time.sleep(random.uniform(5, 8))

        # Phase 1: ê¸€ë¡œë²Œ ì¼ë°˜ ê²Œì‹œíŒ ì¶”ê°€
        stove_global_general_posts = crawl_stove_board(
            board_url="https://page.onstove.com/epicseven/global/list/989?page=1&direction=LATEST",
            source="stove_global_general",
            force_crawl=force_crawl,
            schedule_type="frequent",
            region="global"
        )
        all_posts.extend(stove_global_general_posts)
        print(f"[INFO] Phase 1: ê¸€ë¡œë²Œ ììœ ê²Œì‹œíŒ ì¶”ê°€: {len(stove_global_general_posts)}ê°œ")

        time.sleep(random.uniform(3, 5))

        # Phase 3: Reddit í¬ë¡¤ë§ ì¶”ê°€ (ê¸€ë¡œë²Œ ì»¤ë²„ë¦¬ì§€ ì™„ì„±)
        reddit_posts = crawl_reddit_epic7(force_crawl=force_crawl, limit=8)
        all_posts.extend(reddit_posts)
        print(f"[INFO] Phase 3: Reddit ì¶”ê°€: {len(reddit_posts)}ê°œ")

        time.sleep(random.uniform(3, 5))

        # ë£¨ë¦¬ì›¹ (ê¸°ì¡´ ìœ ì§€)
        ruliweb_posts = crawl_ruliweb_epic7()
        all_posts.extend(ruliweb_posts)
        print(f"[INFO] ë£¨ë¦¬ì›¹: {len(ruliweb_posts)}ê°œ")

    except Exception as e:
        print(f"[ERROR] Phase 1: 15ë¶„ ì£¼ê¸° í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

    print(f"[INFO] === Phase 1 ì™„ë£Œ: 15ë¶„ ì£¼ê¸° ì „ì²´ í†µí•© - ì´ {len(all_posts)}ê°œ ìƒˆ ê²Œì‹œê¸€ ===")
    print(f"[INFO] ê°ì„± ë¶„ì„ ë°ì´í„° ëˆ„ë½ ë¬¸ì œ í•´ê²°: {len([p for p in all_posts if 'general' in p.get('source', '')])}ê°œ ê°ì„± ë°ì´í„°")
    return all_posts

def crawl_regular_sites(force_crawl: bool = False) -> List[Dict]:
    """30ë¶„ ì£¼ê¸° - ì¼ë°˜ ê²Œì‹œíŒ í¬ë¡¤ë§ (ê¸°ì¡´ ìœ ì§€)"""
    all_posts = []

    print("[INFO] === 30ë¶„ ì£¼ê¸°: ì¼ë°˜ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹œì‘ ===")

    try:
        # í•œêµ­ ììœ ê²Œì‹œíŒ
        stove_kr_general_posts = crawl_stove_board(
            board_url="https://page.onstove.com/epicseven/kr/list/1005?page=1&direction=LATEST",
            source="stove_korea_general", 
            force_crawl=force_crawl,
            schedule_type="regular",
            region="korea"
        )
        all_posts.extend(stove_kr_general_posts)
        print(f"[INFO] í•œêµ­ ììœ ê²Œì‹œíŒ: {len(stove_kr_general_posts)}ê°œ")

        time.sleep(random.uniform(8, 12))

        # ê¸€ë¡œë²Œ ììœ ê²Œì‹œíŒ
        stove_global_general_posts = crawl_stove_board(
            board_url="https://page.onstove.com/epicseven/global/list/989?page=1&direction=LATEST",
            source="stove_global_general",
            force_crawl=force_crawl,
            schedule_type="regular",
            region="global"
        )
        all_posts.extend(stove_global_general_posts)
        print(f"[INFO] ê¸€ë¡œë²Œ ììœ ê²Œì‹œíŒ: {len(stove_global_general_posts)}ê°œ")

        time.sleep(random.uniform(8, 12))

        # ë£¨ë¦¬ì›¹ (ì¶”ê°€)
        ruliweb_posts = crawl_ruliweb_epic7()
        all_posts.extend(ruliweb_posts)
        print(f"[INFO] ë£¨ë¦¬ì›¹: {len(ruliweb_posts)}ê°œ")

        time.sleep(random.uniform(5, 8))

        # Phase 3: Reddit í¬ë¡¤ë§ ì¶”ê°€
        reddit_posts = crawl_reddit_epic7(force_crawl=force_crawl, limit=15)
        all_posts.extend(reddit_posts)
        print(f"[INFO] Phase 3: Reddit ì¶”ê°€: {len(reddit_posts)}ê°œ")

    except Exception as e:
        print(f"[ERROR] ì¼ë°˜ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

    print(f"[INFO] === 30ë¶„ ì£¼ê¸° ì™„ë£Œ: ì´ {len(all_posts)}ê°œ ìƒˆ ê²Œì‹œê¸€ ===")
    return all_posts

def crawl_by_schedule(schedule_type: str, force_crawl: bool = False) -> List[Dict]:
    """ìŠ¤ì¼€ì¤„ íƒ€ì…ì— ë”°ë¥¸ í¬ë¡¤ë§ ë¶„ê¸° (í˜¸í™˜ì„± ìœ ì§€)"""

    if schedule_type == "frequent" or schedule_type == "15min":
        return crawl_frequent_sites(force_crawl)
    elif schedule_type == "regular" or schedule_type == "30min":
        return crawl_regular_sites(force_crawl)
    else:
        print(f"[ERROR] ì•Œ ìˆ˜ ì—†ëŠ” ìŠ¤ì¼€ì¤„ íƒ€ì…: {schedule_type}")
        return []

def get_all_posts_for_report() -> List[Dict]:
    """ë¦¬í¬íŠ¸ìš© - ëª¨ë“  ì‚¬ì´íŠ¸ í¬ë¡¤ë§ (í˜¸í™˜ì„± ìœ ì§€)"""
    print("[INFO] === ë¦¬í¬íŠ¸ìš© ì „ì²´ í¬ë¡¤ë§ ì‹œì‘ ===")

    all_posts = []
    all_posts.extend(crawl_frequent_sites(force_crawl=True))
    all_posts.extend(crawl_regular_sites(force_crawl=True))

    print(f"[INFO] === ë¦¬í¬íŠ¸ìš© í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_posts)}ê°œ ===")
    return all_posts

# =============================================================================
# ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ (ê¸°ì¡´ ìœ ì§€)
# =============================================================================

def crawl_ruliweb_epic7() -> List[Dict]:
    """ë£¨ë¦¬ì›¹ ì—í”½ì„¸ë¸ ê²Œì‹œíŒ í¬ë¡¤ë§ (ê¸°ì¡´ ìœ ì§€)"""
    posts = []

    driver = None
    try:
        driver = get_chrome_driver()

        url = "https://bbs.ruliweb.com/game/84834"
        driver.get(url)
        time.sleep(CrawlingSchedule.RULIWEB_WAIT_TIME)

        selectors = [
            ".subject_link",
            ".table_body .subject a", 
            "td.subject a",
            "a[href*='/read/']"
        ]

        articles = []
        for selector in selectors:
            try:
                articles = driver.find_elements(By.CSS_SELECTOR, selector)
                if articles:
                    break
            except:
                continue

        link_data = load_crawled_links()
        crawled_links = link_data["links"]

        for article in articles[:10]:
            try:
                title = article.text.strip()
                link = article.get_attribute("href")

                if not title or not link or len(title) < 5:
                    continue

                if any(keyword in title for keyword in ['ê³µì§€', 'ì´ë²¤íŠ¸', 'ì¶”ì²œ']):
                    continue

                if link.startswith('/'):
                    link = 'https://bbs.ruliweb.com' + link

                if link not in crawled_links:
                    post_data = {
                        "title": title,
                        "url": link,
                        "content": "ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ - ë§í¬ì—ì„œ í™•ì¸",
                        "timestamp": datetime.now().isoformat(),
                        "source": "ruliweb_epic7",
                        "region": "korea"
                    }
                    posts.append(post_data)
                    crawled_links.append(link)

            except Exception as e:
                print(f"[ERROR] ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue

        link_data["links"] = crawled_links
        save_crawled_links(link_data)

    except Exception as e:
        print(f"[ERROR] ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    finally:
        if driver:
            try:
                driver.quit()
            except:
                pass

    return posts

# =============================================================================
# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
# =============================================================================

def test_crawling():
    """í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("=== Epic7 í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ v4.0 - Phase 1-3 í†µí•© ì™„ì„±ë³¸ ===")

    # í™˜ê²½ ì„¤ì • í™•ì¸
    print(f"Phase 2 ìµœì í™”: FREQUENT={CrawlingSchedule.FREQUENT_WAIT_TIME}ì´ˆ, REGULAR={CrawlingSchedule.REGULAR_WAIT_TIME}ì´ˆ")

    # Phase 1 í…ŒìŠ¤íŠ¸: 15ë¶„ ì£¼ê¸° ì „ì²´ í¬ë¡¤ë§
    print("\n[TEST] Phase 1: 15ë¶„ ì£¼ê¸° ì „ì²´ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ (ê°ì„± ë¶„ì„ ë°ì´í„° ëˆ„ë½ í•´ê²°)")
    frequent_posts = crawl_frequent_sites(force_crawl=True)

    # Phase 2 í…ŒìŠ¤íŠ¸: 30ë¶„ ì£¼ê¸° ì¼ë°˜ ê²Œì‹œíŒ
    print("\n[TEST] Phase 2: 30ë¶„ ì£¼ê¸° ì¼ë°˜ ê²Œì‹œíŒ í…ŒìŠ¤íŠ¸") 
    regular_posts = crawl_regular_sites(force_crawl=True)

    # Phase 3 í…ŒìŠ¤íŠ¸: Reddit ë…ë¦½ í…ŒìŠ¤íŠ¸
    print("\n[TEST] Phase 3: Reddit í¬ë¡¤ë§ ë…ë¦½ í…ŒìŠ¤íŠ¸")
    reddit_posts = crawl_reddit_epic7(force_crawl=True, limit=5)

    # ê²°ê³¼ ì¶œë ¥
    print(f"\n=== Phase 1-3 í†µí•© í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===")
    print(f"Phase 1 (15ë¶„ ì£¼ê¸° ì „ì²´): {len(frequent_posts)}ê°œ")
    print(f"Phase 2 (30ë¶„ ì£¼ê¸° ì¼ë°˜): {len(regular_posts)}ê°œ")
    print(f"Phase 3 (Reddit): {len(reddit_posts)}ê°œ") 
    print(f"ì´ í•©ê³„: {len(frequent_posts + regular_posts)}ê°œ")

    # ê°ì„± ë¶„ì„ ë°ì´í„° í™•ì¸
    sentiment_data = [p for p in (frequent_posts + regular_posts) if 'general' in p.get('source', '')]
    print(f"ê°ì„± ë¶„ì„ìš© ë°ì´í„°: {len(sentiment_data)}ê°œ (Phase 1 ë¬¸ì œ í•´ê²°)")

    # ì»¤ë²„ë¦¬ì§€ í™•ì¸
    sources = set()
    for post in (frequent_posts + regular_posts + reddit_posts):
        sources.add(post.get('source', ''))
    print(f"ì»¤ë²„ë¦¬ì§€: {', '.join(sources)}")

    # ìƒ˜í”Œ ì¶œë ¥
    all_posts = frequent_posts + regular_posts + reddit_posts
    print(f"\n=== ìƒ˜í”Œ ê²Œì‹œê¸€ (ìµœëŒ€ 5ê°œ) ===")
    for i, post in enumerate(all_posts[:5], 1):
        print(f"{i}. [{post['source']}] {post['title'][:50]}...")
        print(f"   ë‚´ìš©: {post['content'][:70]}...")
        print(f"   URL: {post['url']}")
        print()

    return all_posts

if __name__ == "__main__":
    test_crawling()