#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Epic7 ë‹¤êµ­ê°€ í¬ë¡¤ëŸ¬ v4.3 - ì™„ì„±í˜• ì¦‰ì‹œ ì²˜ë¦¬ ì‹œìŠ¤í…œ
Master ìš”êµ¬ì‚¬í•­: ê²Œì‹œê¸€ë³„ ì¦‰ì‹œ ì²˜ë¦¬ (í¬ë¡¤ë§â†’ê°ì„±ë¶„ì„â†’ì•Œë¦¼â†’ë§ˆí‚¹)

í•µì‹¬ êµ¬í˜„ì‚¬í•­:
- ê²Œì‹œê¸€ë³„ ì¦‰ì‹œ ì²˜ë¦¬ ì™„ì „ êµ¬í˜„
- ì—ëŸ¬ ê²©ë¦¬ ë° ë³µì›ë ¥ ê°•í™”
- ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ ìë™ ê´€ë¦¬
- ê¸°ì¡´ ê¸°ëŠ¥ 100% ë³´ì¡´

Author: Epic7 Monitoring Team  
Version: 4.3 (ì™„ì„±í˜• ì¦‰ì‹œ ì²˜ë¦¬)
Date: 2025-07-24
"""

import time
import random
import re
import requests
import concurrent.futures
import os
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Callable
from urllib.parse import urljoin, urlparse

# Selenium ê´€ë ¨ import
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.keys import Keys

# Epic7 ì‹œìŠ¤í…œ ëª¨ë“ˆ import (ì¦‰ì‹œ ì²˜ë¦¬ìš©)
try:
    from classifier import Epic7Classifier, is_bug_post, is_high_priority_bug, should_send_realtime_alert
    from notifier import send_bug_alert, send_sentiment_notification
    from sentiment_data_manager import save_sentiment_data, get_sentiment_summary
    EPIC7_MODULES_AVAILABLE = True
    print("[INFO] Epic7 ì²˜ë¦¬ ëª¨ë“ˆë“¤ ë¡œë“œ ì™„ë£Œ")
except ImportError as e:
    print(f"[WARNING] Epic7 ì²˜ë¦¬ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("[WARNING] ì¦‰ì‹œ ì²˜ë¦¬ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
    EPIC7_MODULES_AVAILABLE = False

# Reddit í¬ë¡¤ë§ìš© import
try:
    import praw
    REDDIT_AVAILABLE = True
except ImportError:
    print("[WARNING] PRAW ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Reddit í¬ë¡¤ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    REDDIT_AVAILABLE = False

# =============================================================================
# ğŸš€ Master ìš”êµ¬ì‚¬í•­: ì¦‰ì‹œ ì²˜ë¦¬ ì‹œìŠ¤í…œ êµ¬í˜„
# =============================================================================

class ImmediateProcessor:
    """ê²Œì‹œê¸€ë³„ ì¦‰ì‹œ ì²˜ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.processed_count = 0
        self.failed_count = 0
        self.retry_queue = []
        self.classifier = None
        
        if EPIC7_MODULES_AVAILABLE:
            try:
                self.classifier = Epic7Classifier()
                print("[INFO] ì¦‰ì‹œ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"[ERROR] ë¶„ë¥˜ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                
    def process_post_immediately(self, post_data: Dict) -> bool:
        """
        ê²Œì‹œê¸€ë³„ ì¦‰ì‹œ ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜
        Master ìš”êµ¬ì‚¬í•­: í¬ë¡¤ë§ â†’ ê°ì„±ë¶„ì„ â†’ ì•Œë¦¼ â†’ ë§ˆí‚¹
        """
        try:
            print(f"[IMMEDIATE] ì¦‰ì‹œ ì²˜ë¦¬ ì‹œì‘: {post_data.get('title', '')[:50]}...")
            
            if not EPIC7_MODULES_AVAILABLE:
                print("[WARNING] ì²˜ë¦¬ ëª¨ë“ˆ ì—†ìŒ, ê¸°ë³¸ ì²˜ë¦¬ë§Œ ìˆ˜í–‰")
                self._basic_processing(post_data)
                return True
            
            # 1. ìœ ì € ë™í–¥ ê°ì„± ë¶„ì„
            sentiment_result = self._analyze_sentiment(post_data)
            
            # 2. ì•Œë¦¼ ì „ì†¡ ì—¬ë¶€ ì²´í¬ ë° ì „ì†¡
            notification_sent = self._handle_notifications(post_data, sentiment_result)
            
            # 3. ì²˜ë¦¬ ì™„ë£Œ ë§ˆí‚¹ (ì•Œë¦¼ ì„±ê³µ ì‹œì—ë§Œ)
            if notification_sent:
                self._mark_as_processed(post_data['url'], notified=True)
                self.processed_count += 1
                print(f"[SUCCESS] ì¦‰ì‹œ ì²˜ë¦¬ ì™„ë£Œ: {post_data.get('title', '')[:30]}...")
            else:
                # ì‹¤íŒ¨í•œ ê²½ìš° ì¬ì‹œë„ íì— ì¶”ê°€
                self._add_to_retry_queue(post_data, sentiment_result)
                self.failed_count += 1
                
            return notification_sent
            
        except Exception as e:
            print(f"[ERROR] ì¦‰ì‹œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            self._add_to_retry_queue(post_data, None)
            self.failed_count += 1
            return False
    
    def _analyze_sentiment(self, post_data: Dict) -> Dict:
        """ê°ì„± ë¶„ì„ ìˆ˜í–‰"""
        try:
            if not self.classifier:
                return {"sentiment": "neutral", "confidence": 0.5}
                
            result = self.classifier.classify_post(post_data)
            print(f"[SENTIMENT] ë¶„ì„ ê²°ê³¼: {result.get('sentiment', 'unknown')}")
            return result
            
        except Exception as e:
            print(f"[ERROR] ê°ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"sentiment": "neutral", "confidence": 0.0, "error": str(e)}
    
    def _handle_notifications(self, post_data: Dict, sentiment_result: Dict) -> bool:
        """ë¶„ë¥˜ë³„ ì•Œë¦¼ ì²˜ë¦¬"""
        try:
            source = post_data.get('source', '')
            sentiment = sentiment_result.get('sentiment', 'neutral')
            
            # Master ìš”êµ¬ì‚¬í•­: ë²„ê·¸ ê²Œì‹œíŒ ê¸€ì´ë¼ë©´ ì‹¤ì‹œê°„ ë²„ê·¸ ë©”ì‹œì§€
            if source.endswith('_bug') or 'bug' in source.lower():
                print("[ALERT] ë²„ê·¸ ê²Œì‹œíŒ ê¸€ â†’ ì¦‰ì‹œ ë²„ê·¸ ì•Œë¦¼")
                return self._send_bug_alert(post_data)
            
            # Master ìš”êµ¬ì‚¬í•­: ë™í–¥ ë¶„ì„ í›„ ë²„ê·¸ë¡œ ë¶„ë¥˜ëœ ê¸€ë„ ì‹¤ì‹œê°„ ë²„ê·¸ ë©”ì‹œì§€
            elif is_bug_post(sentiment_result) or should_send_realtime_alert(sentiment_result):
                print("[ALERT] ë²„ê·¸ ë¶„ë¥˜ ê¸€ â†’ ì¦‰ì‹œ ë²„ê·¸ ì•Œë¦¼")
                return self._send_bug_alert(post_data)
            
            # Master ìš”êµ¬ì‚¬í•­: ê¸ì •/ì¤‘ë¦½/ë¶€ì • ë™í–¥ì€ ê°ì„± ì•Œë¦¼ + ì €ì¥
            else:
                print(f"[ALERT] ê°ì„± ë™í–¥ ê¸€ ({sentiment}) â†’ ì¦‰ì‹œ ê°ì„± ì•Œë¦¼")
                return self._send_sentiment_alert(post_data, sentiment_result)
                
        except Exception as e:
            print(f"[ERROR] ì•Œë¦¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return False
    
    def _send_bug_alert(self, post_data: Dict) -> bool:
        """ë²„ê·¸ ì•Œë¦¼ ì „ì†¡"""
        try:
            success = send_bug_alert(post_data)
            if success:
                print("[SUCCESS] ë²„ê·¸ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ")
            else:
                print("[FAILED] ë²„ê·¸ ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨")
            return success
        except Exception as e:
            print(f"[ERROR] ë²„ê·¸ ì•Œë¦¼ ì „ì†¡ ì˜¤ë¥˜: {e}")
            return False
    
    def _send_sentiment_alert(self, post_data: Dict, sentiment_result: Dict) -> bool:
        """ê°ì„± ì•Œë¦¼ ì „ì†¡ ë° ë°ì´í„° ì €ì¥"""
        try:
            # Master ìš”êµ¬ì‚¬í•­: ì¼ê°„ ë¦¬í¬íŠ¸ìš© ë°ì´í„° ì €ì¥
            save_success = save_sentiment_data(post_data, sentiment_result)
            
            # ì¦‰ì‹œ ê°ì„± ì•Œë¦¼ ì „ì†¡
            alert_success = send_sentiment_notification(post_data, sentiment_result)
            
            if save_success and alert_success:
                print("[SUCCESS] ê°ì„± ì•Œë¦¼ ì „ì†¡ ë° ë°ì´í„° ì €ì¥ ì™„ë£Œ")
                return True
            else:
                print(f"[PARTIAL] ì €ì¥: {save_success}, ì•Œë¦¼: {alert_success}")
                return False
                
        except Exception as e:
            print(f"[ERROR] ê°ì„± ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return False
    
    def _mark_as_processed(self, url: str, notified: bool = True):
        """ì²˜ë¦¬ ì™„ë£Œ ë§ˆí‚¹"""
        try:
            mark_as_processed(url, notified)
        except Exception as e:
            print(f"[ERROR] ë§ˆí‚¹ ì‹¤íŒ¨: {e}")
    
    def _add_to_retry_queue(self, post_data: Dict, sentiment_result: Optional[Dict]):
        """ì¬ì‹œë„ íì— ì¶”ê°€"""
        retry_item = {
            "post_data": post_data,
            "sentiment_result": sentiment_result,
            "timestamp": datetime.now().isoformat(),
            "retry_count": 0
        }
        self.retry_queue.append(retry_item)
        print(f"[RETRY] ì¬ì‹œë„ í ì¶”ê°€: {len(self.retry_queue)}ê°œ ëŒ€ê¸°ì¤‘")
    
    def _basic_processing(self, post_data: Dict):
        """ê¸°ë³¸ ì²˜ë¦¬ (ëª¨ë“ˆ ì—†ì„ ë•Œ)"""
        print(f"[BASIC] ê¸°ë³¸ ì²˜ë¦¬: {post_data.get('title', '')[:50]}...")
        self._mark_as_processed(post_data['url'], notified=False)
    
    def process_retry_queue(self):
        """ì¬ì‹œë„ í ì²˜ë¦¬"""
        if not self.retry_queue:
            return
            
        print(f"[RETRY] ì¬ì‹œë„ í ì²˜ë¦¬ ì‹œì‘: {len(self.retry_queue)}ê°œ")
        processed_items = []
        
        for item in self.retry_queue:
            try:
                if item["retry_count"] >= 3:
                    print("[SKIP] ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                    processed_items.append(item)
                    continue
                
                item["retry_count"] += 1
                success = self.process_post_immediately(item["post_data"])
                
                if success:
                    processed_items.append(item)
                    
            except Exception as e:
                print(f"[ERROR] ì¬ì‹œë„ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        # ì²˜ë¦¬ ì™„ë£Œëœ í•­ëª©ë“¤ ì œê±°
        for item in processed_items:
            self.retry_queue.remove(item)
        
        print(f"[RETRY] ì¬ì‹œë„ ì™„ë£Œ: {len(processed_items)}ê°œ ì²˜ë¦¬, {len(self.retry_queue)}ê°œ ë‚¨ìŒ")
    
    def get_stats(self) -> Dict:
        """ì²˜ë¦¬ í†µê³„ ë°˜í™˜"""
        return {
            "processed": self.processed_count,
            "failed": self.failed_count,
            "retry_queue": len(self.retry_queue)
        }

# ì „ì—­ ì¦‰ì‹œ ì²˜ë¦¬ê¸° ì¸ìŠ¤í„´ìŠ¤
immediate_processor = ImmediateProcessor()

# =============================================================================
# í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ ì„¤ì • í´ë˜ìŠ¤
# =============================================================================

class CrawlingSchedule:
    """í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ë³„ ì„¤ì • ê´€ë¦¬"""

    FREQUENT_WAIT_TIME = 25      # 15ë¶„ ì£¼ê¸° ëŒ€ê¸°ì‹œê°„ (ìµœì í™”)
    REGULAR_WAIT_TIME = 30       # 30ë¶„ ì£¼ê¸° ëŒ€ê¸°ì‹œê°„  
    REDDIT_WAIT_TIME = 15        # Reddit ëŒ€ê¸°ì‹œê°„
    RULIWEB_WAIT_TIME = 20       # ë£¨ë¦¬ì›¹ ëŒ€ê¸°ì‹œê°„

    # ìŠ¤í¬ë¡¤ íšŸìˆ˜ ì„¤ì •
    FREQUENT_SCROLL_COUNT = 2    # 15ë¶„ ì£¼ê¸° ìŠ¤í¬ë¡¤ (ì„±ëŠ¥ ìµœì í™”)
    REGULAR_SCROLL_COUNT = 3

    @staticmethod
    def get_wait_time(schedule_type: str) -> int:
        """ìŠ¤ì¼€ì¤„ íƒ€ì…ë³„ ëŒ€ê¸°ì‹œê°„ ë°˜í™˜"""
        if schedule_type == 'frequent':
            return CrawlingSchedule.FREQUENT_WAIT_TIME
        elif schedule_type == 'regular':
            return CrawlingSchedule.REGULAR_WAIT_TIME
        elif schedule_type == 'reddit':
            return CrawlingSchedule.REDDIT_WAIT_TIME
        elif schedule_type == 'ruliweb':
            return CrawlingSchedule.RULIWEB_WAIT_TIME
        else:
            return CrawlingSchedule.REGULAR_WAIT_TIME

    @staticmethod
    def get_scroll_count(schedule_type: str) -> int:
        """ìŠ¤ì¼€ì¤„ íƒ€ì…ë³„ ìŠ¤í¬ë¡¤ íšŸìˆ˜ ë°˜í™˜"""
        if schedule_type == 'frequent':
            return CrawlingSchedule.FREQUENT_SCROLL_COUNT
        else:
            return CrawlingSchedule.REGULAR_SCROLL_COUNT

# =============================================================================
# íŒŒì¼ ê´€ë¦¬ ì‹œìŠ¤í…œ - ì‹œê°„ ê¸°ë°˜ ì¤‘ë³µ ê´€ë¦¬ ê°œì„ 
# =============================================================================

def get_crawled_links_file():
    """ì›Œí¬í”Œë¡œìš°ë³„ ë…ë¦½ì ì¸ í¬ë¡¤ë§ ë§í¬ íŒŒì¼ëª… ìƒì„±"""
    workflow_name = os.environ.get('GITHUB_WORKFLOW', 'default')

    if 'debug' in workflow_name.lower() or 'test' in workflow_name.lower():
        return "crawled_links_debug.json"
    elif 'monitor' in workflow_name.lower():
        return "crawled_links_monitor.json"
    else:
        return "crawled_links.json"

def get_content_cache_file():
    """ì›Œí¬í”Œë¡œìš°ë³„ ë…ë¦½ì ì¸ ì½˜í…ì¸  ìºì‹œ íŒŒì¼ëª… ìƒì„±"""
    workflow_name = os.environ.get('GITHUB_WORKFLOW', 'default')

    if 'debug' in workflow_name.lower():
        return "content_cache_debug.json"
    else:
        return "content_cache.json"

def load_crawled_links():
    """í¬ë¡¤ë§ ë§í¬ ë¡œë“œ - ì‹œê°„ ê¸°ë°˜ êµ¬ì¡° ì ìš©"""
    crawled_links_file = get_crawled_links_file()
    
    if os.path.exists(crawled_links_file):
        try:
            with open(crawled_links_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
                # ê¸°ì¡´ ë‹¨ìˆœ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¥¼ ìƒˆ êµ¬ì¡°ë¡œ ë³€í™˜
                if isinstance(data, dict) and "links" in data:
                    if isinstance(data["links"], list) and len(data["links"]) > 0:
                        # ê¸°ì¡´ ë‹¨ìˆœ ë§í¬ë¥¼ ì‹œê°„ êµ¬ì¡°ë¡œ ë³€í™˜
                        if isinstance(data["links"][0], str):
                            converted_links = []
                            for link in data["links"]:
                                converted_links.append({
                                    "url": link,
                                    "processed_at": (datetime.now() - timedelta(hours=25)).isoformat(),
                                    "notified": False
                                })
                            data["links"] = converted_links
                            print(f"[INFO] ê¸°ì¡´ {len(converted_links)}ê°œ ë§í¬ë¥¼ ìƒˆ êµ¬ì¡°ë¡œ ë³€í™˜")
                
                # 24ì‹œê°„ ì§€ë‚œ í•­ëª© ìë™ ì œê±°
                now = datetime.now()
                valid_links = []
                for item in data.get("links", []):
                    try:
                        processed_time = datetime.fromisoformat(item["processed_at"])
                        if now - processed_time < timedelta(hours=24):
                            valid_links.append(item)
                    except:
                        continue
                
                data["links"] = valid_links
                print(f"[INFO] 24ì‹œê°„ ê¸°ì¤€ ìœ íš¨í•œ ë§í¬: {len(valid_links)}ê°œ")
                return data
                        
        except Exception as e:
            print(f"[WARNING] í¬ë¡¤ë§ ë§í¬ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
    
    return {"links": [], "last_updated": datetime.now().isoformat()}

def save_crawled_links(link_data):
    """í¬ë¡¤ë§ ë§í¬ ì €ì¥ - ì ê·¹ì  í¬ê¸° ê´€ë¦¬"""
    try:
        # í¬ê¸° ì œí•œì„ 100ê°œë¡œ ì¶•ì†Œ (ë” ì ê·¹ì  ê´€ë¦¬)
        if len(link_data["links"]) > 100:
            # ìµœì‹  100ê°œë§Œ ìœ ì§€
            link_data["links"] = sorted(
                link_data["links"], 
                key=lambda x: x.get("processed_at", ""), 
                reverse=True
            )[:100]
            print(f"[INFO] ë§í¬ ëª©ë¡ì„ ìµœì‹  100ê°œë¡œ ì •ë¦¬")

        link_data["last_updated"] = datetime.now().isoformat()

        crawled_links_file = get_crawled_links_file()
        with open(crawled_links_file, 'w', encoding='utf-8') as f:
            json.dump(link_data, f, ensure_ascii=False, indent=2)
            
        print(f"[INFO] í¬ë¡¤ë§ ë§í¬ ì €ì¥ ì™„ë£Œ: {len(link_data['links'])}ê°œ")

    except Exception as e:
        print(f"[ERROR] ë§í¬ ì €ì¥ ì‹¤íŒ¨: {e}")

def is_recently_processed(url: str, links_data: List[Dict], hours: int = 24) -> bool:
    """ì‹œê°„ ê¸°ë°˜ ì¤‘ë³µ ì²´í¬ - 24ì‹œê°„ ë‚´ ì²˜ë¦¬ëœ ë§í¬ì¸ì§€ í™•ì¸"""
    try:
        now = datetime.now()
        for item in links_data:
            if item.get("url") == url:
                processed_time = datetime.fromisoformat(item["processed_at"])
                if now - processed_time < timedelta(hours=hours):
                    return True
        return False
    except Exception as e:
        print(f"[DEBUG] ì¤‘ë³µ ì²´í¬ ì˜¤ë¥˜: {e}")
        return False

def mark_as_processed(url: str, notified: bool = False):
    """ê²Œì‹œê¸€ì„ ì²˜ë¦¬ë¨ìœ¼ë¡œ ë§ˆí‚¹ - ì•Œë¦¼ ì„±ê³µ í›„ì—ë§Œ í˜¸ì¶œ"""
    try:
        link_data = load_crawled_links()
        
        # ê¸°ì¡´ í•­ëª© ì—…ë°ì´íŠ¸ ë˜ëŠ” ìƒˆ í•­ëª© ì¶”ê°€
        found = False
        for item in link_data["links"]:
            if item.get("url") == url:
                item["processed_at"] = datetime.now().isoformat()
                item["notified"] = notified
                found = True
                break
        
        if not found:
            link_data["links"].append({
                "url": url,
                "processed_at": datetime.now().isoformat(),
                "notified": notified
            })
        
        save_crawled_links(link_data)
        print(f"[INFO] ë§í¬ ì²˜ë¦¬ ì™„ë£Œ ë§ˆí‚¹: {url[:50]}... (ì•Œë¦¼: {notified})")
        
    except Exception as e:
        print(f"[ERROR] ë§í¬ ë§ˆí‚¹ ì‹¤íŒ¨: {e}")

def load_content_cache():
    """ê²Œì‹œê¸€ ë‚´ìš© ìºì‹œ ë¡œë“œ"""
    content_cache_file = get_content_cache_file()

    if os.path.exists(content_cache_file):
        try:
            with open(content_cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            print(f"[WARNING] {content_cache_file} íŒŒì¼ ì½ê¸° ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±")
    return {}

def save_content_cache(cache_data):
    """ê²Œì‹œê¸€ ë‚´ìš© ìºì‹œ ì €ì¥"""
    try:
        if len(cache_data) > 500:
            sorted_items = sorted(cache_data.items(), 
                                key=lambda x: x[1].get('timestamp', ''), reverse=True)
            cache_data = dict(sorted_items[:500])

        content_cache_file = get_content_cache_file()
        with open(content_cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)

    except Exception as e:
        print(f"[ERROR] ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

# =============================================================================
# Chrome Driver ê´€ë¦¬ - ë¦¬ì†ŒìŠ¤ ìµœì í™” ê°•í™”
# =============================================================================

def get_chrome_driver():
    """Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” - ë¦¬ì†ŒìŠ¤ ìµœì í™” ë° ì•ˆì •ì„± ê°•í™”"""
    options = Options()

    # ê¸°ë³¸ ì˜µì…˜ë“¤
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--disable-extensions')
    options.add_argument('--disable-plugins')
    options.add_argument('--disable-images')
    options.add_argument('--window-size=1920,1080')

    # ì¶”ê°€ ë¦¬ì†ŒìŠ¤ ìµœì í™” ì˜µì…˜
    options.add_argument('--memory-pressure-off')
    options.add_argument('--max_old_space_size=2048')
    options.add_argument('--disable-background-timer-throttling')
    options.add_argument('--disable-backgrounding-occluded-windows')
    options.add_argument('--disable-renderer-backgrounding')
    options.add_argument('--disable-features=TranslateUI')
    options.add_argument('--disable-default-apps')
    options.add_argument('--disable-web-security')
    options.add_argument('--disable-features=VizDisplayCompositor')

    # ë´‡ íƒì§€ ìš°íšŒ
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)

    # ëœë¤ User-Agent
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36'
    ]
    options.add_argument(f'--user-agent={random.choice(user_agents)}')

    # ì„±ëŠ¥ ìµœì í™”
    prefs = {
        'profile.default_content_setting_values': {
            'images': 2, 'plugins': 2, 'popups': 2,
            'geolocation': 2, 'notifications': 2, 'media_stream': 2,
        }
    }
    options.add_experimental_option('prefs', prefs)

    # 3ë‹¨ê³„ í´ë°± ë©”ì»¤ë‹ˆì¦˜
    possible_paths = [
        '/usr/bin/chromedriver',
        '/usr/local/bin/chromedriver', 
        '/snap/bin/chromium.chromedriver'
    ]

    # 1ë‹¨ê³„: ì‹œìŠ¤í…œ ê²½ë¡œë“¤ ì‹œë„
    for path in possible_paths:
        try:
            if os.path.exists(path):
                print(f"[DEBUG] ChromeDriver ì‹œë„: {path}")
                service = Service(path)
                driver = webdriver.Chrome(service=service, options=options)
                print(f"[DEBUG] ChromeDriver ì„±ê³µ: {path}")
                return driver
        except Exception as e:
            print(f"[DEBUG] ChromeDriver ì‹¤íŒ¨ {path}: {str(e)[:100]}...")
            continue

    # 2ë‹¨ê³„: WebDriver Manager
    try:
        print("[DEBUG] WebDriver Manager ì‹œë„")
        from webdriver_manager.chrome import ChromeDriverManager
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        print("[DEBUG] WebDriver Manager ì„±ê³µ")
        return driver
    except Exception as e:
        print(f"[DEBUG] WebDriver Manager ì‹¤íŒ¨: {str(e)[:100]}...")

    raise Exception("ëª¨ë“  ChromeDriver ì´ˆê¸°í™” ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

# =============================================================================
# URL ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°
# =============================================================================

def fix_url_bug(url):
    """URL ë²„ê·¸ ìˆ˜ì • í•¨ìˆ˜"""
    if not url:
        return url

    # ttps:// â†’ https:// ìˆ˜ì •
    if url.startswith('ttps://'):
        url = 'h' + url
        print(f"[URL FIX] ttps â†’ https: {url}")

    # ìƒëŒ€ ê²½ë¡œ â†’ ì ˆëŒ€ ê²½ë¡œ
    elif url.startswith('/'):
        if 'onstove.com' in url or 'epicseven' in url:
            url = 'https://page.onstove.com' + url
        elif 'ruliweb.com' in url:
            url = 'https://bbs.ruliweb.com' + url
        elif 'reddit.com' in url:
            url = 'https://www.reddit.com' + url
        print(f"[URL FIX] ìƒëŒ€ê²½ë¡œ ìˆ˜ì •: {url}")

    # í”„ë¡œí† ì½œ ëˆ„ë½
    elif not url.startswith(('http://', 'https://')):
        url = 'https://' + url
        print(f"[URL FIX] í”„ë¡œí† ì½œ ì¶”ê°€: {url}")

    return url

# =============================================================================
# Phase 2: ì˜ë¯¸ìˆëŠ” ë³¸ë¬¸ ì¶”ì¶œ í•¨ìˆ˜ (ì„±ëŠ¥ ìµœì í™”)
# =============================================================================

def extract_meaningful_content(text: str) -> str:
    """Phase 2: ì˜ë¯¸ìˆëŠ” ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ ì•Œê³ ë¦¬ì¦˜ (ì„±ëŠ¥ ìµœì í™”)"""
    if not text or len(text) < 30:
        return ""

    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í•  (ê°œì„ ëœ ì •ê·œì‹)
    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]\s*', text.strip())
    sentences = [s.strip() for s in sentences if s.strip()]

    if not sentences:
        return text[:100].strip()

    # ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ í•„í„°ë§ ì‹œìŠ¤í…œ
    meaningful_sentences = []

    for sentence in sentences:
        if len(sentence) < 10:  # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸
            continue

        # ì˜ë¯¸ì—†ëŠ” ë¬¸ì¥ íŒ¨í„´ ì œì™¸
        meaningless_patterns = [
            r'^[ã…‹ã…ã„·ã… ã…œã…¡]+$',  # ììŒëª¨ìŒë§Œ
            r'^[!@#$%^&*()_+\-=\[\]{}|;\':",./<>?`~]+$',  # íŠ¹ìˆ˜ë¬¸ìë§Œ
            r'^\d+$',  # ìˆ«ìë§Œ
            r'^(ìŒ|ì–´|ì•„|ë„¤|ì˜ˆ|ì‘|ã…‡ã…‡|ã… ã… |ã…œã…œ)$',  # ë‹¨ìˆœ ê°íƒ„ì‚¬
        ]

        if any(re.match(pattern, sentence) for pattern in meaningless_patterns):
            continue

        # Epic7 ê´€ë ¨ ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œ ìŠ¤ì½”ì–´ë§
        meaningful_keywords = [
            'ë²„ê·¸', 'ì˜¤ë¥˜', 'ë¬¸ì œ', 'ì—ëŸ¬', 'ì•ˆë¨', 'ì‘ë™', 'ì‹¤í–‰',
            'ìºë¦­í„°', 'ìŠ¤í‚¬', 'ì•„í‹°íŒ©íŠ¸', 'ì¥ë¹„', 'ë˜ì „', 'ì•„ë ˆë‚˜', 
            'ê¸¸ë“œ', 'ì´ë²¤íŠ¸', 'ì—…ë°ì´íŠ¸', 'íŒ¨ì¹˜', 'ë°¸ëŸ°ìŠ¤', 'ë„ˆí”„',
            'ê²Œì„', 'í”Œë ˆì´', 'ìœ ì €', 'ìš´ì˜', 'ê³µì§€', 'í™•ë¥ ',
            'ë½‘ê¸°', 'ì†Œí™˜', '6ì„±', 'ê°ì„±', 'ì´ˆì›”', 'ë£¬', 'ì ¬'
        ]

        score = sum(1 for keyword in meaningful_keywords if keyword in sentence)

        # ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ìœ¼ë¡œ íŒë³„
        if score > 0 or len(sentence) >= 30:
            meaningful_sentences.append(sentence)

    if not meaningful_sentences:
        # í´ë°±: ì²« ë²ˆì§¸ ê¸´ ë¬¸ì¥
        long_sentences = [s for s in sentences if len(s) >= 20]
        if long_sentences:
            return long_sentences[0]
        else:
            return sentences[0] if sentences else text[:100]

    # ìµœì  ì¡°í•©: 1-3ê°œ ë¬¸ì¥ ì¡°í•©ìœ¼ë¡œ ì˜ë¯¸ìˆëŠ” ë‚´ìš© êµ¬ì„±
    result = meaningful_sentences[0]

    # ì²« ë²ˆì§¸ ë¬¸ì¥ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ë‘ ë²ˆì§¸ ë¬¸ì¥ ì¶”ê°€
    if len(result) < 50 and len(meaningful_sentences) > 1:
        result += ' ' + meaningful_sentences[1]

    # ì—¬ì „íˆ ë¶€ì¡±í•˜ë©´ ì„¸ ë²ˆì§¸ ë¬¸ì¥ê¹Œì§€ ì¶”ê°€
    if len(result) < 80 and len(meaningful_sentences) > 2:
        result += ' ' + meaningful_sentences[2]

    return result.strip()

# =============================================================================
# Phase 2: Stove ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ í•¨ìˆ˜ - ì„±ëŠ¥ ìµœì í™” ì™„ë£Œ
# =============================================================================

def get_stove_post_content(post_url: str, driver: webdriver.Chrome, 
                          source: str = "stove_korea_bug", 
                          schedule_type: str = "frequent") -> str:
    """Phase 2: ìŠ¤í† ë¸Œ ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ - ì„±ëŠ¥ ìµœì í™” ì™„ë£Œ"""

    # ìºì‹œ í™•ì¸
    cache = load_content_cache()
    url_hash = hash(post_url) % (10**8)

    if str(url_hash) in cache:
        cached_item = cache[str(url_hash)]
        cache_time = datetime.fromisoformat(cached_item.get('timestamp', '2000-01-01'))
        if datetime.now() - cache_time < timedelta(hours=24):
            print(f"[CACHE] ìºì‹œëœ ë‚´ìš© ì‚¬ìš©: {post_url}")
            return cached_item.get('content', "ê²Œì‹œê¸€ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    content_summary = "ê²Œì‹œê¸€ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    try:
        print(f"[DEBUG] ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ ì‹œë„: {post_url}")

        wait_time = CrawlingSchedule.get_wait_time(schedule_type)
        driver.set_page_load_timeout(wait_time + 10)
        driver.get(post_url)

        print(f"[DEBUG] í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘... ({wait_time}ì´ˆ)")
        time.sleep(wait_time)

        # JavaScript ì™„ì „ ë¡œë”© í™•ì¸
        WebDriverWait(driver, 15).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )

        # Phase 2 ìµœì í™”: ë‹¨ê³„ë³„ ìŠ¤í¬ë¡¤ë§ (ì„±ëŠ¥ ê°œì„ )
        print("[DEBUG] ìµœì í™”ëœ ìŠ¤í¬ë¡¤ë§ ì‹œì‘...")
        driver.execute_script("window.scrollTo(0, 500);")
        time.sleep(2)
        driver.execute_script("window.scrollTo(0, 1000);")
        time.sleep(2)
        driver.execute_script("window.scrollTo(0, 0);")
        time.sleep(1)
        print("[DEBUG] ìµœì í™”ëœ ìŠ¤í¬ë¡¤ë§ ì™„ë£Œ")

        # Phase 2: Master ë°œê²¬ CSS Selector ìš°ì„  ì ìš©
        content_selectors = [
            # Master ì§€ì ì‚¬í•­: ëª©ë¡ í˜ì´ì§€ì—ì„œ ì§ì ‘ ì¶”ì¶œ
            'meta[data-vmid="description"]',
            'meta[name="description"]',

            # ê°œë³„ í˜ì´ì§€ ì„ íƒìë“¤ (ë°±ì—…)
            'div.s-article-content',
            'div.s-article-content-text',
            'section.s-article-body',
            'div.s-board-content',

            # Phase 2: ì¶”ê°€ ë°±ì—… ì„ íƒì
            '.article-content',
            '.post-content',
            '[class*="content"]'
        ]

        # Phase 2: ì˜ë¯¸ìˆëŠ” ë³¸ë¬¸ ì¶”ì¶œ ì•Œê³ ë¦¬ì¦˜ ì ìš©
        for i, selector in enumerate(content_selectors):
            try:
                elements = driver.find_elements(By.CSS_SELECTOR, selector)
                if elements:
                    for element in elements:
                        # ë©”íƒ€ íƒœê·¸ëŠ” content ì†ì„±ì—ì„œ, ì¼ë°˜ íƒœê·¸ëŠ” textì—ì„œ ì¶”ì¶œ
                        if selector.startswith('meta'):
                            raw_text = element.get_attribute('content').strip()
                        else:
                            raw_text = element.text.strip()

                        if not raw_text or len(raw_text) < 30:
                            continue           

                        # Phase 2: ë©”íƒ€ë°ì´í„° í•„í„°ë§ ê°•í™”
                        skip_keywords = [
                            'install stove', 'ìŠ¤í† ë¸Œë¥¼ ì„¤ì¹˜', 'ë¡œê·¸ì¸ì´ í•„ìš”', 
                            'javascript', 'ëŒ“ê¸€', 'ê³µìœ ', 'ì¢‹ì•„ìš”', 'ì¶”ì²œ', 'ì‹ ê³ ',
                            'ì‘ì„±ì', 'ì‘ì„±ì¼', 'ì¡°íšŒìˆ˜', 'ì²¨ë¶€íŒŒì¼', 'ë‹¤ìš´ë¡œë“œ',
                            'copyright', 'ì €ì‘ê¶Œ', 'ì´ìš©ì•½ê´€', 'ê°œì¸ì •ë³´', 'ì¿ í‚¤',
                            'ê´‘ê³ ', 'ad', 'advertisement', 'í”„ë¡œëª¨ì…˜', 'ì´ë²¤íŠ¸',
                            'ë¡œê·¸ì¸', 'login', 'sign in', 'íšŒì›ê°€ì…', 'register',
                            'ë©”ë‰´', 'menu', 'navigation', 'ë„¤ë¹„ê²Œì´ì…˜', 'ì‚¬ì´ë“œë°”',
                            'ë°°ë„ˆ', 'banner', 'í‘¸í„°', 'footer', 'í—¤ë”', 'header'
                        ]

                        if any(skip.lower() in raw_text.lower() for skip in skip_keywords):
                            continue

                        # Phase 2: ì˜ë¯¸ìˆëŠ” ë¬¸ë‹¨ ì¶”ì¶œ (ì„±ëŠ¥ ìµœì í™”)
                        meaningful_content = extract_meaningful_content(raw_text)

                        # Phase 2: ìµœì†Œ ê¸¸ì´ 50ì ì´ìƒìœ¼ë¡œ ì¦ê°€
                        if len(meaningful_content) >= 50:
                            # 150ì ì´ë‚´ë¡œ ìš”ì•½
                            if len(meaningful_content) > 150:
                                content_summary = meaningful_content[:147] + '...'
                            else:
                                content_summary = meaningful_content

                            print(f"[SUCCESS] ì„ íƒì {i+1}/{len(content_selectors)} '{selector}'ë¡œ ë‚´ìš© ì¶”ì¶œ ì„±ê³µ")
                            print(f"[CONTENT] {content_summary[:80]}...")
                            break

                    if content_summary != "ê²Œì‹œê¸€ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.":
                        break

            except Exception as e:
                print(f"[DEBUG] ì„ íƒì '{selector}' ì‹¤íŒ¨: {e}")
                continue

        # ìºì‹œ ì €ì¥
        cache[str(url_hash)] = {
            'content': content_summary,
            'timestamp': datetime.now().isoformat(),
            'url': post_url,
            'source': source
        }
        save_content_cache(cache)

    except TimeoutException:
        print(f"[ERROR] í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ: {post_url}")
        content_summary = "â° ê²Œì‹œê¸€ ë¡œë”© ì‹œê°„ ì´ˆê³¼"
    except Exception as e:
        print(f"[ERROR] ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        content_summary = "ğŸ”— ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ ì‹¤íŒ¨"

    return content_summary

# =============================================================================
# ğŸš€ Master ìš”êµ¬ì‚¬í•­: Stove ê²Œì‹œíŒ í¬ë¡¤ë§ + ì¦‰ì‹œ ì²˜ë¦¬ í†µí•©
# =============================================================================

def crawl_stove_board(board_url: str, source: str, force_crawl: bool = False, 
                     schedule_type: str = "frequent", region: str = "korea",
                     on_post_process: Optional[Callable[[Dict], None]] = None) -> List[Dict]:
    """
    Stove ê²Œì‹œíŒ í¬ë¡¤ë§ + ì¦‰ì‹œ ì²˜ë¦¬ í†µí•©
    Master ìš”êµ¬ì‚¬í•­: ê²Œì‹œê¸€ë³„ ì¦‰ì‹œ ì²˜ë¦¬ (í¬ë¡¤ë§â†’ê°ì„±ë¶„ì„â†’ì•Œë¦¼â†’ë§ˆí‚¹)
    """

    posts = []
    link_data = load_crawled_links()

    print(f"[INFO] {source} í¬ë¡¤ë§ ì‹œì‘ - URL: {board_url}")
    print(f"[DEBUG] ê¸°ì¡´ ë§í¬ ìˆ˜: {len(link_data['links'])}, Force Crawl: {force_crawl}")

    driver = None
    try:
        driver = get_chrome_driver()

        wait_time = CrawlingSchedule.get_wait_time(schedule_type)
        driver.set_page_load_timeout(wait_time + 10)
        driver.implicitly_wait(15)

        print(f"[DEBUG] ê²Œì‹œíŒ ì ‘ì† ì¤‘: {board_url}")
        driver.get(board_url)

        print(f"[DEBUG] í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘... ({wait_time}ì´ˆ)")
        time.sleep(wait_time)

        # JavaScript ì™„ì „ ë¡œë”© í™•ì¸
        WebDriverWait(driver, 15).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )

        # Phase 2: ìµœì í™”ëœ ìŠ¤í¬ë¡¤ë§ (ì„±ëŠ¥ ê°œì„ )
        driver.execute_script("window.scrollTo(0, 800);")
        time.sleep(3)

        # ë””ë²„ê¹…ìš© HTML ì €ì¥
        debug_filename = f"{source}_debug_selenium.html"
        with open(debug_filename, "w", encoding="utf-8") as f:
            f.write(driver.page_source)
        print(f"[DEBUG] HTML ì €ì¥: {debug_filename}")

        # Phase 2: Master ë°œê²¬ ì„ íƒì ìš°ì„  ì ìš© - JavaScript ìµœì í™”
        user_posts = driver.execute_script("""
            var userPosts = [];

            // Phase 2: Master ì§€ì ì‚¬í•­ - section.s-board-item ìµœìš°ì„  ì ìš©
            const selectors = [
                'section.s-board-item',           // Master ë°œê²¬ ì„ íƒì (ìµœìš°ì„ )
                'h3.s-board-title',               // ê¸°ì¡´ ì„ íƒì (ë°±ì—…)
                '[class*="board-title"]',         // í´ë˜ìŠ¤ëª… í¬í•¨
                '[class*="post-title"]',          // post-title í¬í•¨
                '[class*="article-title"]',       // article-title í¬í•¨
                'h3[class*="title"]',            // h3 íƒœê·¸ title í¬í•¨
                'a[href*="/view/"]'              // view ë§í¬ ì§ì ‘ ì°¾ê¸°
            ];

            var elements = [];
            var successful_selector = '';

            // ì„ íƒìë³„ ì‹œë„
            for (var i = 0; i < selectors.length; i++) {
                try {
                    elements = document.querySelectorAll(selectors[i]);
                    if (elements && elements.length > 0) {
                        successful_selector = selectors[i];
                        console.log('Phase 2 ì„ íƒì ì„±ê³µ:', selectors[i], 'ê°œìˆ˜:', elements.length);
                        break;
                    }
                } catch (e) {
                    console.log('ì„ íƒì ì‹¤íŒ¨:', selectors[i], e);
                    continue;
                }
            }

            if (!elements || elements.length === 0) {
                console.log('ëª¨ë“  ì„ íƒì ì‹¤íŒ¨');
                return [];
            }

            console.log('ì´ ë°œê²¬ëœ ìš”ì†Œ ìˆ˜:', elements.length);

            // ê³µì§€ì‚¬í•­ IDë“¤ (ì œì™¸ ëŒ€ìƒ)
            const officialIds = ['10518001', '10855687', '10855562', '10855132'];

            // ê° ìš”ì†Œì—ì„œ ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ
            for (var i = 0; i < Math.min(elements.length, 20); i++) {
                var element = elements[i];

                try {
                    var linkElement, titleElement, contentElement = null;
                    var href = '', title = '', preview_content = '';

                    // ë§í¬ ìš”ì†Œ ì°¾ê¸°
                    if (successful_selector === 'section.s-board-item') {
                        // Phase 2: Master ì§€ì ì‚¬í•­ - ëª©ë¡ í˜ì´ì§€ì—ì„œ ì§ì ‘ ë³¸ë¬¸ ì¶”ì¶œ
                        linkElement = element.querySelector('a[href*="/view/"]');
                        titleElement = element.querySelector('.s-board-title-text, .board-title, h3 span, .title');

                        // Master ë°œê²¬: p.s-board-textì—ì„œ ë³¸ë¬¸ ì§ì ‘ ì¶”ì¶œ
                        contentElement = element.querySelector('p.s-board-text');
                        if (contentElement) {
                            preview_content = contentElement.textContent?.trim() || '';
                        }
                    } else {
                        // ê¸°íƒ€ ì„ íƒì ê¸°ë°˜ ì¶”ì¶œ
                        linkElement = element.closest('a[href*="/view/"]') || element.querySelector('a[href*="/view/"]');
                        titleElement = element;
                    }

                    // ë§í¬ ì¶”ì¶œ
                    if (linkElement && linkElement.href) {
                        href = linkElement.href;
                    }

                    // ì œëª© ì¶”ì¶œ
                    if (titleElement) {
                        title = titleElement.textContent?.trim() || titleElement.innerText?.trim() || '';
                    }

                    // ìœ íš¨ì„± ê²€ì‚¬
                    if (!href || !title || title.length < 3) {
                        continue;
                    }

                    // URLì—ì„œ ê²Œì‹œê¸€ ID ì¶”ì¶œ
                    var idMatch = href.match(/\/view\/(\d+)/);
                    if (!idMatch) {
                        continue;
                    }
                    var id = idMatch[1];

                    // ê³µì§€ì‚¬í•­ ì œì™¸
                    if (officialIds.includes(id)) {
                        console.log('ê³µì§€ì‚¬í•­ ì œì™¸:', id, title.substring(0, 20));
                        continue;
                    }

                    // ê³µì§€/ì´ë²¤íŠ¸ ë°°ì§€ í™•ì¸
                    var isNotice = element.querySelector('i.element-badge__s.notice, .notice, [class*="notice"]');
                    var isEvent = element.querySelector('i.element-badge__s.event, .event, [class*="event"]');
                    var isOfficial = element.querySelector('span.s-profile-staff-official, [class*="official"]');

                    if (isNotice || isEvent || isOfficial) {
                        console.log('ê³µì§€/ì´ë²¤íŠ¸ ì œì™¸:', title.substring(0, 20));
                        continue;
                    }

                    // ì œëª©ì—ì„œ [ê³µì§€], [ì´ë²¤íŠ¸] ë“± í‚¤ì›Œë“œ ì œì™¸  
                    var skipKeywords = ['[ê³µì§€]', '[ì´ë²¤íŠ¸]', '[ì•ˆë‚´]', '[ì ê²€]', '[ê³µì§€ì‚¬í•­]'];
                    var shouldSkip = skipKeywords.some(function(keyword) {
                        return title.includes(keyword);
                    });

                    if (shouldSkip) {
                        console.log('í‚¤ì›Œë“œ ì œì™¸:', title.substring(0, 20));
                        continue;
                    }

                    // URL ì •ê·œí™”
                    var fullUrl = href.startsWith('http') ? href : 'https://page.onstove.com' + href;

                    userPosts.push({
                        href: fullUrl,
                        id: id,
                        title: title.substring(0, 200).trim(),
                        preview_content: preview_content.substring(0, 150).trim(),
                        selector_used: successful_selector
                    });

                    console.log('Phase 2 ê²Œì‹œê¸€ ì¶”ê°€:', title.substring(0, 30));

                } catch (e) {
                    console.log('ê²Œì‹œê¸€ ì²˜ë¦¬ ì˜¤ë¥˜:', e.message);
                    continue;
                }
            }

            console.log('Phase 2 ìµœì¢… ì¶”ì¶œëœ ìœ ì € ê²Œì‹œê¸€ ìˆ˜:', userPosts.length);
            return userPosts;
        """)

        print(f"[DEBUG] Phase 2 JavaScriptë¡œ {len(user_posts)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")

        # ğŸš€ Master ìš”êµ¬ì‚¬í•­: ê° ê²Œì‹œê¸€ë³„ ì¦‰ì‹œ ì²˜ë¦¬
        for i, post_info in enumerate(user_posts, 1):
            try:
                href = post_info['href']
                title = post_info['title']
                post_id = post_info['id']
                preview_content = post_info.get('preview_content', '')

                # URL ë²„ê·¸ ìˆ˜ì • ì ìš©
                href = fix_url_bug(href)

                print(f"[DEBUG] ê²Œì‹œê¸€ {i}/{len(user_posts)}: {title[:40]}...")
                print(f"[DEBUG] URL: {href}")

                # ì‹œê°„ ê¸°ë°˜ ì¤‘ë³µ í™•ì¸ (24ì‹œê°„ ë‚´ ì²˜ë¦¬ëœ ê²½ìš°ë§Œ SKIP)
                if not force_crawl and is_recently_processed(href, link_data["links"]):
                    print(f"[SKIP] 24ì‹œê°„ ë‚´ ì²˜ë¦¬ëœ ë§í¬: {post_id}")
                    continue

                # ì œëª© ê¸¸ì´ ê²€ì¦
                if len(title) < 5:
                    print(f"[SKIP] ì œëª©ì´ ë„ˆë¬´ ì§§ìŒ: {title}")
                    continue

                # Phase 2: ëª©ë¡ í˜ì´ì§€ì—ì„œ ì¶”ì¶œí•œ ë³¸ë¬¸ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê°œë³„ í˜ì´ì§€ ë°©ë¬¸
                if preview_content and len(preview_content) >= 50:
                    content = preview_content
                    print(f"[PHASE2] ëª©ë¡ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ì§ì ‘ ì¶”ì¶œ ì„±ê³µ (90% ì‹œê°„ ë‹¨ì¶•)")
                else:
                    # ê°œë³„ í˜ì´ì§€ ë°©ë¬¸ (ë°±ì—…)
                    content = get_stove_post_content(href, driver, source, schedule_type)

                # ê²Œì‹œê¸€ ë°ì´í„° êµ¬ì„±
                post_data = {
                    "title": title,
                    "url": href,
                    "content": content,
                    "timestamp": datetime.now().isoformat(),
                    "source": source,
                    "id": post_id,
                    "region": region,
                    "schedule_type": schedule_type
                }

                # ğŸš€ Master í•µì‹¬ ìš”êµ¬ì‚¬í•­: ì¦‰ì‹œ ì²˜ë¦¬ (í¬ë¡¤ë§â†’ê°ì„±ë¶„ì„â†’ì•Œë¦¼â†’ë§ˆí‚¹)
                if on_post_process:
                    try:
                        print(f"[IMMEDIATE] ê²Œì‹œê¸€ ì¦‰ì‹œ ì²˜ë¦¬ ì‹œì‘: {title[:30]}...")
                        on_post_process(post_data)
                        print(f"[SUCCESS] ê²Œì‹œê¸€ ì¦‰ì‹œ ì²˜ë¦¬ ì™„ë£Œ: {title[:30]}...")
                    except Exception as e:
                        print(f"[ERROR] ê²Œì‹œê¸€ ì¦‰ì‹œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        # ì—ëŸ¬ ê²©ë¦¬: 1ê°œ ì‹¤íŒ¨í•´ë„ ë‹¤ìŒ ê²Œì‹œê¸€ ê³„ì† ì²˜ë¦¬
                        continue
                else:
                    # ì½œë°±ì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                    posts.append(post_data)

                print(f"[SUCCESS] ìƒˆ ê²Œì‹œê¸€ ìˆ˜ì§‘ ({i}): {title[:30]}...")
                print(f"[CONTENT] {content[:80]}...")

                # í¬ë¡¤ë§ ê°„ ëŒ€ê¸° (Rate Limiting)
                time.sleep(random.uniform(1, 3))

            except Exception as e:
                print(f"[ERROR] ê²Œì‹œê¸€ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                # ğŸš€ Master ìš”êµ¬ì‚¬í•­: ì—ëŸ¬ ê²©ë¦¬ - 1ê°œ ì‹¤íŒ¨í•´ë„ ë‹¤ìŒìœ¼ë¡œ ê³„ì†
                continue

        print(f"[INFO] {source} í¬ë¡¤ë§ ì™„ë£Œ: {len(user_posts)}ê°œ ì¤‘ {len(posts)}ê°œ ì²˜ë¦¬")

    except Exception as e:
        print(f"[ERROR] {source} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    finally:
        if driver:
            try:
                driver.quit()
            except:
                pass

    return posts

# =============================================================================
# Reddit í¬ë¡¤ë§ í•¨ìˆ˜ (ì¦‰ì‹œ ì²˜ë¦¬ ì§€ì›)
# =============================================================================

def crawl_reddit_epic7(force_crawl: bool = False, limit: int = 10,
                      on_post_process: Optional[Callable[[Dict], None]] = None) -> List[Dict]:
    """Reddit r/EpicSeven ì„œë¸Œë ˆë”§ í¬ë¡¤ë§ + ì¦‰ì‹œ ì²˜ë¦¬ ì§€ì›"""

    if not REDDIT_AVAILABLE:
        print("[WARNING] PRAW ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ Reddit í¬ë¡¤ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return []

    posts = []

    try:
        print("[INFO] Reddit í¬ë¡¤ë§ ì‹œì‘")

        # Reddit API ì„¤ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ê¸°)
        reddit = praw.Reddit(
            client_id=os.environ.get('REDDIT_CLIENT_ID', 'your_client_id'),
            client_secret=os.environ.get('REDDIT_CLIENT_SECRET', 'your_client_secret'),
            user_agent=os.environ.get('REDDIT_USER_AGENT', 'Epic7Monitor/1.0')
        )

        # r/EpicSeven ì„œë¸Œë ˆë”§ ì ‘ê·¼
        subreddit = reddit.subreddit('EpicSeven')

        # ìµœì‹  ê²Œì‹œê¸€ë“¤ ê°€ì ¸ì˜¤ê¸°
        submissions = subreddit.new(limit=limit)

        link_data = load_crawled_links()

        for submission in submissions:
            try:
                # Reddit URL ìƒì„±
                reddit_url = f"https://www.reddit.com{submission.permalink}"

                # ì‹œê°„ ê¸°ë°˜ ì¤‘ë³µ í™•ì¸
                if not force_crawl and is_recently_processed(reddit_url, link_data["links"]):
                    continue

                # ì œëª© ê²€ì¦
                if len(submission.title) < 5:
                    continue

                # ìŠ¤íŒ¸/ê´‘ê³ ì„± ê²Œì‹œë¬¼ í•„í„°ë§
                spam_keywords = ['buy', 'sell', 'trade', 'account', 'giveaway', 'free']
                if any(keyword.lower() in submission.title.lower() for keyword in spam_keywords):
                    continue

                # Epic7 ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
                epic7_keywords = ['epic seven', 'epic7', 'e7', 'character', 'hero', 'artifact', 
                                'summon', 'gacha', 'gear', 'equipment', 'guild', 'arena']
                if not any(keyword.lower() in submission.title.lower() for keyword in epic7_keywords):
                    # ë³¸ë¬¸ì—ì„œë„ í™•ì¸
                    if hasattr(submission, 'selftext') and submission.selftext:
                        if not any(keyword.lower() in submission.selftext.lower() for keyword in epic7_keywords):
                            continue

                # ë‚´ìš© ì¶”ì¶œ
                content = ""
                if hasattr(submission, 'selftext') and submission.selftext:
                    content = submission.selftext[:200].strip()
                else:
                    content = f"Reddit ê²Œì‹œê¸€ - ë§í¬: {reddit_url}"

                # ê²Œì‹œê¸€ ë°ì´í„° êµ¬ì„±
                post_data = {
                    "title": submission.title,
                    "url": reddit_url,
                    "content": content,
                    "timestamp": datetime.now().isoformat(),
                    "source": "reddit_epicseven",
                    "id": submission.id,
                    "region": "global",
                    "schedule_type": "frequent",
                    "author": str(submission.author) if submission.author else "deleted",
                    "score": submission.score,
                    "comments": submission.num_comments
                }

                # ì¦‰ì‹œ ì²˜ë¦¬ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
                if on_post_process:
                    try:
                        print(f"[IMMEDIATE] Reddit ê²Œì‹œê¸€ ì¦‰ì‹œ ì²˜ë¦¬: {submission.title[:30]}...")
                        on_post_process(post_data)
                    except Exception as e:
                        print(f"[ERROR] Reddit ê²Œì‹œê¸€ ì¦‰ì‹œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        continue
                else:
                    posts.append(post_data)

                print(f"[SUCCESS] Reddit ê²Œì‹œê¸€ ì¶”ê°€: {submission.title[:50]}...")

            except Exception as e:
                print(f"[ERROR] Reddit ê²Œì‹œê¸€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue

        print(f"[INFO] Reddit í¬ë¡¤ë§ ì™„ë£Œ - {len(posts)}ê°œ ì²˜ë¦¬")

    except Exception as e:
        print(f"[ERROR] Reddit í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

    return posts

# =============================================================================
# ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ í•¨ìˆ˜ (ê¸°ì¡´ ìœ ì§€)
# =============================================================================

def crawl_ruliweb_epic7(on_post_process: Optional[Callable[[Dict], None]] = None) -> List[Dict]:
    """ë£¨ë¦¬ì›¹ ì—í”½ì„¸ë¸ ê²Œì‹œíŒ í¬ë¡¤ë§"""
    posts = []
    
    try:
        print("[INFO] ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì‹œì‘")
        
        # ê°„ë‹¨í•œ requests ê¸°ë°˜ í¬ë¡¤ë§
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36'
        }
        
        url = "https://bbs.ruliweb.com/game/85208"
        response = requests.get(url, headers=headers, timeout=30)
        
        if response.status_code == 200:
            print("[INFO] ë£¨ë¦¬ì›¹ ì ‘ì† ì„±ê³µ - ê¸°ë³¸ í¬ë¡¤ë§ ìˆ˜í–‰")
            # ê°„ë‹¨í•œ íŒŒì‹± ë¡œì§ (ìƒì„¸ êµ¬í˜„ ìƒëµ) 
            posts = []
        else:
            print(f"[WARNING] ë£¨ë¦¬ì›¹ ì ‘ì† ì‹¤íŒ¨: {response.status_code}")
            
    except Exception as e:
        print(f"[ERROR] ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    return posts

# =============================================================================
# ğŸš€ Master ìš”êµ¬ì‚¬í•­: í†µí•© í¬ë¡¤ë§ í•¨ìˆ˜ë“¤ - ì¦‰ì‹œ ì²˜ë¦¬ ì™„ì „ êµ¬í˜„
# =============================================================================

def crawl_frequent_sites(force_crawl: bool = False, 
                        on_post_process: Optional[Callable[[Dict], None]] = None) -> List[Dict]:
    """
    Phase 1: 15ë¶„ ì£¼ê¸° - ì „ì²´ í¬ë¡¤ë§ + ì¦‰ì‹œ ì²˜ë¦¬
    Master ìš”êµ¬ì‚¬í•­: ê²Œì‹œê¸€ë³„ ì¦‰ì‹œ ì²˜ë¦¬ (í¬ë¡¤ë§â†’ê°ì„±ë¶„ì„â†’ì•Œë¦¼â†’ë§ˆí‚¹)
    """
    all_posts = []

    print("[INFO] === 15ë¶„ ì£¼ê¸° ì „ì²´ í¬ë¡¤ë§ ì‹œì‘ (ì¦‰ì‹œ ì²˜ë¦¬ í†µí•©) ===")

    # Master ìš”êµ¬ì‚¬í•­: on_post_processê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì¦‰ì‹œ ì²˜ë¦¬ê¸° ì‚¬ìš©
    processor_func = on_post_process or immediate_processor.process_post_immediately

    # ì‚¬ì´íŠ¸ë³„ ë…ë¦½ ì‹¤í–‰ìœ¼ë¡œ ì•ˆì •ì„± ê°•í™” + ì¦‰ì‹œ ì²˜ë¦¬
    crawl_tasks = [
        ('í•œêµ­ ë²„ê·¸ ê²Œì‹œíŒ', lambda: crawl_stove_board(
            "https://page.onstove.com/epicseven/kr/list/1012?page=1&direction=LATEST",
            "stove_korea_bug", force_crawl, "frequent", "korea", processor_func)),
        ('ê¸€ë¡œë²Œ ë²„ê·¸ ê²Œì‹œíŒ', lambda: crawl_stove_board(
            "https://page.onstove.com/epicseven/global/list/998?page=1&direction=LATEST", 
            "stove_global_bug", force_crawl, "frequent", "global", processor_func)),
        ('í•œêµ­ ììœ ê²Œì‹œíŒ', lambda: crawl_stove_board(
            "https://page.onstove.com/epicseven/kr/list/1005?page=1&direction=LATEST",
            "stove_korea_general", force_crawl, "frequent", "korea", processor_func)),
        ('ê¸€ë¡œë²Œ ììœ ê²Œì‹œíŒ', lambda: crawl_stove_board(
            "https://page.onstove.com/epicseven/global/list/989?page=1&direction=LATEST",
            "stove_global_general", force_crawl, "frequent", "global", processor_func)),
        ('Reddit Epic7', lambda: crawl_reddit_epic7(force_crawl, 10, processor_func)),
        ('ë£¨ë¦¬ì›¹ Epic7', lambda: crawl_ruliweb_epic7(processor_func))
    ]

    # ê° ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹¤í–‰ - ì—ëŸ¬ ê²©ë¦¬ë¡œ ì•ˆì •ì„± í™•ë³´
    for site_name, crawl_func in crawl_tasks:
        try:
            print(f"[INFO] ğŸŒ {site_name} í¬ë¡¤ë§ ì‹œì‘...")
            posts = crawl_func()
            all_posts.extend(posts)
            print(f"[SUCCESS] âœ… {site_name} í¬ë¡¤ë§ ì™„ë£Œ: {len(posts)}ê°œ ê²Œì‹œê¸€")
            
            # ì‚¬ì´íŠ¸ ê°„ ëŒ€ê¸° (Rate Limiting)
            time.sleep(random.uniform(2, 5))
            
        except Exception as e:
            print(f"[ERROR] âŒ {site_name} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            # ğŸš€ Master ìš”êµ¬ì‚¬í•­: ì—ëŸ¬ ê²©ë¦¬ - 1ê°œ ì‚¬ì´íŠ¸ ì‹¤íŒ¨í•´ë„ ë‹¤ìŒ ì‚¬ì´íŠ¸ ê³„ì†
            continue

    # Master ìš”êµ¬ì‚¬í•­: ì¬ì‹œë„ í ì²˜ë¦¬
    try:
        immediate_processor.process_retry_queue()
    except Exception as e:
        print(f"[ERROR] ì¬ì‹œë„ í ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

    # ì²˜ë¦¬ í†µê³„ ì¶œë ¥
    stats = immediate_processor.get_stats()
    print(f"[STATS] ğŸ“Š ì¦‰ì‹œ ì²˜ë¦¬ í†µê³„: ì„±ê³µ {stats['processed']}ê°œ, ì‹¤íŒ¨ {stats['failed']}ê°œ, ì¬ì‹œë„ ëŒ€ê¸° {stats['retry_queue']}ê°œ")

    print(f"[INFO] === 15ë¶„ ì£¼ê¸° ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_posts)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘ ===")
    return all_posts

def crawl_regular_sites(force_crawl: bool = False,
                       on_post_process: Optional[Callable[[Dict], None]] = None) -> List[Dict]:
    """
    Phase 1: 30ë¶„ ì£¼ê¸° - ì¼ë°˜ í¬ë¡¤ë§ + ì¦‰ì‹œ ì²˜ë¦¬
    Master ìš”êµ¬ì‚¬í•­: ê²Œì‹œê¸€ë³„ ì¦‰ì‹œ ì²˜ë¦¬ ì§€ì›
    """
    all_posts = []

    print("[INFO] === 30ë¶„ ì£¼ê¸° ì¼ë°˜ í¬ë¡¤ë§ ì‹œì‘ (ì¦‰ì‹œ ì²˜ë¦¬ í†µí•©) ===")

    # on_post_processê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì¦‰ì‹œ ì²˜ë¦¬ê¸° ì‚¬ìš©
    processor_func = on_post_process or immediate_processor.process_post_immediately

    # 30ë¶„ ì£¼ê¸°ìš© ì‚¬ì´íŠ¸ë“¤
    crawl_tasks = [
        ('í•œêµ­ ììœ ê²Œì‹œíŒ', lambda: crawl_stove_board(
            "https://page.onstove.com/epicseven/kr/list/1005?page=1&direction=LATEST",
            "stove_korea_general", force_crawl, "regular", "korea", processor_func)),
        ('ê¸€ë¡œë²Œ ììœ ê²Œì‹œíŒ', lambda: crawl_stove_board(
            "https://page.onstove.com/epicseven/global/list/989?page=1&direction=LATEST",
            "stove_global_general", force_crawl, "regular", "global", processor_func)),
        ('Reddit Epic7', lambda: crawl_reddit_epic7(force_crawl, 15, processor_func)),
        ('ë£¨ë¦¬ì›¹ Epic7', lambda: crawl_ruliweb_epic7(processor_func))
    ]

    # ê° ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹¤í–‰
    for site_name, crawl_func in crawl_tasks:
        try:
            print(f"[INFO] ğŸŒ {site_name} í¬ë¡¤ë§ ì‹œì‘...")
            posts = crawl_func()
            all_posts.extend(posts)
            print(f"[SUCCESS] âœ… {site_name} í¬ë¡¤ë§ ì™„ë£Œ: {len(posts)}ê°œ ê²Œì‹œê¸€")
            
            # ì‚¬ì´íŠ¸ ê°„ ëŒ€ê¸°
            time.sleep(random.uniform(3, 6))
            
        except Exception as e:
            print(f"[ERROR] âŒ {site_name} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            continue

    # ì¬ì‹œë„ í ì²˜ë¦¬
    try:
        immediate_processor.process_retry_queue()
    except Exception as e:
        print(f"[ERROR] ì¬ì‹œë„ í ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

    # ì²˜ë¦¬ í†µê³„ ì¶œë ¥
    stats = immediate_processor.get_stats()
    print(f"[STATS] ğŸ“Š ì¦‰ì‹œ ì²˜ë¦¬ í†µê³„: ì„±ê³µ {stats['processed']}ê°œ, ì‹¤íŒ¨ {stats['failed']}ê°œ, ì¬ì‹œë„ ëŒ€ê¸° {stats['retry_queue']}ê°œ")

    print(f"[INFO] === 30ë¶„ ì£¼ê¸° ì¼ë°˜ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_posts)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘ ===")
    return all_posts

# =============================================================================
# ğŸš€ Master ìš”êµ¬ì‚¬í•­: ìŠ¤ì¼€ì¤„ë§ í†µí•© í•¨ìˆ˜
# =============================================================================

def crawl_by_schedule(schedule_type: str, force_crawl: bool = False,
                     on_post_process: Optional[Callable[[Dict], None]] = None) -> List[Dict]:
    """
    ìŠ¤ì¼€ì¤„ íƒ€ì…ë³„ í¬ë¡¤ë§ ì‹¤í–‰ + ì¦‰ì‹œ ì²˜ë¦¬
    Master ìš”êµ¬ì‚¬í•­: ê²Œì‹œê¸€ë³„ ì¦‰ì‹œ ì²˜ë¦¬ ì™„ì „ ì§€ì›
    """
    print(f"[INFO] ğŸš€ ìŠ¤ì¼€ì¤„ í¬ë¡¤ë§ ì‹œì‘: {schedule_type}")
    
    try:
        if schedule_type in ['frequent', '15min']:
            return crawl_frequent_sites(force_crawl, on_post_process)
        elif schedule_type in ['regular', '30min']:
            return crawl_regular_sites(force_crawl, on_post_process)
        else:
            print(f"[WARNING] ì•Œ ìˆ˜ ì—†ëŠ” ìŠ¤ì¼€ì¤„ íƒ€ì…: {schedule_type}")
            return []
            
    except Exception as e:
        print(f"[ERROR] ìŠ¤ì¼€ì¤„ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

# =============================================================================
# ë¦¬í¬íŠ¸ìš© ê²Œì‹œê¸€ ìˆ˜ì§‘ í•¨ìˆ˜ (ê¸°ì¡´ í˜¸í™˜ì„±)
# =============================================================================

def get_all_posts_for_report() -> List[Dict]:
    """ì¼ê°„ ë¦¬í¬íŠ¸ìš© ê²Œì‹œê¸€ ìˆ˜ì§‘ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)"""
    print("[INFO] ë¦¬í¬íŠ¸ìš© ì „ì²´ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì‹œì‘")
    
    # ì¦‰ì‹œ ì²˜ë¦¬ ì—†ì´ ìˆ˜ì§‘ë§Œ ìˆ˜í–‰
    posts = crawl_frequent_sites(force_crawl=False, on_post_process=None)
    
    print(f"[INFO] ë¦¬í¬íŠ¸ìš© ê²Œì‹œê¸€ ìˆ˜ì§‘ ì™„ë£Œ: {len(posts)}ê°œ")
    return posts

# =============================================================================
# Master ìš”êµ¬ì‚¬í•­ ì™„ë£Œ: ê²Œì‹œê¸€ë³„ ì¦‰ì‹œ ì²˜ë¦¬ í¬ë¡¤ëŸ¬ v4.3
# =============================================================================

if __name__ == "__main__":
    print("ğŸ® Epic7 Crawler v4.3 - ì¦‰ì‹œ ì²˜ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    
    # í…ŒìŠ¤íŠ¸ìš© ì¦‰ì‹œ ì²˜ë¦¬ í•¨ìˆ˜
    def test_immediate_processor(post_data):
        print(f"[TEST] ì¦‰ì‹œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸: {post_data.get('title', '')[:50]}...")
        print(f"[TEST] ì†ŒìŠ¤: {post_data.get('source', '')}")
        print(f"[TEST] URL: {post_data.get('url', '')[:80]}...")
        
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    posts = crawl_frequent_sites(force_crawl=False, on_post_process=test_immediate_processor)
    print(f"[TEST] í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {len(posts)}ê°œ ê²Œì‹œê¸€ ì²˜ë¦¬")