#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Epic7 ë‹¤êµ­ê°€ í¬ë¡¤ëŸ¬ v3.4 - ì™„ì „ ê°œì„  ë²„ì „ (ìµœì¢… ìˆ˜ì •)
- URL ID ì˜¤ë¥˜ ì™„ì „ ìˆ˜ì • (998, 989, 1012, 1005)
- ë‹¤êµ­ê°€ ì§€ì›: ê¸€ë¡œë²Œ/í•œêµ­/Reddit/ë£¨ë¦¬ì›¹
- CSS Selector 30+ í´ë°± ì‹œìŠ¤í…œ (2025ë…„ Stove êµ¬ì¡°)
- JavaScript ë Œë”ë§ ëŒ€ê¸°ì‹œê°„ ìµœì í™” (20ì´ˆ/25ì´ˆ)
- Force Crawl ë²„ê·¸ ìˆ˜ì • ì™„ë£Œ
- ì§€ì—­ë³„ ìŠ¤ì¼€ì¤„ë§ ìµœì í™”
- ìºì‹œ ì‹œìŠ¤í…œ ê°œì„  ë° ë©”ëª¨ë¦¬ ìµœì í™”
- ì†ŒìŠ¤ ì´ë¦„ ì¼ê´€ì„± ì™„ì „ í•´ê²° (stove_korea_* í†µì¼)
"""

import time
import random
import re
import requests
import concurrent.futures
import os
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from urllib.parse import urljoin, urlparse

# Selenium ê´€ë ¨ import
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from selenium.webdriver.chrome.service import Service

# HTML íŒŒì‹±
from bs4 import BeautifulSoup

# ê³µí†µ ëª¨ë“ˆ ì„í¬íŠ¸
from config import config
from utils import setup_logging

# ë¡œê¹… ì„¤ì •
import logging
logger = logging.getLogger(__name__)

# =============================================================================
# í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ ë° ì„¤ì • v3.4
# =============================================================================

class CrawlingSchedule:
    """í¬ë¡¤ë§ ì£¼ê¸° ì„¤ì • - ë‹¤êµ­ê°€ ì§€ì›"""
    
    # í¬ë¡¤ë§ ì£¼ê¸° (ë¶„)
    FREQUENT_INTERVAL = 15    # ë²„ê·¸ ê²Œì‹œíŒ (ê¸´ê¸‰)
    REGULAR_INTERVAL = 30     # ì¼ë°˜ ê²Œì‹œíŒ
    
    # ëŒ€ê¸° ì‹œê°„ ì„¤ì • (ì§€ì—­ë³„ ìµœì í™”)
    FREQUENT_WAIT_TIME = 20   # ë²„ê·¸ ê²Œì‹œíŒ ëŒ€ê¸°ì‹œê°„
    REGULAR_WAIT_TIME = 25    # ì¼ë°˜ ê²Œì‹œíŒ ëŒ€ê¸°ì‹œê°„
    REDDIT_WAIT_TIME = 15     # Reddit ëŒ€ê¸°ì‹œê°„
    RULIWEB_WAIT_TIME = 18    # ë£¨ë¦¬ì›¹ ëŒ€ê¸°ì‹œê°„
    
    # ìŠ¤í¬ë¡¤ ì„¤ì •
    FREQUENT_SCROLL_COUNT = 3 # ë²„ê·¸ ê²Œì‹œíŒ ìŠ¤í¬ë¡¤
    REGULAR_SCROLL_COUNT = 5  # ì¼ë°˜ ê²Œì‹œíŒ ìŠ¤í¬ë¡¤

class BoardConfig:
    """ê²Œì‹œíŒ ì„¤ì • - URL ID ìˆ˜ì • ì™„ë£Œ"""
    
    # ê¸€ë¡œë²Œ ê²Œì‹œíŒ (ìˆ˜ì •ë¨)
    GLOBAL_BOARDS = {
        'bug': {
            'id': '998',  # e7en001 â†’ 998 ìˆ˜ì •
            'url': 'https://page.onstove.com/epicseven/global/list/998',
            'name': 'Global Bug Report',
            'schedule': 'frequent'
        },
        'general': {
            'id': '989',  # e7en002 â†’ 989 ìˆ˜ì •  
            'url': 'https://page.onstove.com/epicseven/global/list/989',
            'name': 'Global General Discussion',
            'schedule': 'regular'
        }
    }
    
    # í•œêµ­ ê²Œì‹œíŒ (ì‹ ê·œ ì¶”ê°€)
    KOREAN_BOARDS = {
        'bug': {
            'id': '1012',
            'url': 'https://page.onstove.com/epicseven/kr/list/1012',
            'name': 'Korean Bug Report',
            'schedule': 'frequent'
        },
        'general': {
            'id': '1005',
            'url': 'https://page.onstove.com/epicseven/kr/list/1005',
            'name': 'Korean General Discussion',
            'schedule': 'regular'
        }
    }
    
    # ì™¸ë¶€ ì‚¬ì´íŠ¸ (ì‹ ê·œ ì¶”ê°€)
    EXTERNAL_SITES = {
        'reddit': {
            'url': 'https://www.reddit.com/r/EpicSeven/',
            'name': 'Reddit r/EpicSeven',
            'schedule': 'regular'
        },
        'ruliweb': {
            'url': 'https://bbs.ruliweb.com/game/84834',
            'name': 'Ruliweb Epic Seven',
            'schedule': 'regular'
        }
    }

# =============================================================================
# Chrome ë“œë¼ì´ë²„ ê´€ë¦¬ v3.4
# =============================================================================

def get_chrome_driver(headless: bool = True, region: str = 'global') -> webdriver.Chrome:
    """
    Chrome ë“œë¼ì´ë²„ ìƒì„± - 3ë‹¨ê³„ í´ë°± ë©”ì»¤ë‹ˆì¦˜ (ì§€ì—­ ìµœì í™”)
    
    Args:
        headless: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì—¬ë¶€
        region: ì§€ì—­ ì„¤ì • ('global', 'korean', 'reddit', 'ruliweb')
        
    Returns:
        webdriver.Chrome: Chrome ë“œë¼ì´ë²„ ì¸ìŠ¤í„´ìŠ¤
    """
    chrome_options = Options()
    
    if headless:
        chrome_options.add_argument('--headless=new')
    
    # ê¸°ë³¸ ì˜µì…˜ ì„¤ì •
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--window-size=1920,1080')
    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    
    # ì§€ì—­ë³„ User-Agent ìµœì í™”
    user_agents = {
        'global': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
        'korean': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
        'reddit': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
        'ruliweb': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36'
    }
    
    chrome_options.add_argument(f'--user-agent={user_agents.get(region, user_agents["global"])}')
    
    # Stage 1: ì‹œìŠ¤í…œ ê²½ë¡œì—ì„œ Chrome/ChromeDriver ì°¾ê¸°
    possible_paths = [
        '/usr/bin/google-chrome',
        '/usr/bin/google-chrome-stable',
        '/usr/bin/chromium-browser',
        '/snap/bin/chromium',
        'google-chrome',
        'chromium-browser'
    ]
    
    for chrome_path in possible_paths:
        try:
            chrome_options.binary_location = chrome_path
            driver = webdriver.Chrome(options=chrome_options)
            logger.info(f"Chrome ë“œë¼ì´ë²„ ì„±ê³µ (Stage 1, {region}): {chrome_path}")
            return driver
        except Exception as e:
            logger.debug(f"Chrome ê²½ë¡œ ì‹¤íŒ¨: {chrome_path} - {str(e)}")
            continue
    
    # Stage 2: WebDriver Manager ì‚¬ìš©
    try:
        from webdriver_manager.chrome import ChromeDriverManager
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        logger.info(f"Chrome ë“œë¼ì´ë²„ ì„±ê³µ (Stage 2, {region}): WebDriver Manager")
        return driver
    except Exception as e:
        logger.debug(f"WebDriver Manager ì‹¤íŒ¨: {str(e)}")
    
    # Stage 3: ì§ì ‘ ë‹¤ìš´ë¡œë“œ ì‹œë„
    try:
        driver = webdriver.Chrome(options=chrome_options)
        logger.info(f"Chrome ë“œë¼ì´ë²„ ì„±ê³µ (Stage 3, {region}): ì§ì ‘ ì‹¤í–‰")
        return driver
    except Exception as e:
        logger.error(f"ëª¨ë“  Chrome ë“œë¼ì´ë²„ ì‹œë„ ì‹¤íŒ¨: {str(e)}")
        raise Exception("Chrome ë“œë¼ì´ë²„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

# =============================================================================
# ì¤‘ë³µ ë°©ì§€ ì‹œìŠ¤í…œ v3.4 (ë©”ëª¨ë¦¬ ìµœì í™”)
# =============================================================================

def load_crawled_links() -> Dict:
    """í¬ë¡¤ë§ëœ ë§í¬ ëª©ë¡ ë¡œë“œ (ë©”ëª¨ë¦¬ ìµœì í™”)"""
    try:
        if os.path.exists('crawled_links.json'):
            with open('crawled_links.json', 'r', encoding='utf-8') as f:
                data = json.load(f)
                # ìµœëŒ€ 1500ê°œê¹Œì§€ë§Œ ìœ ì§€ (2000â†’1500 ìµœì í™”)
                if len(data.get('links', [])) > 1500:
                    data['links'] = data['links'][-1500:]
                return data
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ë§í¬ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
    
    return {
        'links': [],
        'last_updated': datetime.now().isoformat()
    }

def save_crawled_links(crawled_data: Dict) -> None:
    """í¬ë¡¤ë§ëœ ë§í¬ ëª©ë¡ ì €ì¥"""
    try:
        crawled_data['last_updated'] = datetime.now().isoformat()
        with open('crawled_links.json', 'w', encoding='utf-8') as f:
            json.dump(crawled_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ë§í¬ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

# =============================================================================
# ì½˜í…ì¸  ìºì‹œ ì‹œìŠ¤í…œ v3.4 (ê°œì„ ë¨)
# =============================================================================

def load_content_cache() -> Dict:
    """ì½˜í…ì¸  ìºì‹œ ë¡œë“œ (ê°œì„ ëœ ì •ë¦¬ ë¡œì§)"""
    try:
        if os.path.exists('content_cache.json'):
            with open('content_cache.json', 'r', encoding='utf-8') as f:
                cache = json.load(f)
                
                # 12ì‹œê°„ ì´ìƒ ëœ ìºì‹œ ì œê±° (24ì‹œê°„â†’12ì‹œê°„ ìµœì í™”)
                current_time = datetime.now()
                cleaned_cache = {}
                
                for url_hash, cached_data in cache.items():
                    if 'timestamp' in cached_data:
                        try:
                            cached_time = datetime.fromisoformat(cached_data['timestamp'])
                            if (current_time - cached_time).total_seconds() < 12 * 3600:
                                cleaned_cache[url_hash] = cached_data
                        except:
                            # íŒŒì‹± ì‹¤íŒ¨í•œ ìºì‹œëŠ” ì œê±°
                            continue
                
                return cleaned_cache
    except Exception as e:
        logger.error(f"ì½˜í…ì¸  ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
    
    return {}

def save_content_cache(cache_data: Dict) -> None:
    """ì½˜í…ì¸  ìºì‹œ ì €ì¥ (ë©”ëª¨ë¦¬ ìµœì í™”)"""
    try:
        # ìµœëŒ€ 800ê°œê¹Œì§€ë§Œ ìœ ì§€ (1000â†’800 ìµœì í™”)
        if len(cache_data) > 800:
            sorted_items = sorted(
                cache_data.items(),
                key=lambda x: x[1].get('timestamp', ''),
                reverse=True
            )
            cache_data = dict(sorted_items[:800])
        
        with open('content_cache.json', 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"ì½˜í…ì¸  ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

# =============================================================================
# Stove ê²Œì‹œê¸€ ë³¸ë¬¸ ì¶”ì¶œ v3.4 (CSS Selector ëŒ€í­ ê°•í™”)
# =============================================================================

def get_stove_post_content(post_url: str, driver: webdriver.Chrome = None, source: str = "", schedule_type: str = 'frequent') -> Tuple[str, str]:
    """
    Stove ê²Œì‹œê¸€ ë³¸ë¬¸ ì¶”ì¶œ v3.4 (CSS Selector 30+ í´ë°±)
    """
    # URL í•´ì‹œ ìƒì„± (ìºì‹œ í‚¤)
    import hashlib
    url_hash = hashlib.md5(post_url.encode()).hexdigest()
    
    # ìºì‹œì—ì„œ í™•ì¸
    content_cache = load_content_cache()
    if url_hash in content_cache:
        cached_data = content_cache[url_hash]
        logger.debug(f"ìºì‹œì—ì„œ ì½˜í…ì¸  ë¡œë“œ: {post_url[:50]}...")
        return cached_data.get('content', ''), cached_data.get('summary', '')
    
    # ìƒˆë¡œìš´ ë“œë¼ì´ë²„ ìƒì„± ì—¬ë¶€
    should_quit_driver = False
    if driver is None:
        region = 'korean' if '/kr/' in post_url else 'global'
        driver = get_chrome_driver(region=region)
        should_quit_driver = True
    
    content = ""
    content_summary = ""
    
    try:
        logger.debug(f"ê²Œì‹œê¸€ ë³¸ë¬¸ ì¶”ì¶œ ì‹œì‘: {post_url}")
        driver.get(post_url)
        
        # ìŠ¤ì¼€ì¤„ì— ë”°ë¥¸ ëŒ€ê¸°ì‹œê°„ ì„¤ì •
        if schedule_type == 'frequent':
            wait_time = CrawlingSchedule.FREQUENT_WAIT_TIME
            scroll_count = CrawlingSchedule.FREQUENT_SCROLL_COUNT
        else:
            wait_time = CrawlingSchedule.REGULAR_WAIT_TIME
            scroll_count = CrawlingSchedule.REGULAR_SCROLL_COUNT
        
        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        time.sleep(wait_time)
        
        # DOM ì¤€ë¹„ ìƒíƒœ í™•ì¸
        WebDriverWait(driver, 30).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        
        # ìŠ¤í¬ë¡¤í•˜ì—¬ ë™ì  ì½˜í…ì¸  ë¡œë“œ
        for i in range(scroll_count):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
        
        # ìƒë‹¨ìœ¼ë¡œ ìŠ¤í¬ë¡¤
        driver.execute_script("window.scrollTo(0, 0);")
        time.sleep(1)
        
        # ê°•í™”ëœ CSS Selector ëª©ë¡ (30+ í´ë°±)
        content_selectors = [
            # 2025ë…„ ìµœì‹  Stove êµ¬ì¡° (ìš°ì„ ìˆœìœ„)
            'div[class*="article-content"]',      
            'div[class*="post-content"]',         
            'div[class*="board-content"]',        
            'section[class*="content"]',          
            'div[class*="text-content"]',         
            'div[class*="content-body"]',         
            'div[class*="post-body"]',            
            'div[class*="article-body"]',         
            
            # ê¸°ì¡´ selector ìœ ì§€ (í•˜ìœ„ í˜¸í™˜ì„±)
            'div.s-article-content',
            'div.s-article-content-text',
            'section.s-article-body',
            'div.s-board-content',
            'div.article-content',
            'div.post-content',
            
            # React/Vue êµ¬ì¡° ëŒ€ì‘
            'div[data-testid*="content"]',
            'div[data-testid*="post"]',
            'div[data-testid*="article"]',
            'section[data-testid*="content"]',
            
            # ID ê¸°ë°˜ selector
            '#content',
            '#post-content',
            '#article-content',
            '#main-content',
            
            # í¬ê´„ì  selector (ìˆœì„œ ì¤‘ìš”)
            'main [class*="content"]',            
            'article [class*="content"]',         
            '[id*="content"]',                    
            'div[class*="body"]',                 
            '.content',                           
            '.post',                              
            '.article',                           
            
            # ë§ˆì§€ë§‰ ìˆ˜ë‹¨ë“¤
            'main article',
            'main section',
            'main div:not([class*="header"]):not([class*="nav"]):not([class*="footer"])',
            'p'  # ìµœí›„ì˜ ìˆ˜ë‹¨
        ]
        
        # CSS Selectorë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„
        for selector in content_selectors:
            try:
                elements = driver.find_elements(By.CSS_SELECTOR, selector)
                logger.debug(f"Selector {selector}: {len(elements)}ê°œ ìš”ì†Œ ë°œê²¬")
                
                if elements:
                    for element in elements:
                        try:
                            text = element.get_attribute('innerText') or element.text
                            text = text.strip()
                            
                            # í…ìŠ¤íŠ¸ ê¸¸ì´ ì¡°ê±´: 10ì ì´ìƒ
                            if text and len(text) >= 10:
                                # ì˜ë¯¸ì—†ëŠ” í…ìŠ¤íŠ¸ í•„í„°ë§ (í™•ì¥ë¨)
                                skip_phrases = [
                                    'ë¡œê·¸ì¸', 'ì„¤ì¹˜', 'ê´‘ê³ ', 'ì¿ í‚¤', 'cookie',
                                    'ì´ìš©ì•½ê´€', 'ê°œì¸ì •ë³´', 'ì €ì‘ê¶Œ', 'ë¬´ë‹¨ì „ì¬',
                                    'ëŒ“ê¸€', 'comment', 'ì¢‹ì•„ìš”', 'like', 'ê³µìœ ',
                                    'javascript', 'css', 'loading', 'submit',
                                    'checkbox', 'radio', 'button', 'form'
                                ]
                                
                                if not any(phrase in text.lower() for phrase in skip_phrases):
                                    content = text
                                    
                                    # ì²« ë²ˆì§¸ ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ì„ ìš”ì•½ìœ¼ë¡œ ì‚¬ìš©
                                    lines = text.split('\n')
                                    for line in lines:
                                        line = line.strip()
                                        if len(line) >= 15:  # ìš”ì•½ì€ 15ì ì´ìƒ
                                            content_summary = line[:200]  # ìµœëŒ€ 200ì
                                            break
                                    
                                    logger.info(f"ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ ({selector}): {len(content)}ì")
                                    break
                        except Exception as e:
                            logger.debug(f"ìš”ì†Œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
                            continue
                
                if content:
                    break
                    
            except Exception as e:
                logger.debug(f"Selector {selector} ì˜¤ë¥˜: {str(e)}")
                continue
        
        # ì½˜í…ì¸ ê°€ ì¶”ì¶œë˜ì§€ ì•Šì€ ê²½ìš°
        if not content:
            logger.warning(f"ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {post_url}")
            content = "ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”."
            content_summary = "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"
        
        # ìºì‹œì— ì €ì¥
        content_cache[url_hash] = {
            'content': content,
            'summary': content_summary,
            'timestamp': datetime.now().isoformat(),
            'url': post_url,
            'source': source
        }
        save_content_cache(content_cache)
        
    except Exception as e:
        logger.error(f"ê²Œì‹œê¸€ ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜ {post_url}: {str(e)}")
        content = f"ë³¸ë¬¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        content_summary = "ì˜¤ë¥˜ ë°œìƒ"
    
    finally:
        if should_quit_driver and driver:
            try:
                driver.quit()
            except:
                pass
    
    return content, content_summary

# =============================================================================
# Stove ê²Œì‹œíŒ í¬ë¡¤ë§ v3.4 (ì†ŒìŠ¤ ì´ë¦„ ìµœì¢… ìˆ˜ì •)
# =============================================================================

def fetch_stove_bug_board(force_crawl: bool = False, schedule_type: str = 'frequent', region: str = 'global') -> List[Dict]:
    """Stove ì—í”½ì„¸ë¸ ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§ v3.4 (ì†ŒìŠ¤ ì´ë¦„ ìµœì¢… ìˆ˜ì •)"""
    
    # ì§€ì—­ë³„ URL ì„¤ì • (ì†ŒìŠ¤ ì´ë¦„ ìˆ˜ì • ì™„ë£Œ)
    if region == 'global':
        url = BoardConfig.GLOBAL_BOARDS['bug']['url']
        source = "stove_global_bug"
    else:  # korean
        url = BoardConfig.KOREAN_BOARDS['bug']['url']
        source = "stove_korea_bug"  # stove_korean_bug â†’ stove_korea_bug ìˆ˜ì •
    
    return crawl_stove_board(url, source, force_crawl, schedule_type, region)

def fetch_stove_general_board(force_crawl: bool = False, schedule_type: str = 'regular', region: str = 'global') -> List[Dict]:
    """Stove ì—í”½ì„¸ë¸ ì¼ë°˜ ê²Œì‹œíŒ í¬ë¡¤ë§ v3.4 (ì†ŒìŠ¤ ì´ë¦„ ìµœì¢… ìˆ˜ì •)"""
    
    # ì§€ì—­ë³„ URL ì„¤ì • (ì†ŒìŠ¤ ì´ë¦„ ìˆ˜ì • ì™„ë£Œ)
    if region == 'global':
        url = BoardConfig.GLOBAL_BOARDS['general']['url']
        source = "stove_global_general"
    else:  # korean
        url = BoardConfig.KOREAN_BOARDS['general']['url']
        source = "stove_korea_general"  # stove_korean_general â†’ stove_korea_general ìˆ˜ì •
    
    return crawl_stove_board(url, source, force_crawl, schedule_type, region)

def crawl_stove_board(board_url: str, source: str, force_crawl: bool = False, schedule_type: str = 'frequent', region: str = 'global') -> List[Dict]:
    """Stove ê²Œì‹œíŒ í¬ë¡¤ë§ (í†µí•©) v3.4"""
    posts = []
    driver = None
    
    try:
        logger.info(f"Stove ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹œì‘: {source}" + (f" (Force Crawl)" if force_crawl else ""))
        
        # ì¤‘ë³µ ë°©ì§€ ì‹œìŠ¤í…œ ë¡œë“œ
        crawled_data = load_crawled_links()
        crawled_links = set(crawled_data.get('links', []))
        
        driver = get_chrome_driver(region=region)
        driver.get(board_url)
        
        # ìŠ¤ì¼€ì¤„ì— ë”°ë¥¸ ëŒ€ê¸°ì‹œê°„ ì„¤ì •
        if schedule_type == 'frequent':
            wait_time = CrawlingSchedule.FREQUENT_WAIT_TIME
        else:
            wait_time = CrawlingSchedule.REGULAR_WAIT_TIME
        
        time.sleep(wait_time)
        
        # DOM ì¤€ë¹„ ìƒíƒœ í™•ì¸
        WebDriverWait(driver, 30).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        
        # ê°•í™”ëœ JavaScriptë¡œ ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ
        posts_script = """
        const posts = [];
        
        // ë‹¤ì–‘í•œ ì…€ë ‰í„° ì‹œë„ (ìˆœì„œ ì¤‘ìš”)
        const selectors = [
            'h3.s-board-title',           // ê¸°ì¡´ selector
            '[class*="board-title"]',      // í´ë˜ìŠ¤ëª… í¬í•¨
            '[class*="post-title"]',       // post-title í¬í•¨
            '[class*="article-title"]',    // article-title í¬í•¨
            'h3[class*="title"]',         // h3 íƒœê·¸ title í¬í•¨
            'a[href*="/view/"]'           // view ë§í¬ ì§ì ‘ ì°¾ê¸°
        ];
        
        for (const selector of selectors) {
            const elements = document.querySelectorAll(selector);
            
            if (elements.length > 0) {
                elements.forEach((element, index) => {
                    try {
                        let linkElement, titleElement, href, title;
                        
                        if (selector === 'a[href*="/view/"]') {
                            linkElement = element;
                            titleElement = element;
                        } else {
                            linkElement = element.querySelector('a') || element.querySelector('span.s-board-title-text')?.parentElement;
                            titleElement = element.querySelector('span.s-board-title-text') || element;
                        }
                        
                        if (linkElement && titleElement) {
                            href = linkElement.href || linkElement.getAttribute('href');
                            title = titleElement.textContent?.trim() || titleElement.innerText?.trim();
                            
                            if (href && title && title.length > 2) {
                                const fullUrl = href.startsWith('http') ? href : 'https://page.onstove.com' + href;
                                
                                // ID ì¶”ì¶œ
                                const idMatch = fullUrl.match(/view\/(\d+)/);
                                const id = idMatch ? idMatch[1] : '';
                                
                                // ê³µì§€ì‚¬í•­ ì œì™¸
                                const noticeElement = element.querySelector('i.element-badge__s.notice, i.element-badge__s.event, [class*="notice"], [class*="event"]');
                                if (!noticeElement && !title.includes('[ê³µì§€]') && !title.includes('[ì´ë²¤íŠ¸]')) {
                                    posts.push({
                                        href: fullUrl,
                                        id: id,
                                        title: title,
                                        selector_used: selector
                                    });
                                }
                            }
                        }
                    } catch (e) {
                        console.log('ê²Œì‹œê¸€ ì¶”ì¶œ ì˜¤ë¥˜:', e);
                    }
                });
                
                if (posts.length > 0) {
                    console.log(`ì„±ê³µí•œ selector: ${selector}, ê²Œì‹œê¸€ ìˆ˜: ${posts.length}`);
                    break;  // ì„±ê³µí•˜ë©´ ë‹¤ìŒ ì…€ë ‰í„° ì‹œë„í•˜ì§€ ì•ŠìŒ
                }
            }
        }
        
        return posts;
        """
        
        js_posts = driver.execute_script(posts_script)
        logger.info(f"JavaScript í¬ë¡¤ë§ ì„±ê³µ: {len(js_posts)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")
        
        if not js_posts:
            logger.warning("JavaScriptë¡œ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return []
        
        # ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
        for post_data in js_posts:
            href = post_data.get('href', '')
            title = post_data.get('title', '')
            post_id = post_data.get('id', '')
            
            if not href or not title:
                continue
            
            # Force Crawlì´ ì•„ë‹ ë•Œ ì¤‘ë³µ ì²´í¬
            if not force_crawl and href in crawled_links:
                logger.debug(f"ì¤‘ë³µ ê²Œì‹œê¸€ ìŠ¤í‚µ: {title[:30]}...")
                continue
            
            try:
                # ê²Œì‹œê¸€ ë³¸ë¬¸ ì¶”ì¶œ
                content, content_summary = get_stove_post_content(
                    href, driver, source, schedule_type
                )
                
                post = {
                    'title': title,
                    'url': href,
                    'content': content,
                    'content_summary': content_summary,
                    'timestamp': datetime.now().isoformat(),
                    'source': source,
                    'post_id': post_id,
                    'region': region
                }
                
                posts.append(post)
                
                # ìƒˆ ë§í¬ ì¶”ê°€ (ì¤‘ë³µ ë°©ì§€)
                if href not in crawled_links:
                    crawled_data['links'].append(href)
                    crawled_links.add(href)
                
                logger.debug(f"ê²Œì‹œê¸€ ìˆ˜ì§‘ ì™„ë£Œ: {title[:40]}...")
                
                # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ì§€ì—°
                time.sleep(1)
                
            except Exception as e:
                logger.error(f"ê²Œì‹œê¸€ ì²˜ë¦¬ ì˜¤ë¥˜ {href}: {str(e)}")
                continue
        
        # ì¤‘ë³µ ë°©ì§€ ë°ì´í„° ì €ì¥
        save_crawled_links(crawled_data)
        
        # ë””ë²„ê·¸ HTML ì €ì¥
        try:
            with open(f'{source}_debug_selenium.html', 'w', encoding='utf-8') as f:
                f.write(driver.page_source)
        except:
            pass
        
        logger.info(f"Stove {region} ê²Œì‹œíŒ í¬ë¡¤ë§ ì™„ë£Œ: {len(posts)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘")
        
    except Exception as e:
        logger.error(f"Stove ê²Œì‹œíŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
        return []
    
    finally:
        if driver:
            try:
                driver.quit()
            except:
                pass
    
    return posts

# =============================================================================
# Reddit í¬ë¡¤ë§ v3.4 (ì‹ ê·œ ì¶”ê°€)
# =============================================================================

def fetch_reddit_posts(force_crawl: bool = False, schedule_type: str = 'regular') -> List[Dict]:
    """Reddit r/EpicSeven í¬ë¡¤ë§"""
    posts = []
    driver = None
    
    try:
        logger.info("Reddit í¬ë¡¤ë§ ì‹œì‘" + (f" (Force Crawl)" if force_crawl else ""))
        
        # ì¤‘ë³µ ë°©ì§€ ì‹œìŠ¤í…œ ë¡œë“œ
        crawled_data = load_crawled_links()
        crawled_links = set(crawled_data.get('links', []))
        
        driver = get_chrome_driver(region='reddit')
        driver.get(BoardConfig.EXTERNAL_SITES['reddit']['url'])
        
        time.sleep(CrawlingSchedule.REDDIT_WAIT_TIME)
        
        # DOM ì¤€ë¹„ ìƒíƒœ í™•ì¸
        WebDriverWait(driver, 30).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        
        # Reddit ê²Œì‹œê¸€ ì¶”ì¶œ
        reddit_script = """
        const posts = [];
        const postElements = document.querySelectorAll('[data-testid="post-container"], .Post, [class*="Post"]');
        
        postElements.forEach((element, index) => {
            try {
                const titleElement = element.querySelector('h3, [data-testid*="post-title"], [class*="title"]');
                const linkElement = element.querySelector('a[href*="/comments/"]');
                
                if (titleElement && linkElement) {
                    const title = titleElement.textContent.trim();
                    const href = linkElement.href;
                    
                    if (title && href && title.length > 5) {
                        posts.push({
                            href: href,
                            title: title,
                            id: href.match(/comments\/([^\/]+)/)?.[1] || ''
                        });
                    }
                }
            } catch (e) {
                console.log('Reddit ê²Œì‹œê¸€ ì¶”ì¶œ ì˜¤ë¥˜:', e);
            }
        });
        
        return posts;
        """
        
        js_posts = driver.execute_script(reddit_script)
        logger.info(f"Reddit í¬ë¡¤ë§ ì„±ê³µ: {len(js_posts)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")
        
        # ê²Œì‹œê¸€ ì²˜ë¦¬ (ê¸°ë³¸ ë¡œì§ê³¼ ë™ì¼)
        for post_data in js_posts:
            href = post_data.get('href', '')
            title = post_data.get('title', '')
            post_id = post_data.get('id', '')
            
            if not href or not title:
                continue
            
            if not force_crawl and href in crawled_links:
                continue
            
            try:
                # Redditì€ ë³¸ë¬¸ ì¶”ì¶œ ëŒ€ì‹  ì œëª©ë§Œ ì‚¬ìš©
                content = f"Reddit ê²Œì‹œê¸€ - ìì„¸í•œ ë‚´ìš©ì€ ë§í¬ ì°¸ì¡°"
                content_summary = title[:100]
                
                post = {
                    'title': title,
                    'url': href,
                    'content': content,
                    'content_summary': content_summary,
                    'timestamp': datetime.now().isoformat(),
                    'source': 'reddit',
                    'post_id': post_id,
                    'region': 'reddit'
                }
                
                posts.append(post)
                
                if href not in crawled_links:
                    crawled_data['links'].append(href)
                    crawled_links.add(href)
                
                time.sleep(0.5)  # Redditì€ ë” ì§§ì€ ì§€ì—°
                
            except Exception as e:
                logger.error(f"Reddit ê²Œì‹œê¸€ ì²˜ë¦¬ ì˜¤ë¥˜ {href}: {str(e)}")
                continue
        
        save_crawled_links(crawled_data)
        logger.info(f"Reddit í¬ë¡¤ë§ ì™„ë£Œ: {len(posts)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘")
        
    except Exception as e:
        logger.error(f"Reddit í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
        return []
    
    finally:
        if driver:
            try:
                driver.quit()
            except:
                pass
    
    return posts

# =============================================================================
# ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ v3.4 (ì‹ ê·œ ì¶”ê°€)
# =============================================================================

def fetch_ruliweb_posts(force_crawl: bool = False, schedule_type: str = 'regular') -> List[Dict]:
    """ë£¨ë¦¬ì›¹ ì—í”½ì„¸ë¸ ê²Œì‹œíŒ í¬ë¡¤ë§"""
    posts = []
    driver = None
    
    try:
        logger.info("ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì‹œì‘" + (f" (Force Crawl)" if force_crawl else ""))
        
        crawled_data = load_crawled_links()
        crawled_links = set(crawled_data.get('links', []))
        
        driver = get_chrome_driver(region='ruliweb')
        driver.get(BoardConfig.EXTERNAL_SITES['ruliweb']['url'])
        
        time.sleep(CrawlingSchedule.RULIWEB_WAIT_TIME)
        
        # DOM ì¤€ë¹„ ìƒíƒœ í™•ì¸
        WebDriverWait(driver, 30).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        
        # ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ì¶”ì¶œ
        ruliweb_script = """
        const posts = [];
        const postElements = document.querySelectorAll('.board_list_table tr, [class*="list"] tr, [class*="board"] tr');
        
        postElements.forEach((element, index) => {
            try {
                const titleElement = element.querySelector('.subject a, [class*="subject"] a, [class*="title"] a');
                
                if (titleElement) {
                    const title = titleElement.textContent.trim();
                    const href = titleElement.href;
                    
                    if (title && href && title.length > 3 && !title.includes('[ê³µì§€]')) {
                        posts.push({
                            href: href,
                            title: title,
                            id: href.match(/board\/(\d+)/)?.[1] || ''
                        });
                    }
                }
            } catch (e) {
                console.log('ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ì¶”ì¶œ ì˜¤ë¥˜:', e);
            }
        });
        
        return posts;
        """
        
        js_posts = driver.execute_script(ruliweb_script)
        logger.info(f"ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì„±ê³µ: {len(js_posts)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")
        
        # ê²Œì‹œê¸€ ì²˜ë¦¬
        for post_data in js_posts:
            href = post_data.get('href', '')
            title = post_data.get('title', '')
            post_id = post_data.get('id', '')
            
            if not href or not title:
                continue
            
            if not force_crawl and href in crawled_links:
                continue
            
            try:
                content = f"ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ - ìì„¸í•œ ë‚´ìš©ì€ ë§í¬ ì°¸ì¡°"
                content_summary = title[:100]
                
                post = {
                    'title': title,
                    'url': href,
                    'content': content,
                    'content_summary': content_summary,
                    'timestamp': datetime.now().isoformat(),
                    'source': 'ruliweb',
                    'post_id': post_id,
                    'region': 'ruliweb'
                }
                
                posts.append(post)
                
                if href not in crawled_links:
                    crawled_data['links'].append(href)
                    crawled_links.add(href)
                
                time.sleep(0.5)
                
            except Exception as e:
                logger.error(f"ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ì²˜ë¦¬ ì˜¤ë¥˜ {href}: {str(e)}")
                continue
        
        save_crawled_links(crawled_data)
        logger.info(f"ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì™„ë£Œ: {len(posts)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘")
        
    except Exception as e:
        logger.error(f"ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
        return []
    
    finally:
        if driver:
            try:
                driver.quit()
            except:
                pass
    
    return posts

# =============================================================================
# ì£¼ê¸°ë³„ í¬ë¡¤ë§ ì‹¤í–‰ í•¨ìˆ˜ë“¤ v3.4 (ë‹¤êµ­ê°€ ì§€ì›)
# =============================================================================

def crawl_frequent_sites(force_crawl: bool = False) -> List[Dict]:
    """15ë¶„ ì£¼ê¸° í¬ë¡¤ë§ (ë²„ê·¸ ê²Œì‹œíŒ) - ë‹¤êµ­ê°€ ì§€ì›"""
    logger.info("ğŸ”¥ === 15ë¶„ ê°„ê²© í¬ë¡¤ë§ ì‹œì‘ (ê¸€ë¡œë²Œ+í•œêµ­ ë²„ê·¸ ê²Œì‹œíŒ" + (", force_crawl=True)" if force_crawl else ") ==="))
    
    all_posts = []
    
    # ê¸€ë¡œë²Œ ë²„ê·¸ ê²Œì‹œíŒ
    try:
        global_posts = fetch_stove_bug_board(force_crawl, 'frequent', 'global')
        all_posts.extend(global_posts)
        logger.info(f"ê¸€ë¡œë²Œ ë²„ê·¸ ê²Œì‹œíŒ: {len(global_posts)}ê°œ ê²Œì‹œê¸€")
    except Exception as e:
        logger.error(f"ê¸€ë¡œë²Œ ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
    
    # í•œêµ­ ë²„ê·¸ ê²Œì‹œíŒ
    try:
        korean_posts = fetch_stove_bug_board(force_crawl, 'frequent', 'korean')
        all_posts.extend(korean_posts)
        logger.info(f"í•œêµ­ ë²„ê·¸ ê²Œì‹œíŒ: {len(korean_posts)}ê°œ ê²Œì‹œê¸€")
    except Exception as e:
        logger.error(f"í•œêµ­ ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
    
    logger.info(f"ğŸ”¥ === 15ë¶„ ê°„ê²© í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_posts)}ê°œ ê²Œì‹œê¸€ ===")
    return all_posts

def crawl_regular_sites(force_crawl: bool = False) -> List[Dict]:
    """30ë¶„ ì£¼ê¸° í¬ë¡¤ë§ (ì¼ë°˜ ê²Œì‹œíŒ + ì™¸ë¶€ ì‚¬ì´íŠ¸) - ë‹¤êµ­ê°€ ì§€ì›"""
    logger.info("â° === 30ë¶„ ê°„ê²© í¬ë¡¤ë§ ì‹œì‘ (ëª¨ë“  ê²Œì‹œíŒ" + (", force_crawl=True)" if force_crawl else ") ==="))
    
    all_posts = []
    
    # ê¸€ë¡œë²Œ ì¼ë°˜ ê²Œì‹œíŒ
    try:
        global_posts = fetch_stove_general_board(force_crawl, 'regular', 'global')
        all_posts.extend(global_posts)
        logger.info(f"ê¸€ë¡œë²Œ ì¼ë°˜ ê²Œì‹œíŒ: {len(global_posts)}ê°œ ê²Œì‹œê¸€")
    except Exception as e:
        logger.error(f"ê¸€ë¡œë²Œ ì¼ë°˜ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
    
    # í•œêµ­ ì¼ë°˜ ê²Œì‹œíŒ
    try:
        korean_posts = fetch_stove_general_board(force_crawl, 'regular', 'korean')
        all_posts.extend(korean_posts)
        logger.info(f"í•œêµ­ ì¼ë°˜ ê²Œì‹œíŒ: {len(korean_posts)}ê°œ ê²Œì‹œê¸€")
    except Exception as e:
        logger.error(f"í•œêµ­ ì¼ë°˜ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
    
    # Reddit
    try:
        reddit_posts = fetch_reddit_posts(force_crawl, 'regular')
        all_posts.extend(reddit_posts)
        logger.info(f"Reddit: {len(reddit_posts)}ê°œ ê²Œì‹œê¸€")
    except Exception as e:
        logger.error(f"Reddit í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
    
    # ë£¨ë¦¬ì›¹
    try:
        ruliweb_posts = fetch_ruliweb_posts(force_crawl, 'regular')
        all_posts.extend(ruliweb_posts)
        logger.info(f"ë£¨ë¦¬ì›¹: {len(ruliweb_posts)}ê°œ ê²Œì‹œê¸€")
    except Exception as e:
        logger.error(f"ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
    
    logger.info(f"â° === 30ë¶„ ê°„ê²© í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_posts)}ê°œ ê²Œì‹œê¸€ ===")
    return all_posts

def crawl_by_schedule(current_time: datetime = None, force_crawl: bool = False) -> List[Dict]:
    """ìŠ¤ì¼€ì¤„ì— ë”°ë¥¸ í¬ë¡¤ë§ ì‹¤í–‰ v3.4"""
    if current_time is None:
        current_time = datetime.now()
    
    all_posts = []
    
    # 15ë¶„ë§ˆë‹¤ ì‹¤í–‰ (ë²„ê·¸ ê²Œì‹œíŒë“¤)
    if current_time.minute % CrawlingSchedule.FREQUENT_INTERVAL == 0 or force_crawl:
        frequent_posts = crawl_frequent_sites(force_crawl)
        all_posts.extend(frequent_posts)
    
    # 30ë¶„ë§ˆë‹¤ ì‹¤í–‰ (ì¼ë°˜ ê²Œì‹œíŒë“¤ + ì™¸ë¶€ ì‚¬ì´íŠ¸)
    if current_time.minute % CrawlingSchedule.REGULAR_INTERVAL == 0 or force_crawl:
        regular_posts = crawl_regular_sites(force_crawl)
        all_posts.extend(regular_posts)
    
    return all_posts

# =============================================================================
# ë¦¬í¬íŠ¸ìš© ë°ì´í„° ìˆ˜ì§‘ v3.4
# =============================================================================

def get_all_posts_for_report(hours: int = 24, force_crawl: bool = False) -> List[Dict]:
    """ë¦¬í¬íŠ¸ ìƒì„±ì„ ìœ„í•œ ì „ì²´ ê²Œì‹œê¸€ ìˆ˜ì§‘ v3.4"""
    logger.info(f"ğŸ“Š === ë¦¬í¬íŠ¸ìš© ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ (ìµœê·¼ {hours}ì‹œê°„" + (", force_crawl=True)" if force_crawl else ") ==="))
    
    all_posts = []
    
    # ëª¨ë“  ê²Œì‹œíŒì—ì„œ ë°ì´í„° ìˆ˜ì§‘
    try:
        # ê¸€ë¡œë²Œ ê²Œì‹œíŒë“¤
        all_posts.extend(fetch_stove_bug_board(force_crawl, 'frequent', 'global'))
        all_posts.extend(fetch_stove_general_board(force_crawl, 'regular', 'global'))
        
        # í•œêµ­ ê²Œì‹œíŒë“¤
        all_posts.extend(fetch_stove_bug_board(force_crawl, 'frequent', 'korean'))
        all_posts.extend(fetch_stove_general_board(force_crawl, 'regular', 'korean'))
        
        # ì™¸ë¶€ ì‚¬ì´íŠ¸ë“¤
        all_posts.extend(fetch_reddit_posts(force_crawl, 'regular'))
        all_posts.extend(fetch_ruliweb_posts(force_crawl, 'regular'))
        
    except Exception as e:
        logger.error(f"ë¦¬í¬íŠ¸ìš© ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {str(e)}")
    
    # ì‹œê°„ ë²”ìœ„ í•„í„°ë§
    cutoff_time = datetime.now() - timedelta(hours=hours)
    filtered_posts = []
    
    for post in all_posts:
        try:
            post_time = datetime.fromisoformat(post.get('timestamp', ''))
            if post_time >= cutoff_time:
                filtered_posts.append(post)
        except:
            # timestamp íŒŒì‹± ì‹¤íŒ¨ ì‹œ í¬í•¨
            filtered_posts.append(post)
    
    logger.info(f"ğŸ“Š === ë¦¬í¬íŠ¸ìš© ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(filtered_posts)}ê°œ ê²Œì‹œê¸€ ===")
    return filtered_posts

# =============================================================================
# ë©”ì¸ ì‹¤í–‰ ë¶€ v3.4
# =============================================================================

if __name__ == "__main__":
    import argparse
    
    # ë¡œê¹… ì„¤ì •
    setup_logging()
    
    # ëª…ë ¹í–‰ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(description='Epic7 ë‹¤êµ­ê°€ í¬ë¡¤ëŸ¬ v3.4')
    parser.add_argument('--force-crawl', action='store_true', help='ê°•ì œ í¬ë¡¤ë§ (ì¤‘ë³µ ì²´í¬ ë¬´ì‹œ)')
    parser.add_argument('--schedule', action='store_true', help='ìŠ¤ì¼€ì¤„ì— ë”°ë¥¸ í¬ë¡¤ë§')
    parser.add_argument('--frequent', action='store_true', help='15ë¶„ ì£¼ê¸° í¬ë¡¤ë§ë§Œ ì‹¤í–‰ (ë²„ê·¸ ê²Œì‹œíŒ)')
    parser.add_argument('--regular', action='store_true', help='30ë¶„ ì£¼ê¸° í¬ë¡¤ë§ë§Œ ì‹¤í–‰ (ì¼ë°˜+ì™¸ë¶€)')
    parser.add_argument('--report', type=int, metavar='HOURS', help='ë¦¬í¬íŠ¸ìš© ë°ì´í„° ìˆ˜ì§‘ (ì‹œê°„ ì§€ì •)')
    parser.add_argument('--region', choices=['global', 'korean', 'all'], default='all', help='í¬ë¡¤ë§ ì§€ì—­ ì„ íƒ')
    parser.add_argument('--site', choices=['stove', 'reddit', 'ruliweb', 'all'], default='all', help='í¬ë¡¤ë§ ì‚¬ì´íŠ¸ ì„ íƒ')
    
    args = parser.parse_args()
    
    try:
        logger.info(f"Epic7 ë‹¤êµ­ê°€ í¬ë¡¤ëŸ¬ v3.4 ì‹œì‘ - ì§€ì—­: {args.region}, ì‚¬ì´íŠ¸: {args.site}")
        
        if args.report:
            # ë¦¬í¬íŠ¸ìš© ë°ì´í„° ìˆ˜ì§‘
            posts = get_all_posts_for_report(args.report, args.force_crawl)
            print(f"ë¦¬í¬íŠ¸ìš© ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(posts)}ê°œ ê²Œì‹œê¸€")
            
        elif args.frequent:
            # 15ë¶„ ì£¼ê¸° í¬ë¡¤ë§ë§Œ ì‹¤í–‰
            posts = crawl_frequent_sites(args.force_crawl)
            print(f"15ë¶„ ì£¼ê¸° í¬ë¡¤ë§ ì™„ë£Œ: {len(posts)}ê°œ ê²Œì‹œê¸€")
            
        elif args.regular:
            # 30ë¶„ ì£¼ê¸° í¬ë¡¤ë§ë§Œ ì‹¤í–‰
            posts = crawl_regular_sites(args.force_crawl)
            print(f"30ë¶„ ì£¼ê¸° í¬ë¡¤ë§ ì™„ë£Œ: {len(posts)}ê°œ ê²Œì‹œê¸€")
            
        elif args.schedule:
            # ìŠ¤ì¼€ì¤„ì— ë”°ë¥¸ í¬ë¡¤ë§
            posts = crawl_by_schedule(force_crawl=args.force_crawl)
            print(f"ìŠ¤ì¼€ì¤„ ê¸°ë°˜ í¬ë¡¤ë§ ì™„ë£Œ: {len(posts)}ê°œ ê²Œì‹œê¸€")
            
        else:
            # ì „ì²´ í¬ë¡¤ë§ (ê¸°ë³¸)
            posts = []
            posts.extend(crawl_frequent_sites(args.force_crawl))
            posts.extend(crawl_regular_sites(args.force_crawl))
            print(f"ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ: {len(posts)}ê°œ ê²Œì‹œê¸€")
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        if posts:
            print(f"\n=== ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½ ===")
            for post in posts[:5]:  # ìƒìœ„ 5ê°œë§Œ ì¶œë ¥
                print(f"- {post.get('title', 'ì œëª© ì—†ìŒ')[:50]}... [{post.get('source', 'Unknown')}]")
            
            # ì†ŒìŠ¤ë³„ í†µê³„
            source_stats = {}
            for post in posts:
                source = post.get('source', 'Unknown')
                source_stats[source] = source_stats.get(source, 0) + 1
            
            print(f"\n=== ì†ŒìŠ¤ë³„ í†µê³„ ===")
            for source, count in source_stats.items():
                print(f"- {source}: {count}ê°œ")
        
    except KeyboardInterrupt:
        print("\ní¬ë¡¤ë§ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
        print(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")