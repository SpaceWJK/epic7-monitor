#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Epic7 ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ í¬ë¡¤ë§ ì—”ì§„ (ì™„ì „ ìˆ˜ì • ë²„ì „)
Korean/Global ëª¨ë“œ ë¶„ê¸° ì²˜ë¦¬ì™€ ê¸€ë¡œë²Œ í¬ë¡¤ë§ í•¨ìˆ˜ ì™„ì „ êµ¬í˜„

Author: Epic7 Monitoring Team
Version: 2.1.0
Date: 2025-07-16
"""

import json
import os
import sys
import time
import random
import hashlib
import re
import requests
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from urllib.parse import urljoin, urlparse

# Selenium ê´€ë ¨ import
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.keys import Keys

# HTML íŒŒì‹±
from bs4 import BeautifulSoup

# ë¡œê¹… ì„¤ì •
import logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# =============================================================================
# ëª¨ë“œ ê¸°ë°˜ íŒŒì¼ ë¶„ë¦¬ ì‹œìŠ¤í…œ
# =============================================================================

def get_file_paths(mode: str = "korean"):
    """ëª¨ë“œì— ë”°ë¥¸ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
    is_debug = os.environ.get('GITHUB_WORKFLOW', '').lower() in ['debug', 'test']
    
    if mode == "korean":
        links_file = "crawled_links_korean_debug.json" if is_debug else "crawled_links_korean.json"
        cache_file = "content_cache_korean_debug.json" if is_debug else "content_cache_korean.json"
    elif mode == "global":
        links_file = "crawled_links_global_debug.json" if is_debug else "crawled_links_global.json"
        cache_file = "content_cache_global_debug.json" if is_debug else "content_cache_global.json"
    else:  # all mode
        links_file = "crawled_links_debug.json" if is_debug else "crawled_links.json"
        cache_file = "content_cache_debug.json" if is_debug else "content_cache.json"
    
    return links_file, cache_file

def load_crawled_links(mode: str = "korean"):
    """ëª¨ë“œë³„ í¬ë¡¤ë§ëœ ë§í¬ë“¤ì„ ë¡œë“œ"""
    links_file, _ = get_file_paths(mode)
    
    if os.path.exists(links_file):
        try:
            with open(links_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            if isinstance(data, list):
                return {"links": data, "last_updated": datetime.now().isoformat()}
            return data
        except (json.JSONDecodeError, FileNotFoundError):
            logger.warning(f"{links_file} íŒŒì¼ ì½ê¸° ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±")
    
    return {"links": [], "last_updated": datetime.now().isoformat()}

def save_crawled_links(link_data, mode: str = "korean"):
    """ëª¨ë“œë³„ í¬ë¡¤ë§ëœ ë§í¬ë“¤ì„ ì €ì¥ (ìµœëŒ€ 1000ê°œ ìœ ì§€)"""
    links_file, _ = get_file_paths(mode)
    
    try:
        if len(link_data["links"]) > 1000:
            link_data["links"] = link_data["links"][-1000:]
        
        link_data["last_updated"] = datetime.now().isoformat()
        
        with open(links_file, 'w', encoding='utf-8') as f:
            json.dump(link_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"{links_file} ì €ì¥ ì™„ë£Œ: {len(link_data['links'])}ê°œ ë§í¬")
    except Exception as e:
        logger.error(f"{links_file} ì €ì¥ ì‹¤íŒ¨: {e}")

def load_content_cache(mode: str = "korean"):
    """ëª¨ë“œë³„ ê²Œì‹œê¸€ ë‚´ìš© ìºì‹œ ë¡œë“œ"""
    _, cache_file = get_file_paths(mode)
    
    if os.path.exists(cache_file):
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            logger.warning(f"{cache_file} íŒŒì¼ ì½ê¸° ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±")
    
    return {}

def save_content_cache(cache_data, mode: str = "korean"):
    """ëª¨ë“œë³„ ê²Œì‹œê¸€ ë‚´ìš© ìºì‹œ ì €ì¥"""
    _, cache_file = get_file_paths(mode)
    
    try:
        # ìºì‹œ í¬ê¸° ì œí•œ (ìµœëŒ€ 500ê°œ)
        if len(cache_data) > 500:
            sorted_items = sorted(cache_data.items(), key=lambda x: x[1].get('timestamp', ''), reverse=True)
            cache_data = dict(sorted_items[:500])
        
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"{cache_file} ì €ì¥ ì™„ë£Œ: {len(cache_data)}ê°œ ìºì‹œ")
    except Exception as e:
        logger.error(f"{cache_file} ì €ì¥ ì‹¤íŒ¨: {e}")

# =============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# =============================================================================

def get_url_hash(url: str) -> str:
    """URLì˜ í•´ì‹œê°’ ìƒì„±"""
    return hashlib.md5(url.encode('utf-8')).hexdigest()

def extract_content_summary(content: str) -> str:
    """ê²Œì‹œê¸€ ë‚´ìš©ì„ í•œ ì¤„ë¡œ ìš”ì•½"""
    if not content or len(content.strip()) < 10:
        return "ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”."
    
    # ë‚´ìš© ì •ë¦¬
    content = re.sub(r'\s+', ' ', content.strip())
    content = re.sub(r'[^\w\sê°€-í£.,!?]', '', content)
    
    # ì²« ë¬¸ì¥ ì¶”ì¶œ
    sentences = re.split(r'[.!?]', content)
    first_sentence = sentences[0].strip() if sentences else content
    
    if len(first_sentence) > 100:
        first_sentence = first_sentence[:97] + '...'
    elif len(first_sentence) > 10:
        first_sentence = first_sentence + '...'
    
    return first_sentence if first_sentence else "ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”."

def fix_stove_url(url):
    """ìŠ¤í† ë¸Œ URL ìˆ˜ì • í•¨ìˆ˜"""
    if not url:
        return url
    
    if url.startswith('ttps://'):
        url = 'h' + url
    elif url.startswith('ttp://'):
        url = 'h' + url
    elif url.startswith('/'):
        url = 'https://page.onstove.com' + url
    elif not url.startswith('http'):
        url = 'https://page.onstove.com' + ('/' if not url.startswith('/') else '') + url
    
    return url

def retry_on_failure(max_retries: int = 3, delay: float = 2.0):
    """ì¬ì‹œë„ ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    if attempt < max_retries - 1:
                        logger.warning(f"{func.__name__} ì‹œë„ {attempt + 1}/{max_retries} ì‹¤íŒ¨: {e}")
                        time.sleep(delay * (attempt + 1))
                    else:
                        logger.error(f"{func.__name__} ìµœì¢… ì‹¤íŒ¨: {e}")
            
            if last_exception:
                raise last_exception
        return wrapper
    return decorator

# =============================================================================
# Chrome ë“œë¼ì´ë²„ ê´€ë¦¬ (Chrome 138+ í˜¸í™˜ì„±)
# =============================================================================

@retry_on_failure(max_retries=3, delay=1.0)
def get_chrome_driver():
    """Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” (Chrome 138+ í˜¸í™˜ì„± ì™„ì „ ì ìš©)"""
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--disable-extensions')
    options.add_argument('--disable-plugins')
    options.add_argument('--disable-images')
    options.add_argument('--window-size=1920,1080')
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument('--disable-web-security')
    options.add_argument('--disable-features=VizDisplayCompositor')
    options.add_argument('--disable-background-timer-throttling')
    options.add_argument('--disable-renderer-backgrounding')
    options.add_argument('--disable-backgrounding-occluded-windows')
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    
    # User Agent ì„¤ì •
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36'
    ]
    options.add_argument(f'--user-agent={random.choice(user_agents)}')
    
    # í”„ë¦¬í¼ëŸ°ìŠ¤ ì„¤ì •
    prefs = {
        'profile.default_content_setting_values': {
            'images': 2,
            'plugins': 2,
            'popups': 2,
            'geolocation': 2,
            'notifications': 2,
            'media_stream': 2,
        }
    }
    options.add_experimental_option('prefs', prefs)
    
    # ChromeDriver ê²½ë¡œ ì‹œë„
    possible_paths = [
        '/usr/local/bin/chromedriver',
        '/usr/bin/chromedriver',
        '/snap/bin/chromium.chromedriver',
        '/opt/chrome/chromedriver'
    ]
    
    for path in possible_paths:
        try:
            if os.path.exists(path):
                service = Service(path)
                driver = webdriver.Chrome(service=service, options=options)
                driver.set_page_load_timeout(45)
                driver.implicitly_wait(15)
                logger.info(f"ChromeDriver ì´ˆê¸°í™” ì„±ê³µ: {path}")
                return driver
        except Exception as e:
            logger.debug(f"ChromeDriver ê²½ë¡œ {path} ì‹¤íŒ¨: {str(e)[:100]}...")
            continue
    
    # ì‹œìŠ¤í…œ ê¸°ë³¸ ChromeDriver ì‹œë„
    try:
        driver = webdriver.Chrome(options=options)
        driver.set_page_load_timeout(45)
        driver.implicitly_wait(15)
        logger.info("ì‹œìŠ¤í…œ ê¸°ë³¸ ChromeDriver ì´ˆê¸°í™” ì„±ê³µ")
        return driver
    except Exception as e:
        logger.debug(f"ì‹œìŠ¤í…œ ê¸°ë³¸ ChromeDriver ì‹¤íŒ¨: {str(e)[:100]}...")
       
    raise Exception("ChromeDriver ì´ˆê¸°í™” ì‹¤íŒ¨ - ì‹œìŠ¤í…œ ChromeDriverë¥¼ í™•ì¸í•˜ì„¸ìš”.")

# =============================================================================
# Discord ê´€ë ¨ í•¨ìˆ˜ë“¤ (ëˆ„ë½ëœ í•¨ìˆ˜ ì™„ì „ êµ¬í˜„)
# =============================================================================

def check_discord_webhooks():
    """Discord ì›¹í›… í™˜ê²½ë³€ìˆ˜ í™•ì¸"""
    webhooks = {}
    
    # ë²„ê·¸ ì•Œë¦¼ ì›¹í›…
    bug_webhook = os.environ.get('DISCORD_WEBHOOK_BUG')
    if bug_webhook:
        webhooks['bug'] = bug_webhook
        logger.info("Discord ë²„ê·¸ ì•Œë¦¼ ì›¹í›… í™•ì¸ë¨")
    
    # ê°ì„± ì•Œë¦¼ ì›¹í›…
    sentiment_webhook = os.environ.get('DISCORD_WEBHOOK_SENTIMENT')
    if sentiment_webhook:
        webhooks['sentiment'] = sentiment_webhook
        logger.info("Discord ê°ì„± ì•Œë¦¼ ì›¹í›… í™•ì¸ë¨")
    
    # ë¦¬í¬íŠ¸ ì›¹í›…
    report_webhook = os.environ.get('DISCORD_WEBHOOK_REPORT')
    if report_webhook:
        webhooks['report'] = report_webhook
        logger.info("Discord ë¦¬í¬íŠ¸ ì›¹í›… í™•ì¸ë¨")
    
    if not webhooks:
        logger.warning("Discord ì›¹í›… í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    return webhooks

def send_discord_message(webhook_url: str, message: str, title: str = "Epic7 ëª¨ë‹ˆí„°ë§"):
    """Discord ë©”ì‹œì§€ ì „ì†¡"""
    if not webhook_url:
        logger.error("Discord ì›¹í›… URLì´ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    try:
        # ë©”ì‹œì§€ ê¸¸ì´ ì œí•œ (Discord í•œê³„ ê³ ë ¤)
        if len(message) > 1900:
            message = message[:1900] + "\n...(ë©”ì‹œì§€ ê¸¸ì´ ì´ˆê³¼ë¡œ ìƒëµ)"
        
        # Discord ì›¹í›… í˜ì´ë¡œë“œ
        payload = {
            "embeds": [
                {
                    "title": title,
                    "description": message,
                    "color": 0x3498db,
                    "timestamp": datetime.now().isoformat(),
                    "footer": {
                        "text": "Epic7 ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"
                    }
                }
            ]
        }
        
        # ì›¹í›… ì „ì†¡
        response = requests.post(webhook_url, json=payload, timeout=10)
        if response.status_code == 204:
            logger.info("Discord ë©”ì‹œì§€ ì „ì†¡ ì„±ê³µ")
            return True
        else:
            logger.error(f"Discord ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨: {response.status_code}")
            return False
            
    except Exception as e:
        logger.error(f"Discord ë©”ì‹œì§€ ì „ì†¡ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def get_file_path(mode: str = "korean"):
    """í˜¸í™˜ì„±ì„ ìœ„í•œ wrapper í•¨ìˆ˜ - get_file_pathsì˜ ì²« ë²ˆì§¸ ê°’ë§Œ ë°˜í™˜"""
    links_file, cache_file = get_file_paths(mode)
    return links_file

# =============================================================================
# ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ í•¨ìˆ˜
# =============================================================================

def get_stove_post_content(post_url: str, driver: webdriver.Chrome = None, mode: str = "korean") -> str:
    """ìŠ¤í† ë¸Œ ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ (ê°•í™”ëœ ë²„ì „)"""
    cache = load_content_cache(mode)
    url_hash = get_url_hash(post_url)
    
    # ìºì‹œ í™•ì¸
    if url_hash in cache:
        cached_item = cache[url_hash]
        cache_time = datetime.fromisoformat(cached_item.get('timestamp', '2000-01-01'))
        if datetime.now() - cache_time < timedelta(hours=24):
            return cached_item.get('content', "ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”.")
    
    # ë“œë¼ì´ë²„ ì´ˆê¸°í™”
    driver_created = False
    if driver is None:
        try:
            driver = get_chrome_driver()
            driver_created = True
        except Exception as e:
            logger.error(f"Driver ìƒì„± ì‹¤íŒ¨: {e}")
            return "ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”."
    
    content_summary = "ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”."
    
    try:
        driver.get(post_url)
        time.sleep(10)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        
        WebDriverWait(driver, 20).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        
        # ìŠ¤í¬ë¡¤ë§ìœ¼ë¡œ ì½˜í…ì¸  ë¡œë”©
        for i in range(3):
            driver.execute_script(f"window.scrollTo(0, {500 * (i + 1)});")
            time.sleep(3)
        
        driver.execute_script("window.scrollTo(0, 0);")
        time.sleep(3)
        
        # ì½˜í…ì¸  ì„ íƒìë“¤
        content_selectors = [
            'div.s-article-content',
            'div.s-article-content-text',
            'div[class*="s-article-content"]',
            'section.s-article-body',
            'div.s-board-content',
            'div.article-content',
            'div.post-content',
            'div.content-body',
            'main.content',
            'div[class*="text-content"]',
            'div[class*="post-body"]',
            'div[class*="article-body"]'
        ]
        
        extracted_content = ""
        for selector in content_selectors:
            try:
                elements = driver.find_elements(By.CSS_SELECTOR, selector)
                if elements:
                    for element in elements:
                        text = element.text.strip()
                        if text and len(text) > 50:
                            if not any(skip_text in text.lower() for skip_text in ['install stove', 'ìŠ¤í† ë¸Œë¥¼ ì„¤ì¹˜', 'ë¡œê·¸ì¸ì´ í•„ìš”', 'javascript']):
                                extracted_content = text
                                break
                    if extracted_content:
                        break
            except Exception:
                continue
        
        # ë‚´ìš© ì²˜ë¦¬
        if extracted_content:
            lines = extracted_content.split('\n')
            meaningful_lines = []
            for line in lines:
                line = line.strip()
                if (len(line) > 15 and 
                    'ë¡œê·¸ì¸' not in line and 
                    'íšŒì›ê°€ì…' not in line and 
                    'ë©”ë‰´' not in line and 
                    'ê²€ìƒ‰' not in line and 
                    'ê³µì§€ì‚¬í•­' not in line and 
                    'ì´ë²¤íŠ¸' not in line and 
                    'Install STOVE' not in line and 
                    'ìŠ¤í† ë¸Œë¥¼ ì„¤ì¹˜' not in line):
                    meaningful_lines.append(line)
            
            if meaningful_lines:
                longest_line = max(meaningful_lines, key=len)
                if len(longest_line) > 20:
                    content_summary = extract_content_summary(longest_line)
                else:
                    content_summary = extract_content_summary(meaningful_lines[0])
        
        # JavaScript ì¶”ì¶œ ì‹œë„
        if content_summary == "ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”.":
            try:
                js_content = driver.execute_script("""
                var contentElements = [
                    document.querySelector('div.s-article-content'),
                    document.querySelector('div[class*="article-content"]'),
                    document.querySelector('div[class*="post-content"]'),
                    document.querySelector('main'),
                    document.querySelector('article')
                ];
                
                for (var i = 0; i < contentElements.length; i++) {
                    var element = contentElements[i];
                    if (element && element.innerText) {
                        var text = element.innerText.trim();
                        if (text.length > 50 && 
                            !text.toLowerCase().includes('install stove') && 
                            !text.includes('ìŠ¤í† ë¸Œë¥¼ ì„¤ì¹˜') && 
                            !text.includes('ë¡œê·¸ì¸ì´ í•„ìš”')) {
                            return text;
                        }
                    }
                }
                return '';
                """)
                
                if js_content and len(js_content.strip()) > 50:
                    content_summary = extract_content_summary(js_content)
            except Exception as e:
                logger.error(f"JavaScript ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        # BeautifulSoup ì¶”ì¶œ ì‹œë„
        if content_summary == "ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”.":
            try:
                html = driver.page_source
                soup = BeautifulSoup(html, 'html.parser')
                
                stove_content_tags = [
                    soup.find('div', class_='s-article-content'),
                    soup.find('div', class_='s-article-content-text'),
                    soup.find('section', class_='s-article-body'),
                    soup.find('div', class_='s-board-content')
                ]
                
                for tag in stove_content_tags:
                    if tag:
                        text = tag.get_text(strip=True)
                        if text and len(text) > 50:
                            if not any(skip in text.lower() for skip in ['install stove', 'ìŠ¤í† ë¸Œë¥¼ ì„¤ì¹˜']):
                                content_summary = extract_content_summary(text)
                                break
            except Exception as e:
                logger.error(f"BeautifulSoup ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        # ìºì‹œ ì €ì¥
        cache[url_hash] = {
            'content': content_summary,
            'timestamp': datetime.now().isoformat(),
            'url': post_url
        }
        save_content_cache(cache, mode)
        
    except TimeoutException:
        logger.error(f"í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ: {post_url}")
        content_summary = "â° ê²Œì‹œê¸€ ë¡œë”© ì‹œê°„ ì´ˆê³¼. ë§í¬ë¥¼ í´ë¦­í•˜ì—¬ í™•ì¸í•˜ì„¸ìš”."
    except Exception as e:
        logger.error(f"ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        content_summary = "ğŸ”— ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”."
    finally:
        if driver_created and driver:
            try:
                driver.quit()
            except:
                pass
    
    return content_summary

# =============================================================================
# í•œêµ­ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ í•¨ìˆ˜ë“¤
# =============================================================================

@retry_on_failure(max_retries=2, delay=3.0)
def fetch_stove_bug_board(mode: str = "korean"):
    """ìŠ¤í† ë¸Œ ì—í”½ì„¸ë¸ ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§"""
    posts = []
    link_data = load_crawled_links(mode)
    crawled_links = link_data["links"]
    
    logger.info(f"ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹œì‘ (mode: {mode})")
    
    driver = None
    try:
        driver = get_chrome_driver()
        url = "https://page.onstove.com/epicseven/kr/list/1012?page=1&direction=LATEST"
        driver.get(url)
        time.sleep(15)
        
        WebDriverWait(driver, 20).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        
        # ìŠ¤í¬ë¡¤ë§ìœ¼ë¡œ ê²Œì‹œê¸€ ë¡œë”©
        for i in range(3):
            driver.execute_script(f"window.scrollTo(0, {500 * (i + 1)});")
            time.sleep(3)
        
        driver.execute_script("window.scrollTo(0, 0);")
        time.sleep(3)
        
        # JavaScriptë¡œ ê²Œì‹œê¸€ ì¶”ì¶œ
        user_posts = driver.execute_script("""
        var posts = [];
        var items = document.querySelectorAll('section.s-board-item');
        
        for (var i = 0; i < Math.min(items.length, 15); i++) {
            var item = items[i];
            var link = item.querySelector('a[href*="/view/"]');
            var title = item.querySelector('.s-board-title-text, .board-title, h3 span');
            
            if (link && title && link.href && title.innerText) {
                var titleText = title.innerText.trim();
                if (titleText.length > 3) {
                    var isNotice = item.querySelector('.notice, [class*="notice"]');
                    var isEvent = item.querySelector('.event, [class*="event"]');
                    if (!isNotice && !isEvent) {
                        posts.push({
                            title: titleText,
                            href: link.href,
                            id: link.href.split('/').pop()
                        });
                    }
                }
            }
        }
        return posts;
        """)
        
        # ê²Œì‹œê¸€ ì²˜ë¦¬
        for i, post_info in enumerate(user_posts, 1):
            try:
                href = fix_stove_url(post_info['href'])
                title = post_info['title']
                
                if href in crawled_links:
                    continue
                
                if title and href and len(title) > 3:
                    content = get_stove_post_content(href, driver, mode)
                    
                    post_data = {
                        "title": title,
                        "url": href,
                        "content": content,
                        "timestamp": datetime.now().isoformat(),
                        "source": "stove_bug",
                        "site": "STOVE ë²„ê·¸ì‹ ê³ "
                    }
                    
                    posts.append(post_data)
                    crawled_links.append(href)
                    
                    logger.info(f"ìŠ¤í† ë¸Œ ë²„ê·¸ ìƒˆ ê²Œì‹œê¸€: {title[:50]}...")
                    time.sleep(random.uniform(2, 4))
                    
            except Exception as e:
                logger.error(f"ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œê¸€ {i} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
                
    except Exception as e:
        logger.error(f"ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    finally:
        if driver:
            driver.quit()
    
    # ë§í¬ ë°ì´í„° ì €ì¥
    link_data["links"] = crawled_links
    save_crawled_links(link_data, mode)
    
    return posts

@retry_on_failure(max_retries=2, delay=3.0)
def fetch_stove_general_board(mode: str = "korean"):
    """ìŠ¤í† ë¸Œ ì—í”½ì„¸ë¸ ììœ ê²Œì‹œíŒ í¬ë¡¤ë§"""
    posts = []
    link_data = load_crawled_links(mode)
    crawled_links = link_data["links"]
    
    logger.info(f"ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹œì‘ (mode: {mode})")
    
    driver = None
    try:
        driver = get_chrome_driver()
        url = "https://page.onstove.com/epicseven/kr/list/1005?page=1&direction=LATEST"
        driver.get(url)
        time.sleep(15)
        
        WebDriverWait(driver, 20).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        
        # ìŠ¤í¬ë¡¤ë§ìœ¼ë¡œ ê²Œì‹œê¸€ ë¡œë”©
        for i in range(3):
            driver.execute_script(f"window.scrollTo(0, {500 * (i + 1)});")
            time.sleep(3)
        
        # JavaScriptë¡œ ê²Œì‹œê¸€ ì¶”ì¶œ
        user_posts = driver.execute_script("""
        var posts = [];
        var items = document.querySelectorAll('section.s-board-item');
        
        for (var i = 0; i < Math.min(items.length, 15); i++) {
            var item = items[i];
            var link = item.querySelector('a[href*="/view/"]');
            var title = item.querySelector('.s-board-title-text, .board-title, h3 span');
            
            if (link && title && link.href && title.innerText) {
                var titleText = title.innerText.trim();
                if (titleText.length > 3) {
                    var isNotice = item.querySelector('.notice, [class*="notice"]');
                    var isEvent = item.querySelector('.event, [class*="event"]');
                    if (!isNotice && !isEvent) {
                        posts.push({
                            title: titleText,
                            href: link.href
                        });
                    }
                }
            }
        }
        return posts;
        """)
        
        # ê²Œì‹œê¸€ ì²˜ë¦¬
        for post_info in user_posts:
            try:
                href = fix_stove_url(post_info['href'])
                title = post_info['title']
                
                if href in crawled_links:
                    continue
                
                if not title or len(title) < 4:
                    continue
                
                # ì˜ë¯¸ì—†ëŠ” ì œëª© í•„í„°ë§
                meaningless_patterns = [
                    r'^[.]{3,}$',
                    r'^[ã…‹ã…ã…—ã…œã…‘]{3,}$',
                    r'^[!@#$%^&*()]{3,}$',
                ]
                
                is_meaningless = any(re.match(pattern, title) for pattern in meaningless_patterns)
                if is_meaningless:
                    continue
                
                content = get_stove_post_content(href, driver, mode)
                
                post_data = {
                    "title": title,
                    "url": href,
                    "content": content,
                    "timestamp": datetime.now().isoformat(),
                    "source": "stove_general",
                    "site": "STOVE ììœ ê²Œì‹œíŒ"
                }
                
                posts.append(post_data)
                crawled_links.append(href)
                
                logger.info(f"ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ ìƒˆ ê²Œì‹œê¸€: {title[:50]}...")
                time.sleep(random.uniform(2, 4))
                
            except Exception as e:
                logger.error(f"ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ ê²Œì‹œê¸€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
                
    except Exception as e:
        logger.error(f"ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    finally:
        if driver:
            driver.quit()
    
    # ë§í¬ ë°ì´í„° ì €ì¥
    link_data["links"] = crawled_links
    save_crawled_links(link_data, mode)
    
    return posts

@retry_on_failure(max_retries=2, delay=3.0)
def fetch_ruliweb_epic7_board(mode: str = "korean"):
    """ë£¨ë¦¬ì›¹ ì—í”½ì„¸ë¸ ê²Œì‹œíŒ í¬ë¡¤ë§"""
    posts = []
    link_data = load_crawled_links(mode)
    crawled_links = link_data["links"]
    
    logger.info(f"ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì‹œì‘ (mode: {mode})")
    
    driver = None
    try:
        driver = get_chrome_driver()
        url = "https://bbs.ruliweb.com/game/84834"
        driver.get(url)
        time.sleep(10)
        
        # ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
        selectors = [
            ".subject_link",
            ".table_body .subject a",
            "td.subject a",
            "a[href*='/read/']",
            ".board_list_table .subject_link",
            "table tr td a[href*='read']"
        ]
        
        articles = []
        for selector in selectors:
            try:
                articles = driver.find_elements(By.CSS_SELECTOR, selector)
                if articles:
                    logger.info(f"ë£¨ë¦¬ì›¹ ì„ íƒì ì„±ê³µ: {selector} ({len(articles)}ê°œ)")
                    break
            except NoSuchElementException:
                continue
        
        if not articles:
            logger.warning("ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return posts
        
        # ê²Œì‹œê¸€ ì²˜ë¦¬
        for i, article in enumerate(articles[:15]):
            try:
                title = article.text.strip()
                link = article.get_attribute("href")
                
                if not title or not link or len(title) < 3:
                    continue
                
                # ê³µì§€ì‚¬í•­ ì œì™¸
                if any(keyword in title for keyword in ['ê³µì§€', 'í•„ë…', 'ì´ë²¤íŠ¸', 'ì¶”ì²œ', 'ë² ìŠ¤íŠ¸', 'ê³µì§€ì‚¬í•­']):
                    continue
                
                # ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
                if link.startswith('/'):
                    link = 'https://bbs.ruliweb.com' + link
                
                if link not in crawled_links:
                    post_data = {
                        "title": title,
                        "url": link,
                        "content": "ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”.",
                        "timestamp": datetime.now().isoformat(),
                        "source": "ruliweb_epic7",
                        "site": "ë£¨ë¦¬ì›¹"
                    }
                    
                    posts.append(post_data)
                    crawled_links.append(link)
                    
                    logger.info(f"ë£¨ë¦¬ì›¹ ìƒˆ ê²Œì‹œê¸€: {title[:50]}...")
                    
            except Exception as e:
                logger.error(f"ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ {i+1} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
                
    except Exception as e:
        logger.error(f"ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    finally:
        if driver:
            driver.quit()
    
    # ë§í¬ ë°ì´í„° ì €ì¥
    link_data["links"] = crawled_links
    save_crawled_links(link_data, mode)
    
    return posts

@retry_on_failure(max_retries=2, delay=3.0)
def fetch_arca_epic7_board(mode: str = "korean"):
    """ì•„ì¹´ë¼ì´ë¸Œ ì—í”½ì„¸ë¸ ì±„ë„ í¬ë¡¤ë§"""
    posts = []
    link_data = load_crawled_links(mode)
    crawled_links = link_data["links"]
    
    logger.info(f"ì•„ì¹´ë¼ì´ë¸Œ í¬ë¡¤ë§ ì‹œì‘ (mode: {mode})")
    
    driver = None
    try:
        driver = get_chrome_driver()
        url = "https://arca.live/b/epic7"
        driver.get(url)
        time.sleep(10)
        
        WebDriverWait(driver, 15).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        
        driver.execute_script("window.scrollTo(0, 800);")
        time.sleep(5)
        
        # ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
        selectors = [
            ".vrow .title a",
            ".vrow-inner .title a",
            "a[href*='/b/epic7/']",
            ".article-title a",
            ".list-table .title a"
        ]
        
        articles = []
        for selector in selectors:
            try:
                articles = driver.find_elements(By.CSS_SELECTOR, selector)
                if articles:
                    logger.info(f"ì•„ì¹´ë¼ì´ë¸Œ ì„ íƒì ì„±ê³µ: {selector} ({len(articles)}ê°œ)")
                    break
            except NoSuchElementException:
                continue
        
        if not articles:
            logger.warning("ì•„ì¹´ë¼ì´ë¸Œ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return posts
        
        # ê²Œì‹œê¸€ ì²˜ë¦¬
        for i, article in enumerate(articles[:15]):
            try:
                title = article.text.strip()
                link = article.get_attribute("href")
                
                if not title or not link or len(title) < 3:
                    continue
                
                # ê³µì§€ì‚¬í•­ ì œì™¸
                if any(keyword in title for keyword in ['ê³µì§€', 'í•„ë…', 'ì´ë²¤íŠ¸', 'ì¶”ì²œ', 'ë² ìŠ¤íŠ¸']):
                    continue
                
                # ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
                if link.startswith('/'):
                    link = 'https://arca.live' + link
                
                if link not in crawled_links:
                    post_data = {
                        "title": title,
                        "url": link,
                        "content": "ì•„ì¹´ë¼ì´ë¸Œ ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”.",
                        "timestamp": datetime.now().isoformat(),
                        "source": "arca_epic7",
                        "site": "ì•„ì¹´ë¼ì´ë¸Œ"
                    }
                    
                    posts.append(post_data)
                    crawled_links.append(link)
                    
                    logger.info(f"ì•„ì¹´ë¼ì´ë¸Œ ìƒˆ ê²Œì‹œê¸€: {title[:50]}...")
                    
            except Exception as e:
                logger.error(f"ì•„ì¹´ë¼ì´ë¸Œ ê²Œì‹œê¸€ {i+1} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
                
    except Exception as e:
        logger.error(f"ì•„ì¹´ë¼ì´ë¸Œ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    finally:
        if driver:
            driver.quit()
    
    # ë§í¬ ë°ì´í„° ì €ì¥
    link_data["links"] = crawled_links
    save_crawled_links(link_data, mode)
    
    return posts

# =============================================================================
# ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ í•¨ìˆ˜ë“¤
# =============================================================================

@retry_on_failure(max_retries=2, delay=3.0)
def fetch_stove_global_bug_board(mode: str = "global"):
    """STOVE ê¸€ë¡œë²Œ ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§"""
    posts = []
    link_data = load_crawled_links(mode)
    crawled_links = link_data["links"]
    
    logger.info(f"STOVE ê¸€ë¡œë²Œ ë²„ê·¸ í¬ë¡¤ë§ ì‹œì‘ (mode: {mode})")
    
    driver = None
    try:
        driver = get_chrome_driver()
        url = "https://page.onstove.com/epicseven/global/list/998?page=1&direction=LATEST"
        driver.get(url)
        time.sleep(15)
        
        WebDriverWait(driver, 20).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        
        # ìŠ¤í¬ë¡¤ë§ìœ¼ë¡œ ê²Œì‹œê¸€ ë¡œë”©
        for i in range(3):
            driver.execute_script(f"window.scrollTo(0, {500 * (i + 1)});")
            time.sleep(3)
        
        # JavaScriptë¡œ ê²Œì‹œê¸€ ì¶”ì¶œ
        user_posts = driver.execute_script("""
        var posts = [];
        var items = document.querySelectorAll('section.s-board-item');
        
        for (var i = 0; i < Math.min(items.length, 15); i++) {
            var item = items[i];
            var link = item.querySelector('a[href*="/view/"]');
            var title = item.querySelector('.s-board-title-text, .board-title, h3 span');
            
            if (link && title && link.href && title.innerText) {
                var titleText = title.innerText.trim();
                if (titleText.length > 3) {
                    var isNotice = item.querySelector('.notice, [class*="notice"]');
                    var isEvent = item.querySelector('.event, [class*="event"]');
                    if (!isNotice && !isEvent) {
                        posts.push({
                            title: titleText,
                            href: link.href
                        });
                    }
                }
            }
        }
        return posts;
        """)
        
        # ê²Œì‹œê¸€ ì²˜ë¦¬
        for post_info in user_posts:
            try:
                href = fix_stove_url(post_info['href'])
                title = post_info['title']
                
                if href in crawled_links:
                    continue
                
                content = get_stove_post_content(href, driver, mode)
                
                post_data = {
                    "title": title,
                    "url": href,
                    "content": content,
                    "timestamp": datetime.now().isoformat(),
                    "source": "stove_global_bug",
                    "site": "STOVE Global Bug"
                }
                
                posts.append(post_data)
                crawled_links.append(href)
                
                logger.info(f"STOVE ê¸€ë¡œë²Œ ë²„ê·¸ ìƒˆ ê²Œì‹œê¸€: {title[:50]}...")
                time.sleep(random.uniform(2, 4))
                
            except Exception as e:
                logger.error(f"STOVE ê¸€ë¡œë²Œ ë²„ê·¸ ê²Œì‹œê¸€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
                
    except Exception as e:
        logger.error(f"STOVE ê¸€ë¡œë²Œ ë²„ê·¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    finally:
        if driver:
            driver.quit()
    
    # ë§í¬ ë°ì´í„° ì €ì¥
    link_data["links"] = crawled_links
    save_crawled_links(link_data, mode)
    
    return posts

@retry_on_failure(max_retries=2, delay=3.0)
def fetch_stove_global_general_board(mode: str = "global"):
    """STOVE ê¸€ë¡œë²Œ ììœ ê²Œì‹œíŒ í¬ë¡¤ë§"""
    posts = []
    link_data = load_crawled_links(mode)
    crawled_links = link_data["links"]
    
    logger.info(f"STOVE ê¸€ë¡œë²Œ ììœ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹œì‘ (mode: {mode})")
    
    driver = None
    try:
        driver = get_chrome_driver()
        url = "https://page.onstove.com/epicseven/global/list/989?page=1&direction=LATEST"
        driver.get(url)
        time.sleep(15)
        
        WebDriverWait(driver, 20).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        
        # ìŠ¤í¬ë¡¤ë§ìœ¼ë¡œ ê²Œì‹œê¸€ ë¡œë”©
        for i in range(3):
            driver.execute_script(f"window.scrollTo(0, {500 * (i + 1)});")
            time.sleep(3)
        
        # JavaScriptë¡œ ê²Œì‹œê¸€ ì¶”ì¶œ
        user_posts = driver.execute_script("""
        var posts = [];
        var items = document.querySelectorAll('section.s-board-item');
        
        for (var i = 0; i < Math.min(items.length, 15); i++) {
            var item = items[i];
            var link = item.querySelector('a[href*="/view/"]');
            var title = item.querySelector('.s-board-title-text, .board-title, h3 span');
            
            if (link && title && link.href && title.innerText) {
                var titleText = title.innerText.trim();
                if (titleText.length > 3) {
                    var isNotice = item.querySelector('.notice, [class*="notice"]');
                    var isEvent = item.querySelector('.event, [class*="event"]');
                    if (!isNotice && !isEvent) {
                        posts.push({
                            title: titleText,
                            href: link.href
                        });
                    }
                }
            }
        }
        return posts;
        """)
        
        # ê²Œì‹œê¸€ ì²˜ë¦¬
        for post_info in user_posts:
            try:
                href = fix_stove_url(post_info['href'])
                title = post_info['title']
                
                if href in crawled_links:
                    continue
                
                content = get_stove_post_content(href, driver, mode)
                
                post_data = {
                    "title": title,
                    "url": href,
                    "content": content,
                    "timestamp": datetime.now().isoformat(),
                    "source": "stove_global_general",
                    "site": "STOVE Global General"
                }
                
                posts.append(post_data)
                crawled_links.append(href)
                
                logger.info(f"STOVE ê¸€ë¡œë²Œ ììœ ê²Œì‹œíŒ ìƒˆ ê²Œì‹œê¸€: {title[:50]}...")
                time.sleep(random.uniform(2, 4))
                
            except Exception as e:
                logger.error(f"STOVE ê¸€ë¡œë²Œ ììœ ê²Œì‹œíŒ ê²Œì‹œê¸€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
                
    except Exception as e:
        logger.error(f"STOVE ê¸€ë¡œë²Œ ììœ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    finally:
        if driver:
            driver.quit()
    
    # ë§í¬ ë°ì´í„° ì €ì¥
    link_data["links"] = crawled_links
    save_crawled_links(link_data, mode)
    
    return posts

@retry_on_failure(max_retries=2, delay=3.0)
def fetch_reddit_epic7_board(mode: str = "global"):
    """Reddit r/EpicSeven ìµœì‹ ê¸€ í¬ë¡¤ë§"""
    posts = []
    link_data = load_crawled_links(mode)
    crawled_links = link_data["links"]
    
    logger.info(f"Reddit í¬ë¡¤ë§ ì‹œì‘ (mode: {mode})")
    
    try:
        url = "https://www.reddit.com/r/EpicSeven/new.json?limit=20"
        headers = {
            "User-Agent": "Epic7MonitorBot/2.0",
            "Accept": "application/json"
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        data = response.json()
        
        if 'data' in data and 'children' in data['data']:
            for child in data['data']['children']:
                try:
                    item = child['data']
                    title = item.get('title', '').strip()
                    permalink = "https://www.reddit.com" + item.get('permalink', '')
                    
                    if not title or not permalink or len(title) < 3:
                        continue
                    
                    if permalink in crawled_links:
                        continue
                    
                    post_data = {
                        "title": title,
                        "url": permalink,
                        "content": f"Reddit ê²Œì‹œê¸€: {title[:100]}...",
                        "timestamp": datetime.now().isoformat(),
                        "source": "reddit_epic7",
                        "site": "Reddit"
                    }
                    
                    posts.append(post_data)
                    crawled_links.append(permalink)
                    
                    logger.info(f"Reddit ìƒˆ ê²Œì‹œê¸€: {title[:50]}...")
                    
                except Exception as e:
                    logger.error(f"Reddit ê²Œì‹œê¸€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
                    
    except requests.RequestException as e:
        logger.error(f"Reddit API ìš”ì²­ ì‹¤íŒ¨: {e}")
    except Exception as e:
        logger.error(f"Reddit í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    # ë§í¬ ë°ì´í„° ì €ì¥
    link_data["links"] = crawled_links
    save_crawled_links(link_data, mode)
    
    return posts

@retry_on_failure(max_retries=2, delay=3.0)
def fetch_epic7_official_forum(mode: str = "global"):
    """Epic7 ê³µì‹ í¬ëŸ¼ í¬ë¡¤ë§ (í•¨ìˆ˜ëª… ìˆ˜ì •ë¨)"""
    posts = []
    link_data = load_crawled_links(mode)
    crawled_links = link_data["links"]
    
    logger.info(f"Epic7 ê³µì‹ í¬ëŸ¼ í¬ë¡¤ë§ ì‹œì‘ (mode: {mode})")
    
    driver = None
    try:
        driver = get_chrome_driver()
        url = "https://epic7.gg/forum"
        driver.get(url)
        time.sleep(15)
        
        WebDriverWait(driver, 15).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        
        driver.execute_script("window.scrollTo(0, 800);")
        time.sleep(5)
        
        # ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
        selectors = [
            ".topic-title a",
            ".forum-post-title a",
            "a[href*='/topic/']",
            ".post-title a",
            ".thread-title a"
        ]
        
        articles = []
        for selector in selectors:
            try:
                articles = driver.find_elements(By.CSS_SELECTOR, selector)
                if articles:
                    logger.info(f"ê³µì‹ í¬ëŸ¼ ì„ íƒì ì„±ê³µ: {selector} ({len(articles)}ê°œ)")
                    break
            except NoSuchElementException:
                continue
        
        if not articles:
            logger.warning("ê³µì‹ í¬ëŸ¼ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return posts
        
        # ê²Œì‹œê¸€ ì²˜ë¦¬
        for i, article in enumerate(articles[:15]):
            try:
                title = article.text.strip()
                link = article.get_attribute("href")
                
                if not title or not link or len(title) < 3:
                    continue
                
                # ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
                if link.startswith('/'):
                    link = 'https://epic7.gg' + link
                
                if link not in crawled_links:
                    post_data = {
                        "title": title,
                        "url": link,
                        "content": "Epic7 ê³µì‹ í¬ëŸ¼ ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”.",
                        "timestamp": datetime.now().isoformat(),
                        "source": "epic7_official_forum",
                        "site": "Epic7 ê³µì‹ í¬ëŸ¼"
                    }
                    
                    posts.append(post_data)
                    crawled_links.append(link)
                    
                    logger.info(f"ê³µì‹ í¬ëŸ¼ ìƒˆ ê²Œì‹œê¸€: {title[:50]}...")
                    
            except Exception as e:
                logger.error(f"ê³µì‹ í¬ëŸ¼ ê²Œì‹œê¸€ {i+1} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
                
    except Exception as e:
        logger.error(f"ê³µì‹ í¬ëŸ¼ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    finally:
        if driver:
            driver.quit()
    
    # ë§í¬ ë°ì´í„° ì €ì¥
    link_data["links"] = crawled_links
    save_crawled_links(link_data, mode)
    
    return posts

# =============================================================================
# í†µí•© í¬ë¡¤ë§ í•¨ìˆ˜ë“¤
# =============================================================================

def crawl_korean_sites(mode: str = "korean"):
    """í•œêµ­ ì‚¬ì´íŠ¸ í†µí•© í¬ë¡¤ë§"""
    logger.info(f"í•œêµ­ ì‚¬ì´íŠ¸ í†µí•© í¬ë¡¤ë§ ì‹œì‘ (mode: {mode})")
    
    all_posts = []
    
    # ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œíŒ
    try:
        stove_bug_posts = fetch_stove_bug_board(mode)
        all_posts.extend(stove_bug_posts)
        logger.info(f"ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œíŒ: {len(stove_bug_posts)}ê°œ ê²Œì‹œê¸€")
    except Exception as e:
        logger.error(f"ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    # ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ
    try:
        stove_general_posts = fetch_stove_general_board(mode)
        all_posts.extend(stove_general_posts)
        logger.info(f"ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ: {len(stove_general_posts)}ê°œ ê²Œì‹œê¸€")
    except Exception as e:
        logger.error(f"ìŠ¤í† ë¸Œ ììœ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    # ë£¨ë¦¬ì›¹
    try:
        ruliweb_posts = fetch_ruliweb_epic7_board(mode)
        all_posts.extend(ruliweb_posts)
        logger.info(f"ë£¨ë¦¬ì›¹: {len(ruliweb_posts)}ê°œ ê²Œì‹œê¸€")
    except Exception as e:
        logger.error(f"ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    # ì•„ì¹´ë¼ì´ë¸Œ
    try:
        arca_posts = fetch_arca_epic7_board(mode)
        all_posts.extend(arca_posts)
        logger.info(f"ì•„ì¹´ë¼ì´ë¸Œ: {len(arca_posts)}ê°œ ê²Œì‹œê¸€")
    except Exception as e:
        logger.error(f"ì•„ì¹´ë¼ì´ë¸Œ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    logger.info(f"í•œêµ­ ì‚¬ì´íŠ¸ í†µí•© í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_posts)}ê°œ ê²Œì‹œê¸€")
    return all_posts

def crawl_global_sites(mode: str = "global"):
    """ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ í†µí•© í¬ë¡¤ë§"""
    logger.info(f"ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ í†µí•© í¬ë¡¤ë§ ì‹œì‘ (mode: {mode})")
    
    all_posts = []
    
    # STOVE ê¸€ë¡œë²Œ ë²„ê·¸ ê²Œì‹œíŒ
    try:
        stove_global_bug_posts = fetch_stove_global_bug_board(mode)
        all_posts.extend(stove_global_bug_posts)
        logger.info(f"STOVE ê¸€ë¡œë²Œ ë²„ê·¸: {len(stove_global_bug_posts)}ê°œ ê²Œì‹œê¸€")
    except Exception as e:
        logger.error(f"STOVE ê¸€ë¡œë²Œ ë²„ê·¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    # STOVE ê¸€ë¡œë²Œ ììœ ê²Œì‹œíŒ
    try:
        stove_global_general_posts = fetch_stove_global_general_board(mode)
        all_posts.extend(stove_global_general_posts)
        logger.info(f"STOVE ê¸€ë¡œë²Œ ììœ ê²Œì‹œíŒ: {len(stove_global_general_posts)}ê°œ ê²Œì‹œê¸€")
    except Exception as e:
        logger.error(f"STOVE ê¸€ë¡œë²Œ ììœ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    # Reddit
    try:
        reddit_posts = fetch_reddit_epic7_board(mode)
        all_posts.extend(reddit_posts)
        logger.info(f"Reddit: {len(reddit_posts)}ê°œ ê²Œì‹œê¸€")
    except Exception as e:
        logger.error(f"Reddit í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    # Epic7 ê³µì‹ í¬ëŸ¼
    try:
        forum_posts = fetch_epic7_official_forum(mode)
        all_posts.extend(forum_posts)
        logger.info(f"Epic7 ê³µì‹ í¬ëŸ¼: {len(forum_posts)}ê°œ ê²Œì‹œê¸€")
    except Exception as e:
        logger.error(f"Epic7 ê³µì‹ í¬ëŸ¼ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    logger.info(f"ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ í†µí•© í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_posts)}ê°œ ê²Œì‹œê¸€")
    return all_posts

def crawl_all_sites(mode: str = "all"):
    """ëª¨ë“  ì‚¬ì´íŠ¸ í†µí•© í¬ë¡¤ë§"""
    logger.info(f"ëª¨ë“  ì‚¬ì´íŠ¸ í†µí•© í¬ë¡¤ë§ ì‹œì‘ (mode: {mode})")
    
    all_posts = []
    
    # í•œêµ­ ì‚¬ì´íŠ¸ í¬ë¡¤ë§
    korean_posts = crawl_korean_sites("korean")
    all_posts.extend(korean_posts)
    
    # ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§
    global_posts = crawl_global_sites("global")
    all_posts.extend(global_posts)
    
    logger.info(f"ëª¨ë“  ì‚¬ì´íŠ¸ í†µí•© í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_posts)}ê°œ ê²Œì‹œê¸€")
    return all_posts

# =============================================================================
# ë¦¬í¬íŠ¸ ìƒì„± í•¨ìˆ˜ë“¤
# =============================================================================

def get_all_posts_for_report(mode: str = "all"):
    """ë¦¬í¬íŠ¸ìš© ëª¨ë“  ê²Œì‹œê¸€ ìˆ˜ì§‘"""
    logger.info(f"ë¦¬í¬íŠ¸ìš© ê²Œì‹œê¸€ ìˆ˜ì§‘ ì‹œì‘ (mode: {mode})")
    
    all_posts = []
    
    # í•œêµ­ ì‚¬ì´íŠ¸
    korean_posts = crawl_korean_sites("korean")
    all_posts.extend(korean_posts)
    
    # ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸
    global_posts = crawl_global_sites("global")
    all_posts.extend(global_posts)
    
    # ì‹œê°„ìˆœ ì •ë ¬
    all_posts.sort(key=lambda x: x.get('timestamp', ''), reverse=True)
    
    logger.info(f"ë¦¬í¬íŠ¸ìš© ê²Œì‹œê¸€ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(all_posts)}ê°œ ê²Œì‹œê¸€")
    return all_posts

# =============================================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ë“¤
# =============================================================================

def main_crawl(mode: str = "korean"):
    """ë©”ì¸ í¬ë¡¤ë§ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info(f"Epic7 ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ í¬ë¡¤ë§ ì‹œì‘ (mode: {mode})")
    
    if mode == "korean":
        return crawl_korean_sites(mode)
    elif mode == "global":
        return crawl_global_sites(mode)
    elif mode == "all":
        return crawl_all_sites(mode)
    else:
        logger.error(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë“œ: {mode}")
        return []

def test_crawling():
    """í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    logger.info("í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # í•œêµ­ ì‚¬ì´íŠ¸ í…ŒìŠ¤íŠ¸
    korean_posts = crawl_korean_sites("korean")
    logger.info(f"í•œêµ­ ì‚¬ì´íŠ¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼: {len(korean_posts)}ê°œ ê²Œì‹œê¸€")
    
    # ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ í…ŒìŠ¤íŠ¸
    global_posts = crawl_global_sites("global")
    logger.info(f"ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼: {len(global_posts)}ê°œ ê²Œì‹œê¸€")
    
    logger.info("í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    return korean_posts + global_posts

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_crawling()