#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Epic7 ì£¼ê¸°ë³„ í¬ë¡¤ëŸ¬ v3.3 - Force Crawl ì§€ì› (ìˆ˜ì •ë¨)
- CSS Selector ë‹¤ì¤‘ í´ë°± ì‹œìŠ¤í…œ ì ìš© (2025ë…„ Stove êµ¬ì¡° ì—…ë°ì´íŠ¸)
- JavaScript ë Œë”ë§ ëŒ€ê¸°ì‹œê°„ ìµœì í™” (20ì´ˆ/25ì´ˆ)
- Force Crawl ì˜µì…˜ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬ ìš°íšŒ ê°€ëŠ¥
- ë²„ê·¸ ê²Œì‹œíŒ: 15ë¶„ ê°„ê²©, ì¼ë°˜ ê²Œì‹œíŒ: 30ë¶„ ê°„ê²©
- ì‹¤ì‹œê°„ ì•Œë¦¼: ë²„ê·¸ ê²Œì‹œíŒ ì¦‰ì‹œ ì „ì†¡
- í…ìŠ¤íŠ¸ ê¸¸ì´ ì¡°ê±´ ì™„í™” (30ì â†’ 10ì)
"""

import time
import random
import re
import requests
import concurrent.futures
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from urllib.parse import urljoin, urlparse

# Selenium ê´€ë ¨ import
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from selenium.webdriver.chrome.service import Service

# HTML íŒŒì‹±
from bs4 import BeautifulSoup

# ê³µí†µ ëª¨ë“ˆ ì„í¬íŠ¸
from config import config
from utils import setup_logging

# ë¡œê¹… ì„¤ì •
import logging
logger = logging.getLogger(__name__)

# =============================================================================
# í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ ë° ì„¤ì •
# =============================================================================

class CrawlingSchedule:
    """í¬ë¡¤ë§ ì£¼ê¸° ì„¤ì •"""
    
    # í¬ë¡¤ë§ ì£¼ê¸° (ë¶„)
    FREQUENT_INTERVAL = 15    # ë²„ê·¸ ê²Œì‹œíŒ (ê¸´ê¸‰)
    REGULAR_INTERVAL = 30     # ì¼ë°˜ ê²Œì‹œíŒ
    
    # ëŒ€ê¸° ì‹œê°„ ì„¤ì •
    FREQUENT_WAIT_TIME = 20   # ë²„ê·¸ ê²Œì‹œíŒ ëŒ€ê¸°ì‹œê°„
    REGULAR_WAIT_TIME = 25    # ì¼ë°˜ ê²Œì‹œíŒ ëŒ€ê¸°ì‹œê°„
    
    # ìŠ¤í¬ë¡¤ ì„¤ì •
    FREQUENT_SCROLL_COUNT = 3 # ë²„ê·¸ ê²Œì‹œíŒ ìŠ¤í¬ë¡¤
    REGULAR_SCROLL_COUNT = 5  # ì¼ë°˜ ê²Œì‹œíŒ ìŠ¤í¬ë¡¤

# =============================================================================
# Chrome ë“œë¼ì´ë²„ ê´€ë¦¬
# =============================================================================

def get_chrome_driver(headless: bool = True) -> webdriver.Chrome:
    """
    Chrome ë“œë¼ì´ë²„ ìƒì„± - 3ë‹¨ê³„ í´ë°± ë©”ì»¤ë‹ˆì¦˜
    
    Args:
        headless: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì—¬ë¶€
        
    Returns:
        webdriver.Chrome: Chrome ë“œë¼ì´ë²„ ì¸ìŠ¤í„´ìŠ¤
    """
    chrome_options = Options()
    
    if headless:
        chrome_options.add_argument('--headless=new')
    
    # ê¸°ë³¸ ì˜µì…˜ ì„¤ì •
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--window-size=1920,1080')
    chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36')
    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    
    # Stage 1: ì‹œìŠ¤í…œ ê²½ë¡œì—ì„œ Chrome/ChromeDriver ì°¾ê¸°
    possible_paths = [
        '/usr/bin/google-chrome',
        '/usr/bin/google-chrome-stable',
        '/usr/bin/chromium-browser',
        '/snap/bin/chromium',
        'google-chrome',
        'chromium-browser'
    ]
    
    for chrome_path in possible_paths:
        try:
            chrome_options.binary_location = chrome_path
            driver = webdriver.Chrome(options=chrome_options)
            logger.info(f"Chrome ë“œë¼ì´ë²„ ì„±ê³µ (Stage 1): {chrome_path}")
            return driver
        except Exception as e:
            logger.debug(f"Chrome ê²½ë¡œ ì‹¤íŒ¨: {chrome_path} - {str(e)}")
            continue
    
    # Stage 2: WebDriver Manager ì‚¬ìš©
    try:
        from webdriver_manager.chrome import ChromeDriverManager
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        logger.info("Chrome ë“œë¼ì´ë²„ ì„±ê³µ (Stage 2): WebDriver Manager")
        return driver
    except Exception as e:
        logger.debug(f"WebDriver Manager ì‹¤íŒ¨: {str(e)}")
    
    # Stage 3: ì§ì ‘ ë‹¤ìš´ë¡œë“œ ì‹œë„
    try:
        driver = webdriver.Chrome(options=chrome_options)
        logger.info("Chrome ë“œë¼ì´ë²„ ì„±ê³µ (Stage 3): ì§ì ‘ ì‹¤í–‰")
        return driver
    except Exception as e:
        logger.error(f"ëª¨ë“  Chrome ë“œë¼ì´ë²„ ì‹œë„ ì‹¤íŒ¨: {str(e)}")
        raise Exception("Chrome ë“œë¼ì´ë²„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

# =============================================================================
# ì¤‘ë³µ ë°©ì§€ ì‹œìŠ¤í…œ
# =============================================================================

def load_crawled_links() -> Dict:
    """
    í¬ë¡¤ë§ëœ ë§í¬ ëª©ë¡ ë¡œë“œ
    
    Returns:
        Dict: í¬ë¡¤ë§ëœ ë§í¬ ì •ë³´
    """
    try:
        if os.path.exists('crawled_links.json'):
            import json
            with open('crawled_links.json', 'r', encoding='utf-8') as f:
                data = json.load(f)
                # ìµœëŒ€ 2000ê°œê¹Œì§€ë§Œ ìœ ì§€
                if len(data.get('links', [])) > 2000:
                    data['links'] = data['links'][-2000:]
                return data
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ë§í¬ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
    
    return {
        'links': [],
        'last_updated': datetime.now().isoformat()
    }

def save_crawled_links(crawled_data: Dict) -> None:
    """
    í¬ë¡¤ë§ëœ ë§í¬ ëª©ë¡ ì €ì¥
    
    Args:
        crawled_data: ì €ì¥í•  í¬ë¡¤ë§ ë°ì´í„°
    """
    try:
        import json
        crawled_data['last_updated'] = datetime.now().isoformat()
        with open('crawled_links.json', 'w', encoding='utf-8') as f:
            json.dump(crawled_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ë§í¬ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

# =============================================================================
# ì½˜í…ì¸  ìºì‹œ ì‹œìŠ¤í…œ
# =============================================================================

def load_content_cache() -> Dict:
    """
    ì½˜í…ì¸  ìºì‹œ ë¡œë“œ
    
    Returns:
        Dict: ìºì‹œëœ ì½˜í…ì¸  ë°ì´í„°
    """
    try:
        if os.path.exists('content_cache.json'):
            import json
            with open('content_cache.json', 'r', encoding='utf-8') as f:
                cache = json.load(f)
                
                # 24ì‹œê°„ ì´ìƒ ëœ ìºì‹œ ì œê±°
                current_time = datetime.now()
                cleaned_cache = {}
                
                for url_hash, cached_data in cache.items():
                    if 'timestamp' in cached_data:
                        cached_time = datetime.fromisoformat(cached_data['timestamp'])
                        if (current_time - cached_time).total_seconds() < 24 * 3600:
                            cleaned_cache[url_hash] = cached_data
                
                return cleaned_cache
    except Exception as e:
        logger.error(f"ì½˜í…ì¸  ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
    
    return {}

def save_content_cache(cache_data: Dict) -> None:
    """
    ì½˜í…ì¸  ìºì‹œ ì €ì¥
    
    Args:
        cache_data: ì €ì¥í•  ìºì‹œ ë°ì´í„°
    """
    try:
        import json
        # ìµœëŒ€ 1000ê°œê¹Œì§€ë§Œ ìœ ì§€
        if len(cache_data) > 1000:
            # ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì˜¤ë˜ëœ ê²ƒë¶€í„° ì œê±°
            sorted_items = sorted(
                cache_data.items(),
                key=lambda x: x[1].get('timestamp', ''),
                reverse=True
            )
            cache_data = dict(sorted_items[:1000])
        
        with open('content_cache.json', 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"ì½˜í…ì¸  ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

# =============================================================================
# Stove ê²Œì‹œê¸€ ë³¸ë¬¸ ì¶”ì¶œ (ìˆ˜ì •ë¨)
# =============================================================================

def get_stove_post_content(post_url: str, driver: webdriver.Chrome = None, source: str = "", schedule_type: str = 'frequent') -> Tuple[str, str]:
    """
    Stove ê²Œì‹œê¸€ ë³¸ë¬¸ ì¶”ì¶œ (ìˆ˜ì •ë¨)
    
    Args:
        post_url: ê²Œì‹œê¸€ URL
        driver: Chrome ë“œë¼ì´ë²„ (ì„ íƒì‚¬í•­)
        source: ì†ŒìŠ¤ ì •ë³´
        schedule_type: ìŠ¤ì¼€ì¤„ íƒ€ì… ('frequent' ë˜ëŠ” 'regular')
        
    Returns:
        Tuple[str, str]: (ë³¸ë¬¸ ë‚´ìš©, ë³¸ë¬¸ ìš”ì•½)
    """
    # URL í•´ì‹œ ìƒì„± (ìºì‹œ í‚¤)
    import hashlib
    url_hash = hashlib.md5(post_url.encode()).hexdigest()
    
    # ìºì‹œì—ì„œ í™•ì¸
    content_cache = load_content_cache()
    if url_hash in content_cache:
        cached_data = content_cache[url_hash]
        logger.debug(f"ìºì‹œì—ì„œ ì½˜í…ì¸  ë¡œë“œ: {post_url[:50]}...")
        return cached_data.get('content', ''), cached_data.get('summary', '')
    
    # ìƒˆë¡œìš´ ë“œë¼ì´ë²„ ìƒì„± ì—¬ë¶€
    should_quit_driver = False
    if driver is None:
        driver = get_chrome_driver()
        should_quit_driver = True
    
    content = ""
    content_summary = ""
    
    try:
        logger.debug(f"ê²Œì‹œê¸€ ë³¸ë¬¸ ì¶”ì¶œ ì‹œì‘: {post_url}")
        driver.get(post_url)
        
        # ìŠ¤ì¼€ì¤„ì— ë”°ë¥¸ ëŒ€ê¸°ì‹œê°„ ì„¤ì •
        if schedule_type == 'frequent':
            wait_time = CrawlingSchedule.FREQUENT_WAIT_TIME
            scroll_count = CrawlingSchedule.FREQUENT_SCROLL_COUNT
        else:
            wait_time = CrawlingSchedule.REGULAR_WAIT_TIME
            scroll_count = CrawlingSchedule.REGULAR_SCROLL_COUNT
        
        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        time.sleep(wait_time)
        
        # DOM ì¤€ë¹„ ìƒíƒœ í™•ì¸
        WebDriverWait(driver, 30).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        
        # ìŠ¤í¬ë¡¤í•˜ì—¬ ë™ì  ì½˜í…ì¸  ë¡œë“œ
        for i in range(scroll_count):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
        
        # ìƒë‹¨ìœ¼ë¡œ ìŠ¤í¬ë¡¤
        driver.execute_script("window.scrollTo(0, 0);")
        time.sleep(1)
        
        # ìˆ˜ì •ëœ CSS Selector ëª©ë¡ (2025ë…„ Stove êµ¬ì¡°)
        content_selectors = [
            # 2025ë…„ ìµœì‹  Stove êµ¬ì¡°
            'div[class*="article-content"]',      # article-contentê°€ í¬í•¨ëœ ëª¨ë“  í´ë˜ìŠ¤
            'div[class*="post-content"]',         # post-contentê°€ í¬í•¨ëœ ëª¨ë“  í´ë˜ìŠ¤
            'div[class*="board-content"]',        # board-contentê°€ í¬í•¨ëœ ëª¨ë“  í´ë˜ìŠ¤
            'section[class*="content"]',          # contentê°€ í¬í•¨ëœ section
            'div[class*="text-content"]',         # text-contentê°€ í¬í•¨ëœ í´ë˜ìŠ¤
            'div.content-body',                   # content-body í´ë˜ìŠ¤
            'div.post-body',                      # post-body í´ë˜ìŠ¤
            'div.article-body',                   # article-body í´ë˜ìŠ¤
            
            # ê¸°ì¡´ selector ìœ ì§€ (í•˜ìœ„ í˜¸í™˜ì„±)
            'div.s-article-content',
            'div.s-article-content-text',
            'section.s-article-body',
            'div.s-board-content',
            'div.article-content',
            'div.post-content',
            
            # í¬ê´„ì  selector ì¶”ê°€
            'main [class*="content"]',            # main íƒœê·¸ ë‚´ content í´ë˜ìŠ¤
            'article [class*="content"]',         # article íƒœê·¸ ë‚´ content í´ë˜ìŠ¤
            '[data-testid*="content"]',           # data-testid content ì†ì„±
            '[id*="content"]',                    # idì— content í¬í•¨
            'div[class*="body"]',                 # bodyê°€ í¬í•¨ëœ div
            '.content',                           # ë‹¨ìˆœ content í´ë˜ìŠ¤
            '.post',                              # ë‹¨ìˆœ post í´ë˜ìŠ¤
            '.article',                           # ë‹¨ìˆœ article í´ë˜ìŠ¤
            'p',                                  # ë§ˆì§€ë§‰ ìˆ˜ë‹¨: p íƒœê·¸ë“¤
        ]
        
        # CSS Selectorë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„
        for selector in content_selectors:
            try:
                elements = driver.find_elements(By.CSS_SELECTOR, selector)
                logger.debug(f"Selector {selector}: {len(elements)}ê°œ ìš”ì†Œ ë°œê²¬")
                
                if elements:
                    for element in elements:
                        try:
                            text = element.get_attribute('innerText') or element.text
                            text = text.strip()
                            
                            # í…ìŠ¤íŠ¸ ê¸¸ì´ ì¡°ê±´ ì™„í™”: 30ì â†’ 10ì
                            if text and len(text) >= 10:  # 30ìì—ì„œ 10ìë¡œ ë³€ê²½
                                # ì˜ë¯¸ì—†ëŠ” í…ìŠ¤íŠ¸ í•„í„°ë§
                                skip_phrases = [
                                    'ë¡œê·¸ì¸', 'ì„¤ì¹˜', 'ê´‘ê³ ', 'ì¿ í‚¤', 'cookie',
                                    'ì´ìš©ì•½ê´€', 'ê°œì¸ì •ë³´', 'ì €ì‘ê¶Œ', 'ë¬´ë‹¨ì „ì¬',
                                    'ëŒ“ê¸€', 'comment', 'ì¢‹ì•„ìš”', 'like', 'ê³µìœ '
                                ]
                                
                                if not any(phrase in text.lower() for phrase in skip_phrases):
                                    content = text
                                    
                                    # ì²« ë²ˆì§¸ ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ì„ ìš”ì•½ìœ¼ë¡œ ì‚¬ìš©
                                    lines = text.split('\n')
                                    for line in lines:
                                        line = line.strip()
                                        if len(line) >= 15:  # ìš”ì•½ì€ 15ì ì´ìƒ
                                            content_summary = line[:200]  # ìµœëŒ€ 200ì
                                            break
                                    
                                    logger.info(f"ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ ({selector}): {len(content)}ì")
                                    break
                        except Exception as e:
                            logger.debug(f"ìš”ì†Œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
                            continue
                
                if content:
                    break
                    
            except Exception as e:
                logger.debug(f"Selector {selector} ì˜¤ë¥˜: {str(e)}")
                continue
        
        # ì½˜í…ì¸ ê°€ ì¶”ì¶œë˜ì§€ ì•Šì€ ê²½ìš°
        if not content:
            logger.warning(f"ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {post_url}")
            content = "ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ì„ ìœ„í•´ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”."
            content_summary = "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"
        
        # ìºì‹œì— ì €ì¥
        content_cache[url_hash] = {
            'content': content,
            'summary': content_summary,
            'timestamp': datetime.now().isoformat(),
            'url': post_url,
            'source': source
        }
        save_content_cache(content_cache)
        
    except Exception as e:
        logger.error(f"ê²Œì‹œê¸€ ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜ {post_url}: {str(e)}")
        content = f"ë³¸ë¬¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        content_summary = "ì˜¤ë¥˜ ë°œìƒ"
    
    finally:
        if should_quit_driver and driver:
            try:
                driver.quit()
            except:
                pass
    
    return content, content_summary

# =============================================================================
# Stove ê²Œì‹œíŒ í¬ë¡¤ë§
# =============================================================================

def fetch_stove_bug_board(force_crawl: bool = False, schedule_type: str = 'frequent') -> List[Dict]:
    """
    Stove ì—í”½ì„¸ë¸ ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§
    
    Args:
        force_crawl: ê°•ì œ í¬ë¡¤ë§ ì—¬ë¶€
        schedule_type: ìŠ¤ì¼€ì¤„ íƒ€ì…
        
    Returns:
        List[Dict]: ê²Œì‹œê¸€ ëª©ë¡
    """
    url = "https://page.onstove.com/epicseven/global/list/e7en001?listType=2&direction=latest&display_opt=usertag%2Cdate%2Ctime%2Cthumb%2Crecommend%2Ccomment_cnt&st=&sw=&mylist=&isRecommendSort="
    return crawl_stove_board(url, "stove_bug", force_crawl, schedule_type)

def fetch_stove_general_board(force_crawl: bool = False, schedule_type: str = 'regular') -> List[Dict]:
    """
    Stove ì—í”½ì„¸ë¸ ì¼ë°˜ ê²Œì‹œíŒ í¬ë¡¤ë§
    
    Args:
        force_crawl: ê°•ì œ í¬ë¡¤ë§ ì—¬ë¶€
        schedule_type: ìŠ¤ì¼€ì¤„ íƒ€ì…
        
    Returns:
        List[Dict]: ê²Œì‹œê¸€ ëª©ë¡
    """
    url = "https://page.onstove.com/epicseven/global/list/e7en002?listType=2&direction=latest&display_opt=usertag%2Cdate%2Ctime%2Cthumb%2Crecommend%2Ccomment_cnt&st=&sw=&mylist=&isRecommendSort="
    return crawl_stove_board(url, "stove_general", force_crawl, schedule_type)

def crawl_stove_board(board_url: str, source: str, force_crawl: bool = False, schedule_type: str = 'frequent') -> List[Dict]:
    """
    Stove ê²Œì‹œíŒ í¬ë¡¤ë§ (í†µí•©)
    
    Args:
        board_url: ê²Œì‹œíŒ URL
        source: ì†ŒìŠ¤ ì‹ë³„ì
        force_crawl: ê°•ì œ í¬ë¡¤ë§ ì—¬ë¶€
        schedule_type: ìŠ¤ì¼€ì¤„ íƒ€ì…
        
    Returns:
        List[Dict]: ê²Œì‹œê¸€ ëª©ë¡
    """
    posts = []
    driver = None
    
    try:
        logger.info(f"Stove ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹œì‘: {source}" + (f" (Force Crawl)" if force_crawl else ""))
        
        # ì¤‘ë³µ ë°©ì§€ ì‹œìŠ¤í…œ ë¡œë“œ
        crawled_data = load_crawled_links()
        crawled_links = set(crawled_data.get('links', []))
        
        driver = get_chrome_driver()
        driver.get(board_url)
        
        # ìŠ¤ì¼€ì¤„ì— ë”°ë¥¸ ëŒ€ê¸°ì‹œê°„ ì„¤ì •
        if schedule_type == 'frequent':
            wait_time = CrawlingSchedule.FREQUENT_WAIT_TIME
        else:
            wait_time = CrawlingSchedule.REGULAR_WAIT_TIME
        
        time.sleep(wait_time)
        
        # DOM ì¤€ë¹„ ìƒíƒœ í™•ì¸
        WebDriverWait(driver, 30).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        
        # JavaScriptë¡œ ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ
        posts_script = """
        const posts = [];
        const postElements = document.querySelectorAll('h3.s-board-title');
        
        postElements.forEach((element, index) => {
            try {
                const linkElement = element.querySelector('a') || element.querySelector('span.s-board-title-text').parentElement;
                const titleElement = element.querySelector('span.s-board-title-text');
                
                if (linkElement && titleElement) {
                    const href = linkElement.href || linkElement.getAttribute('href');
                    const title = titleElement.textContent.trim();
                    
                    if (href && title) {
                        const fullUrl = href.startsWith('http') ? href : 'https://page.onstove.com' + href;
                        
                        // ID ì¶”ì¶œ
                        const idMatch = fullUrl.match(/view\/(\d+)/);
                        const id = idMatch ? idMatch[1] : '';
                        
                        // ê³µì§€ì‚¬í•­ ì œì™¸
                        const noticeElement = element.querySelector('i.element-badge__s.notice, i.element-badge__s.event');
                        if (!noticeElement) {
                            posts.push({
                                href: fullUrl,
                                id: id,
                                title: title
                            });
                        }
                    }
                }
            } catch (e) {
                console.log('ê²Œì‹œê¸€ ì¶”ì¶œ ì˜¤ë¥˜:', e);
            }
        });
        
        return posts;
        """
        
        js_posts = driver.execute_script(posts_script)
        logger.info(f"JavaScript í¬ë¡¤ë§ ì„±ê³µ: {len(js_posts)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")
        
        if not js_posts:
            logger.warning("JavaScriptë¡œ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return []
        
        # ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
        for post_data in js_posts:
            href = post_data.get('href', '')
            title = post_data.get('title', '')
            post_id = post_data.get('id', '')
            
            if not href or not title:
                continue
            
            # Force Crawlì´ ì•„ë‹ ë•Œ ì¤‘ë³µ ì²´í¬
            if not force_crawl and href in crawled_links:
                logger.debug(f"ì¤‘ë³µ ê²Œì‹œê¸€ ìŠ¤í‚µ: {title[:30]}...")
                continue
            
            try:
                # ê²Œì‹œê¸€ ë³¸ë¬¸ ì¶”ì¶œ
                content, content_summary = get_stove_post_content(
                    href, driver, source, schedule_type
                )
                
                post = {
                    'title': title,
                    'url': href,
                    'content': content,
                    'content_summary': content_summary,
                    'timestamp': datetime.now().isoformat(),
                    'source': source,
                    'post_id': post_id
                }
                
                posts.append(post)
                
                # ìƒˆ ë§í¬ ì¶”ê°€ (ì¤‘ë³µ ë°©ì§€)
                if href not in crawled_links:
                    crawled_data['links'].append(href)
                    crawled_links.add(href)
                
                logger.debug(f"ê²Œì‹œê¸€ ìˆ˜ì§‘ ì™„ë£Œ: {title[:40]}...")
                
                # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ì§€ì—°
                time.sleep(1)
                
            except Exception as e:
                logger.error(f"ê²Œì‹œê¸€ ì²˜ë¦¬ ì˜¤ë¥˜ {href}: {str(e)}")
                continue
        
        # ì¤‘ë³µ ë°©ì§€ ë°ì´í„° ì €ì¥
        save_crawled_links(crawled_data)
        
        # ë””ë²„ê·¸ HTML ì €ì¥
        try:
            with open(f'{source}_debug_selenium.html', 'w', encoding='utf-8') as f:
                f.write(driver.page_source)
        except:
            pass
        
        logger.info(f"Stove ê²Œì‹œíŒ í¬ë¡¤ë§ ì™„ë£Œ: {len(posts)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘")
        
    except Exception as e:
        logger.error(f"Stove ê²Œì‹œíŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
        return []
    
    finally:
        if driver:
            try:
                driver.quit()
            except:
                pass
    
    return posts

# =============================================================================
# ì£¼ê¸°ë³„ í¬ë¡¤ë§ ì‹¤í–‰ í•¨ìˆ˜ë“¤
# =============================================================================

def crawl_frequent_sites(force_crawl: bool = False) -> List[Dict]:
    """
    15ë¶„ ì£¼ê¸° í¬ë¡¤ë§ (ë²„ê·¸ ê²Œì‹œíŒ)
    
    Args:
        force_crawl: ê°•ì œ í¬ë¡¤ë§ ì—¬ë¶€
        
    Returns:
        List[Dict]: ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ ëª©ë¡
    """
    logger.info("ğŸ”¥ === 15ë¶„ ê°„ê²© í¬ë¡¤ë§ ì‹œì‘ (ë²„ê·¸ ê²Œì‹œíŒ" + (", force_crawl=True)" if force_crawl else ") ==="))
    
    all_posts = []
    
    # Stove ë²„ê·¸ ê²Œì‹œíŒ
    try:
        stove_posts = fetch_stove_bug_board(force_crawl, 'frequent')
        all_posts.extend(stove_posts)
        logger.info(f"Stove ë²„ê·¸ ê²Œì‹œíŒ: {len(stove_posts)}ê°œ ê²Œì‹œê¸€")
    except Exception as e:
        logger.error(f"Stove ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
    
    logger.info(f"ğŸ”¥ === 15ë¶„ ê°„ê²© í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_posts)}ê°œ ê²Œì‹œê¸€ ===")
    return all_posts

def crawl_regular_sites(force_crawl: bool = False) -> List[Dict]:
    """
    30ë¶„ ì£¼ê¸° í¬ë¡¤ë§ (ì¼ë°˜ ê²Œì‹œíŒ)
    
    Args:
        force_crawl: ê°•ì œ í¬ë¡¤ë§ ì—¬ë¶€
        
    Returns:
        List[Dict]: ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ ëª©ë¡
    """
    logger.info("â° === 30ë¶„ ê°„ê²© í¬ë¡¤ë§ ì‹œì‘ (ì¼ë°˜ ê²Œì‹œíŒ" + (", force_crawl=True)" if force_crawl else ") ==="))
    
    all_posts = []
    
    # Stove ì¼ë°˜ ê²Œì‹œíŒ
    try:
        stove_posts = fetch_stove_general_board(force_crawl, 'regular')
        all_posts.extend(stove_posts)
        logger.info(f"Stove ì¼ë°˜ ê²Œì‹œíŒ: {len(stove_posts)}ê°œ ê²Œì‹œê¸€")
    except Exception as e:
        logger.error(f"Stove ì¼ë°˜ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
    
    logger.info(f"â° === 30ë¶„ ê°„ê²© í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_posts)}ê°œ ê²Œì‹œê¸€ ===")
    return all_posts

def crawl_by_schedule(current_time: datetime = None, force_crawl: bool = False) -> List[Dict]:
    """
    ìŠ¤ì¼€ì¤„ì— ë”°ë¥¸ í¬ë¡¤ë§ ì‹¤í–‰
    
    Args:
        current_time: í˜„ì¬ ì‹œê°„ (í…ŒìŠ¤íŠ¸ìš©)
        force_crawl: ê°•ì œ í¬ë¡¤ë§ ì—¬ë¶€
        
    Returns:
        List[Dict]: ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ ëª©ë¡
    """
    if current_time is None:
        current_time = datetime.now()
    
    all_posts = []
    
    # 15ë¶„ë§ˆë‹¤ ì‹¤í–‰ (ë²„ê·¸ ê²Œì‹œíŒ)
    if current_time.minute % CrawlingSchedule.FREQUENT_INTERVAL == 0 or force_crawl:
        frequent_posts = crawl_frequent_sites(force_crawl)
        all_posts.extend(frequent_posts)
    
    # 30ë¶„ë§ˆë‹¤ ì‹¤í–‰ (ì¼ë°˜ ê²Œì‹œíŒ)
    if current_time.minute % CrawlingSchedule.REGULAR_INTERVAL == 0 or force_crawl:
        regular_posts = crawl_regular_sites(force_crawl)
        all_posts.extend(regular_posts)
    
    return all_posts

# =============================================================================
# ë¦¬í¬íŠ¸ìš© ë°ì´í„° ìˆ˜ì§‘
# =============================================================================

def get_all_posts_for_report(hours: int = 24, force_crawl: bool = False) -> List[Dict]:
    """
    ë¦¬í¬íŠ¸ ìƒì„±ì„ ìœ„í•œ ì „ì²´ ê²Œì‹œê¸€ ìˆ˜ì§‘
    
    Args:
        hours: ìˆ˜ì§‘í•  ì‹œê°„ ë²”ìœ„ (ì‹œê°„)
        force_crawl: ê°•ì œ í¬ë¡¤ë§ ì—¬ë¶€
        
    Returns:
        List[Dict]: ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ ëª©ë¡
    """
    logger.info(f"ğŸ“Š === ë¦¬í¬íŠ¸ìš© ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ (ìµœê·¼ {hours}ì‹œê°„" + (", force_crawl=True)" if force_crawl else ") ==="))
    
    all_posts = []
    
    # ëª¨ë“  ê²Œì‹œíŒì—ì„œ ë°ì´í„° ìˆ˜ì§‘
    try:
        # ë²„ê·¸ ê²Œì‹œíŒ
        stove_bug_posts = fetch_stove_bug_board(force_crawl, 'frequent')
        all_posts.extend(stove_bug_posts)
        
        # ì¼ë°˜ ê²Œì‹œíŒ
        stove_general_posts = fetch_stove_general_board(force_crawl, 'regular')
        all_posts.extend(stove_general_posts)
        
    except Exception as e:
        logger.error(f"ë¦¬í¬íŠ¸ìš© ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {str(e)}")
    
    # ì‹œê°„ ë²”ìœ„ í•„í„°ë§
    cutoff_time = datetime.now() - timedelta(hours=hours)
    filtered_posts = []
    
    for post in all_posts:
        try:
            post_time = datetime.fromisoformat(post.get('timestamp', ''))
            if post_time >= cutoff_time:
                filtered_posts.append(post)
        except:
            # timestamp íŒŒì‹± ì‹¤íŒ¨ ì‹œ í¬í•¨
            filtered_posts.append(post)
    
    logger.info(f"ğŸ“Š === ë¦¬í¬íŠ¸ìš© ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(filtered_posts)}ê°œ ê²Œì‹œê¸€ ===")
    return filtered_posts

# =============================================================================
# ë©”ì¸ ì‹¤í–‰ ë¶€
# =============================================================================

if __name__ == "__main__":
    import argparse
    
    # ë¡œê¹… ì„¤ì •
    setup_logging()
    
    # ëª…ë ¹í–‰ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(description='Epic7 í¬ë¡¤ëŸ¬ v3.3')
    parser.add_argument('--force-crawl', action='store_true', help='ê°•ì œ í¬ë¡¤ë§ (ì¤‘ë³µ ì²´í¬ ë¬´ì‹œ)')
    parser.add_argument('--schedule', action='store_true', help='ìŠ¤ì¼€ì¤„ì— ë”°ë¥¸ í¬ë¡¤ë§')
    parser.add_argument('--frequent', action='store_true', help='15ë¶„ ì£¼ê¸° í¬ë¡¤ë§ë§Œ ì‹¤í–‰')
    parser.add_argument('--regular', action='store_true', help='30ë¶„ ì£¼ê¸° í¬ë¡¤ë§ë§Œ ì‹¤í–‰')
    parser.add_argument('--report', type=int, metavar='HOURS', help='ë¦¬í¬íŠ¸ìš© ë°ì´í„° ìˆ˜ì§‘ (ì‹œê°„ ì§€ì •)')
    
    args = parser.parse_args()
    
    try:
        if args.report:
            # ë¦¬í¬íŠ¸ìš© ë°ì´í„° ìˆ˜ì§‘
            posts = get_all_posts_for_report(args.report, args.force_crawl)
            print(f"ë¦¬í¬íŠ¸ìš© ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(posts)}ê°œ ê²Œì‹œê¸€")
            
        elif args.frequent:
            # 15ë¶„ ì£¼ê¸° í¬ë¡¤ë§ë§Œ ì‹¤í–‰
            posts = crawl_frequent_sites(args.force_crawl)
            print(f"15ë¶„ ì£¼ê¸° í¬ë¡¤ë§ ì™„ë£Œ: {len(posts)}ê°œ ê²Œì‹œê¸€")
            
        elif args.regular:
            # 30ë¶„ ì£¼ê¸° í¬ë¡¤ë§ë§Œ ì‹¤í–‰
            posts = crawl_regular_sites(args.force_crawl)
            print(f"30ë¶„ ì£¼ê¸° í¬ë¡¤ë§ ì™„ë£Œ: {len(posts)}ê°œ ê²Œì‹œê¸€")
            
        elif args.schedule:
            # ìŠ¤ì¼€ì¤„ì— ë”°ë¥¸ í¬ë¡¤ë§
            posts = crawl_by_schedule(force_crawl=args.force_crawl)
            print(f"ìŠ¤ì¼€ì¤„ ê¸°ë°˜ í¬ë¡¤ë§ ì™„ë£Œ: {len(posts)}ê°œ ê²Œì‹œê¸€")
            
        else:
            # ì „ì²´ í¬ë¡¤ë§ (ê¸°ë³¸)
            posts = []
            posts.extend(crawl_frequent_sites(args.force_crawl))
            posts.extend(crawl_regular_sites(args.force_crawl))
            print(f"ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ: {len(posts)}ê°œ ê²Œì‹œê¸€")
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        if posts:
            print(f"\n=== ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½ ===")
            for post in posts[:5]:  # ìƒìœ„ 5ê°œë§Œ ì¶œë ¥
                print(f"- {post.get('title', 'ì œëª© ì—†ìŒ')[:50]}... [{post.get('source', 'Unknown')}]")
        
    except KeyboardInterrupt:
        print("\ní¬ë¡¤ë§ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
        print(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")