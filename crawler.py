#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Epic7 ë‹¤êµ­ê°€ í¬ë¡¤ëŸ¬ v4.4 - 6ê°œ ì†ŒìŠ¤ ì™„ì „ êµ¬í˜„ ì™„ì„±í˜•
Master ìš”êµ¬ì‚¬í•­: ê²Œì‹œê¸€ë³„ ì¦‰ì‹œ ì²˜ë¦¬ (í¬ë¡¤ë§â†’ê°ì„±ë¶„ì„â†’ì•Œë¦¼â†’ë§ˆí‚¹)

í•µì‹¬ êµ¬í˜„ì‚¬í•­:
- 6ê°œ í¬ë¡¤ë§ ì†ŒìŠ¤ ì™„ì „ êµ¬í˜„ (STOVE 4ê°œ + Reddit + ë£¨ë¦¬ì›¹)
- ë£¨ë¦¬ì›¹ íŒŒì‹± ë¡œì§ ì™„ì „ êµ¬í˜„ (ì‹ ê·œ ì¶”ê°€)
- ê²Œì‹œê¸€ë³„ ì¦‰ì‹œ ì²˜ë¦¬ ì™„ì „ êµ¬í˜„
- ì—ëŸ¬ ê²©ë¦¬ ë° ë³µì›ë ¥ ê°•í™”
- ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ ìë™ ê´€ë¦¬
- Dev ì •ì±… ê¸°ì¤€ ì ìš©
- í•œêµ­/ê¸€ë¡œë²Œ ë¶„ë¦¬ êµ¬ì¡° ì§€ì›

Author: Epic7 Monitoring Team  
Version: 4.4 (6ê°œ ì†ŒìŠ¤ ì™„ì „ êµ¬í˜„)
Date: 2025-07-28
Fixed: ë£¨ë¦¬ì›¹ íŒŒì‹± ë¡œì§ ì™„ì „ êµ¬í˜„
"""

import time
import random
import re
import requests
import concurrent.futures
import os
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Callable
from urllib.parse import urljoin, urlparse

# Selenium ê´€ë ¨ import
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.keys import Keys

# âœ¨ ì‹ ê·œ ì¶”ê°€: BeautifulSoup ì„í¬íŠ¸ (ë£¨ë¦¬ì›¹ íŒŒì‹±ìš©)
try:
    from bs4 import BeautifulSoup
    BEAUTIFULSOUP_AVAILABLE = True
    print("[INFO] BeautifulSoup4 ë¡œë“œ ì™„ë£Œ - ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì§€ì›")
except ImportError:
    print("[WARNING] BeautifulSoup4 ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    BEAUTIFULSOUP_AVAILABLE = False

# Epic7 ì‹œìŠ¤í…œ ëª¨ë“ˆ import (ì¦‰ì‹œ ì²˜ë¦¬ìš©)
try:
    from classifier import Epic7Classifier, is_bug_post, is_high_priority_bug, should_send_realtime_alert
    from notifier import send_bug_alert, send_sentiment_notification
    from sentiment_data_manager import save_sentiment_data, get_sentiment_summary
    EPIC7_MODULES_AVAILABLE = True
    print("[INFO] Epic7 ì²˜ë¦¬ ëª¨ë“ˆë“¤ ë¡œë“œ ì™„ë£Œ")
except ImportError as e:
    print(f"[WARNING] Epic7 ì²˜ë¦¬ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("[WARNING] ì¦‰ì‹œ ì²˜ë¦¬ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
    EPIC7_MODULES_AVAILABLE = False

# Reddit í¬ë¡¤ë§ìš© import
try:
    import praw
    REDDIT_AVAILABLE = True
    print("[INFO] PRAW ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ - Reddit í¬ë¡¤ë§ ì§€ì›")
except ImportError:
    print("[WARNING] PRAW ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Reddit í¬ë¡¤ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    REDDIT_AVAILABLE = False

# =============================================================================
# ğŸš€ Master ìš”êµ¬ì‚¬í•­: ì¦‰ì‹œ ì²˜ë¦¬ ì‹œìŠ¤í…œ êµ¬í˜„
# =============================================================================

class ImmediateProcessor:
    """ê²Œì‹œê¸€ë³„ ì¦‰ì‹œ ì²˜ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.processed_count = 0
        self.failed_count = 0
        self.retry_queue = []
        self.classifier = None
        
        if EPIC7_MODULES_AVAILABLE:
            try:
                self.classifier = Epic7Classifier()
                print("[INFO] ì¦‰ì‹œ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"[ERROR] ë¶„ë¥˜ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                
    def process_post_immediately(self, post_data: Dict) -> bool:
        """
        ê²Œì‹œê¸€ë³„ ì¦‰ì‹œ ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜
        Master ìš”êµ¬ì‚¬í•­: í¬ë¡¤ë§ â†’ ê°ì„±ë¶„ì„ â†’ ì•Œë¦¼ â†’ ë§ˆí‚¹
        """
        try:
            print(f"[IMMEDIATE] ì¦‰ì‹œ ì²˜ë¦¬ ì‹œì‘: {post_data.get('title', '')[:50]}...")
            
            if not EPIC7_MODULES_AVAILABLE:
                print("[WARNING] ì²˜ë¦¬ ëª¨ë“ˆ ì—†ìŒ, ê¸°ë³¸ ì²˜ë¦¬ë§Œ ìˆ˜í–‰")
                self._basic_processing(post_data)
                return True
            
            # 1. ìœ ì € ë™í–¥ ê°ì„± ë¶„ì„
            sentiment_result = self._analyze_sentiment(post_data)
            
            # 2. ì•Œë¦¼ ì „ì†¡ ì—¬ë¶€ ì²´í¬ ë° ì „ì†¡
            notification_sent = self._handle_notifications(post_data, sentiment_result)
            
            # 3. ì²˜ë¦¬ ì™„ë£Œ ë§ˆí‚¹ (ì•Œë¦¼ ì„±ê³µ ì‹œì—ë§Œ)
            if notification_sent:
                self._mark_as_processed(post_data['url'], notified=True)
                self.processed_count += 1
                print(f"[SUCCESS] ì¦‰ì‹œ ì²˜ë¦¬ ì™„ë£Œ: {post_data.get('title', '')[:30]}...")
            else:
                # ì‹¤íŒ¨í•œ ê²½ìš° ì¬ì‹œë„ íì— ì¶”ê°€
                self._add_to_retry_queue(post_data, sentiment_result)
                self.failed_count += 1
                
            return notification_sent
            
        except Exception as e:
            print(f"[ERROR] ì¦‰ì‹œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            self._add_to_retry_queue(post_data, None)
            self.failed_count += 1
            return False
    
    def _analyze_sentiment(self, post_data: Dict) -> Dict:
        """ê°ì„± ë¶„ì„ ìˆ˜í–‰"""
        try:
            if not self.classifier:
                return {"sentiment": "neutral", "confidence": 0.5}
                
            result = self.classifier.classify_post(post_data)
            print(f"[SENTIMENT] ë¶„ì„ ê²°ê³¼: {result.get('sentiment', 'unknown')}")
            return result
            
        except Exception as e:
            print(f"[ERROR] ê°ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"sentiment": "neutral", "confidence": 0.0, "error": str(e)}
    
    def _handle_notifications(self, post_data: Dict, sentiment_result: Dict) -> bool:
        """ë¶„ë¥˜ë³„ ì•Œë¦¼ ì²˜ë¦¬"""
        try:
            source = post_data.get('source', '')
            sentiment = sentiment_result.get('sentiment', 'neutral')
            
            # Master ìš”êµ¬ì‚¬í•­: ë²„ê·¸ ê²Œì‹œíŒ ê¸€ì´ë¼ë©´ ì‹¤ì‹œê°„ ë²„ê·¸ ë©”ì‹œì§€
            if source.endswith('_bug') or 'bug' in source.lower():
                print("[ALERT] ë²„ê·¸ ê²Œì‹œíŒ ê¸€ â†’ ì¦‰ì‹œ ë²„ê·¸ ì•Œë¦¼")
                return self._send_bug_alert(post_data)
            
            # Master ìš”êµ¬ì‚¬í•­: ë™í–¥ ë¶„ì„ í›„ ë²„ê·¸ë¡œ ë¶„ë¥˜ëœ ê¸€ë„ ì‹¤ì‹œê°„ ë²„ê·¸ ë©”ì‹œì§€
            elif is_bug_post(sentiment_result) or should_send_realtime_alert(sentiment_result):
                print("[ALERT] ë²„ê·¸ ë¶„ë¥˜ ê¸€ â†’ ì¦‰ì‹œ ë²„ê·¸ ì•Œë¦¼")
                return self._send_bug_alert(post_data)
            
            # Master ìš”êµ¬ì‚¬í•­: ê¸ì •/ì¤‘ë¦½/ë¶€ì • ë™í–¥ì€ ê°ì„± ì•Œë¦¼ + ì €ì¥
            else:
                print(f"[ALERT] ê°ì„± ë™í–¥ ê¸€ ({sentiment}) â†’ ì¦‰ì‹œ ê°ì„± ì•Œë¦¼")
                return self._send_sentiment_alert(post_data, sentiment_result)
                
        except Exception as e:
            print(f"[ERROR] ì•Œë¦¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return False
    
    def _send_bug_alert(self, post_data: Dict) -> bool:
        """ë²„ê·¸ ì•Œë¦¼ ì „ì†¡"""
        try:
            success = send_bug_alert(post_data)
            if success:
                print("[SUCCESS] ë²„ê·¸ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ")
            else:
                print("[FAILED] ë²„ê·¸ ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨")
            return success
        except Exception as e:
            print(f"[ERROR] ë²„ê·¸ ì•Œë¦¼ ì „ì†¡ ì˜¤ë¥˜: {e}")
            return False
    
    def _send_sentiment_alert(self, post_data: Dict, sentiment_result: Dict) -> bool:
        """ê°ì„± ì•Œë¦¼ ì „ì†¡ ë° ë°ì´í„° ì €ì¥"""
        try:
            # Master ìš”êµ¬ì‚¬í•­: ì¼ê°„ ë¦¬í¬íŠ¸ìš© ë°ì´í„° ì €ì¥
            save_success = save_sentiment_data(post_data, sentiment_result)
            
            # ì¦‰ì‹œ ê°ì„± ì•Œë¦¼ ì „ì†¡
            alert_success = send_sentiment_notification(post_data, sentiment_result)
            
            if save_success and alert_success:
                print("[SUCCESS] ê°ì„± ì•Œë¦¼ ì „ì†¡ ë° ë°ì´í„° ì €ì¥ ì™„ë£Œ")
                return True
            else:
                print(f"[PARTIAL] ì €ì¥: {save_success}, ì•Œë¦¼: {alert_success}")
                return False
                
        except Exception as e:
            print(f"[ERROR] ê°ì„± ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return False
    
    def _mark_as_processed(self, url: str, notified: bool = True):
        """ì²˜ë¦¬ ì™„ë£Œ ë§ˆí‚¹"""
        try:
            mark_as_processed(url, notified)
        except Exception as e:
            print(f"[ERROR] ë§ˆí‚¹ ì‹¤íŒ¨: {e}")
    
    def _add_to_retry_queue(self, post_data: Dict, sentiment_result: Optional[Dict]):
        """ì¬ì‹œë„ íì— ì¶”ê°€"""
        retry_item = {
            "post_data": post_data,
            "sentiment_result": sentiment_result,
            "timestamp": datetime.now().isoformat(),
            "retry_count": 0
        }
        self.retry_queue.append(retry_item)
        print(f"[RETRY] ì¬ì‹œë„ í ì¶”ê°€: {len(self.retry_queue)}ê°œ ëŒ€ê¸°ì¤‘")
    
    def _basic_processing(self, post_data: Dict):
        """ê¸°ë³¸ ì²˜ë¦¬ (ëª¨ë“ˆ ì—†ì„ ë•Œ)"""
        print(f"[BASIC] ê¸°ë³¸ ì²˜ë¦¬: {post_data.get('title', '')[:50]}...")
        self._mark_as_processed(post_data['url'], notified=False)
    
    def process_retry_queue(self):
        """ì¬ì‹œë„ í ì²˜ë¦¬"""
        if not self.retry_queue:
            return
            
        print(f"[RETRY] ì¬ì‹œë„ í ì²˜ë¦¬ ì‹œì‘: {len(self.retry_queue)}ê°œ")
        processed_items = []
        
        for item in self.retry_queue:
            try:
                if item["retry_count"] >= 3:
                    print("[SKIP] ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                    processed_items.append(item)
                    continue
                
                item["retry_count"] += 1
                success = self.process_post_immediately(item["post_data"])
                
                if success:
                    processed_items.append(item)
                    
            except Exception as e:
                print(f"[ERROR] ì¬ì‹œë„ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        # ì²˜ë¦¬ ì™„ë£Œëœ í•­ëª©ë“¤ ì œê±°
        for item in processed_items:
            self.retry_queue.remove(item)
        
        print(f"[RETRY] ì¬ì‹œë„ ì™„ë£Œ: {len(processed_items)}ê°œ ì²˜ë¦¬, {len(self.retry_queue)}ê°œ ë‚¨ìŒ")
    
    def get_stats(self) -> Dict:
        """ì²˜ë¦¬ í†µê³„ ë°˜í™˜"""
        return {
            "processed": self.processed_count,
            "failed": self.failed_count,
            "retry_queue": len(self.retry_queue)
        }

# ì „ì—­ ì¦‰ì‹œ ì²˜ë¦¬ê¸° ì¸ìŠ¤í„´ìŠ¤
immediate_processor = ImmediateProcessor()

# =============================================================================
# í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ ì„¤ì • í´ë˜ìŠ¤
# =============================================================================

class CrawlingSchedule:
    """í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ë³„ ì„¤ì • ê´€ë¦¬"""

    FREQUENT_WAIT_TIME = 25      # 15ë¶„ ì£¼ê¸° ëŒ€ê¸°ì‹œê°„ (ìµœì í™”)
    REGULAR_WAIT_TIME = 30       # 30ë¶„ ì£¼ê¸° ëŒ€ê¸°ì‹œê°„  
    REDDIT_WAIT_TIME = 15        # Reddit ëŒ€ê¸°ì‹œê°„
    RULIWEB_WAIT_TIME = 20       # ë£¨ë¦¬ì›¹ ëŒ€ê¸°ì‹œê°„

    # ìŠ¤í¬ë¡¤ íšŸìˆ˜ ì„¤ì •
    FREQUENT_SCROLL_COUNT = 2    # 15ë¶„ ì£¼ê¸° ìŠ¤í¬ë¡¤ (ì„±ëŠ¥ ìµœì í™”)
    REGULAR_SCROLL_COUNT = 3

    @staticmethod
    def get_wait_time(schedule_type: str) -> int:
        """ìŠ¤ì¼€ì¤„ íƒ€ì…ë³„ ëŒ€ê¸°ì‹œê°„ ë°˜í™˜"""
        if schedule_type == 'frequent':
            return CrawlingSchedule.FREQUENT_WAIT_TIME
        elif schedule_type == 'regular':
            return CrawlingSchedule.REGULAR_WAIT_TIME
        elif schedule_type == 'reddit':
            return CrawlingSchedule.REDDIT_WAIT_TIME
        elif schedule_type == 'ruliweb':
            return CrawlingSchedule.RULIWEB_WAIT_TIME
        else:
            return CrawlingSchedule.REGULAR_WAIT_TIME

    @staticmethod
    def get_scroll_count(schedule_type: str) -> int:
        """ìŠ¤ì¼€ì¤„ íƒ€ì…ë³„ ìŠ¤í¬ë¡¤ íšŸìˆ˜ ë°˜í™˜"""
        if schedule_type == 'frequent':
            return CrawlingSchedule.FREQUENT_SCROLL_COUNT
        else:
            return CrawlingSchedule.REGULAR_SCROLL_COUNT

# =============================================================================
# íŒŒì¼ ê´€ë¦¬ ì‹œìŠ¤í…œ - ì‹œê°„ ê¸°ë°˜ ì¤‘ë³µ ê´€ë¦¬ ê°œì„ 
# =============================================================================

def get_crawled_links_file():
    """ì›Œí¬í”Œë¡œìš°ë³„ ë…ë¦½ì ì¸ í¬ë¡¤ë§ ë§í¬ íŒŒì¼ëª… ìƒì„±"""
    workflow_name = os.environ.get('GITHUB_WORKFLOW', 'default')

    if 'debug' in workflow_name.lower() or 'test' in workflow_name.lower():
        return "crawled_links_debug.json"
    elif 'monitor' in workflow_name.lower():
        return "crawled_links_monitor.json"
    else:
        return "crawled_links.json"

def get_content_cache_file():
    """ì›Œí¬í”Œë¡œìš°ë³„ ë…ë¦½ì ì¸ ì½˜í…ì¸  ìºì‹œ íŒŒì¼ëª… ìƒì„±"""
    workflow_name = os.environ.get('GITHUB_WORKFLOW', 'default')

    if 'debug' in workflow_name.lower():
        return "content_cache_debug.json"
    else:
        return "content_cache.json"

def load_crawled_links():
    """í¬ë¡¤ë§ ë§í¬ ë¡œë“œ - ì‹œê°„ ê¸°ë°˜ êµ¬ì¡° ì ìš©"""
    crawled_links_file = get_crawled_links_file()
    
    if os.path.exists(crawled_links_file):
        try:
            with open(crawled_links_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
                # ê¸°ì¡´ ë‹¨ìˆœ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¥¼ ìƒˆ êµ¬ì¡°ë¡œ ë³€í™˜
                if isinstance(data, dict) and "links" in data:
                    if isinstance(data["links"], list) and len(data["links"]) > 0:
                        # ê¸°ì¡´ ë‹¨ìˆœ ë§í¬ë¥¼ ì‹œê°„ êµ¬ì¡°ë¡œ ë³€í™˜
                        if isinstance(data["links"][0], str):
                            converted_links = []
                            for link in data["links"]:
                                converted_links.append({
                                    "url": link,
                                    "processed_at": (datetime.now() - timedelta(hours=25)).isoformat(),
                                    "notified": False
                                })
                            data["links"] = converted_links
                            print(f"[INFO] ê¸°ì¡´ {len(converted_links)}ê°œ ë§í¬ë¥¼ ìƒˆ êµ¬ì¡°ë¡œ ë³€í™˜")
                
                # 24ì‹œê°„ ì§€ë‚œ í•­ëª© ìë™ ì œê±°
                now = datetime.now()
                valid_links = []
                for item in data.get("links", []):
                    try:
                        processed_time = datetime.fromisoformat(item["processed_at"])
                        if now - processed_time < timedelta(hours=24):
                            valid_links.append(item)
                    except:
                        continue
                
                data["links"] = valid_links
                print(f"[INFO] 24ì‹œê°„ ê¸°ì¤€ ìœ íš¨í•œ ë§í¬: {len(valid_links)}ê°œ")
                return data
                        
        except Exception as e:
            print(f"[WARNING] í¬ë¡¤ë§ ë§í¬ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
    
    return {"links": [], "last_updated": datetime.now().isoformat()}

def save_crawled_links(link_data):
    """í¬ë¡¤ë§ ë§í¬ ì €ì¥ - ì ê·¹ì  í¬ê¸° ê´€ë¦¬"""
    try:
        # í¬ê¸° ì œí•œì„ 100ê°œë¡œ ì¶•ì†Œ (ë” ì ê·¹ì  ê´€ë¦¬)
        if len(link_data["links"]) > 100:
            # ìµœì‹  100ê°œë§Œ ìœ ì§€
            link_data["links"] = sorted(
                link_data["links"], 
                key=lambda x: x.get("processed_at", ""), 
                reverse=True
            )[:100]
            print(f"[INFO] ë§í¬ ëª©ë¡ì„ ìµœì‹  100ê°œë¡œ ì •ë¦¬")

        link_data["last_updated"] = datetime.now().isoformat()

        crawled_links_file = get_crawled_links_file()
        with open(crawled_links_file, 'w', encoding='utf-8') as f:
            json.dump(link_data, f, ensure_ascii=False, indent=2)
            
        print(f"[INFO] í¬ë¡¤ë§ ë§í¬ ì €ì¥ ì™„ë£Œ: {len(link_data['links'])}ê°œ")

    except Exception as e:
        print(f"[ERROR] ë§í¬ ì €ì¥ ì‹¤íŒ¨: {e}")

def is_recently_processed(url: str, links_data: List[Dict], hours: int = 24) -> bool:
    """ì‹œê°„ ê¸°ë°˜ ì¤‘ë³µ ì²´í¬ - 24ì‹œê°„ ë‚´ ì²˜ë¦¬ëœ ë§í¬ì¸ì§€ í™•ì¸"""
    try:
        now = datetime.now()
        for item in links_data:
            if item.get("url") == url:
                processed_time = datetime.fromisoformat(item["processed_at"])
                if now - processed_time < timedelta(hours=hours):
                    return True
        return False
    except Exception as e:
        print(f"[DEBUG] ì¤‘ë³µ ì²´í¬ ì˜¤ë¥˜: {e}")
        return False

def mark_as_processed(url: str, notified: bool = False):
    """ê²Œì‹œê¸€ì„ ì²˜ë¦¬ë¨ìœ¼ë¡œ ë§ˆí‚¹ - ì•Œë¦¼ ì„±ê³µ í›„ì—ë§Œ í˜¸ì¶œ"""
    try:
        link_data = load_crawled_links()
        
        # ê¸°ì¡´ í•­ëª© ì—…ë°ì´íŠ¸ ë˜ëŠ” ìƒˆ í•­ëª© ì¶”ê°€
        found = False
        for item in link_data["links"]:
            if item.get("url") == url:
                item["processed_at"] = datetime.now().isoformat()
                item["notified"] = notified
                found = True
                break
        
        if not found:
            link_data["links"].append({
                "url": url,
                "processed_at": datetime.now().isoformat(),
                "notified": notified
            })
        
        save_crawled_links(link_data)
        print(f"[INFO] ë§í¬ ì²˜ë¦¬ ì™„ë£Œ ë§ˆí‚¹: {url[:50]}... (ì•Œë¦¼: {notified})")
        
    except Exception as e:
        print(f"[ERROR] ë§í¬ ë§ˆí‚¹ ì‹¤íŒ¨: {e}")

def load_content_cache():
    """ê²Œì‹œê¸€ ë‚´ìš© ìºì‹œ ë¡œë“œ"""
    content_cache_file = get_content_cache_file()

    if os.path.exists(content_cache_file):
        try:
            with open(content_cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            print(f"[WARNING] {content_cache_file} íŒŒì¼ ì½ê¸° ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±")
    return {}

def save_content_cache(cache_data):
    """ê²Œì‹œê¸€ ë‚´ìš© ìºì‹œ ì €ì¥"""
    try:
        if len(cache_data) > 500:
            sorted_items = sorted(cache_data.items(), 
                                key=lambda x: x[1].get('timestamp', ''), reverse=True)
            cache_data = dict(sorted_items[:500])

        content_cache_file = get_content_cache_file()
        with open(content_cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)

    except Exception as e:
        print(f"[ERROR] ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

# =============================================================================
# Chrome Driver ê´€ë¦¬ - ë¦¬ì†ŒìŠ¤ ìµœì í™” ê°•í™”
# =============================================================================

def get_chrome_driver():
    """Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” - ë¦¬ì†ŒìŠ¤ ìµœì í™” ë° ì•ˆì •ì„± ê°•í™”"""
    options = Options()

    # ê¸°ë³¸ ì˜µì…˜ë“¤
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--disable-extensions')
    options.add_argument('--disable-plugins')
    options.add_argument('--disable-images')
    options.add_argument('--window-size=1920,1080')

    # ì¶”ê°€ ë¦¬ì†ŒìŠ¤ ìµœì í™” ì˜µì…˜
    options.add_argument('--memory-pressure-off')
    options.add_argument('--max_old_space_size=2048')
    options.add_argument('--disable-background-timer-throttling')
    options.add_argument('--disable-backgrounding-occluded-windows')
    options.add_argument('--disable-renderer-backgrounding')
    options.add_argument('--disable-features=TranslateUI')
    options.add_argument('--disable-default-apps')
    options.add_argument('--disable-web-security')
    options.add_argument('--disable-features=VizDisplayCompositor')

    # ë´‡ íƒì§€ ìš°íšŒ
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)

    # ëœë¤ User-Agent
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36'
    ]
    options.add_argument(f'--user-agent={random.choice(user_agents)}')

    # ì„±ëŠ¥ ìµœì í™”
    prefs = {
        'profile.default_content_setting_values': {
            'images': 2, 'plugins': 2, 'popups': 2,
            'geolocation': 2, 'notifications': 2, 'media_stream': 2,
        }
    }
    options.add_experimental_option('prefs', prefs)

    # 3ë‹¨ê³„ í´ë°± ë©”ì»¤ë‹ˆì¦˜
    possible_paths = [
        '/usr/bin/chromedriver',
        '/usr/local/bin/chromedriver', 
        '/snap/bin/chromium.chromedriver'
    ]

    # 1ë‹¨ê³„: ì‹œìŠ¤í…œ ê²½ë¡œë“¤ ì‹œë„
    for path in possible_paths:
        try:
            if os.path.exists(path):
                print(f"[DEBUG] ChromeDriver ì‹œë„: {path}")
                service = Service(path)
                driver = webdriver.Chrome(service=service, options=options)
                print(f"[DEBUG] ChromeDriver ì„±ê³µ: {path}")
                return driver
        except Exception as e:
            print(f"[DEBUG] ChromeDriver ì‹¤íŒ¨ {path}: {str(e)[:100]}...")
            continue

    # 2ë‹¨ê³„: WebDriver Manager
    try:
        print("[DEBUG] WebDriver Manager ì‹œë„")
        from webdriver_manager.chrome import ChromeDriverManager
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        print("[DEBUG] WebDriver Manager ì„±ê³µ")
        return driver
    except Exception as e:
        print(f"[DEBUG] WebDriver Manager ì‹¤íŒ¨: {str(e)[:100]}...")

    raise Exception("ëª¨ë“  ChromeDriver ì´ˆê¸°í™” ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

# =============================================================================
# URL ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°
# =============================================================================

def fix_url_bug(url):
    """URL ë²„ê·¸ ìˆ˜ì • í•¨ìˆ˜"""
    if not url:
        return url

    # ttps:// â†’ https:// ìˆ˜ì •
    if url.startswith('ttps://'):
        url = 'h' + url
        print(f"[URL FIX] ttps â†’ https: {url}")

    # ìƒëŒ€ ê²½ë¡œ â†’ ì ˆëŒ€ ê²½ë¡œ
    elif url.startswith('/'):
        if 'onstove.com' in url or 'epicseven' in url:
            url = 'https://page.onstove.com' + url
        elif 'ruliweb.com' in url:
            url = 'https://bbs.ruliweb.com' + url
        elif 'reddit.com' in url:
            url = 'https://www.reddit.com' + url
        print(f"[URL FIX] ìƒëŒ€ê²½ë¡œ ìˆ˜ì •: {url}")

    # í”„ë¡œí† ì½œ ëˆ„ë½
    elif not url.startswith(('http://', 'https://')):
        url = 'https://' + url
        print(f"[URL FIX] í”„ë¡œí† ì½œ ì¶”ê°€: {url}")

    return url

# =============================================================================
# Phase 2: ì˜ë¯¸ìˆëŠ” ë³¸ë¬¸ ì¶”ì¶œ í•¨ìˆ˜ (ì„±ëŠ¥ ìµœì í™”)
# =============================================================================

def extract_meaningful_content(text: str) -> str:
    """Phase 2: ì˜ë¯¸ìˆëŠ” ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ ì•Œê³ ë¦¬ì¦˜ (ì„±ëŠ¥ ìµœì í™”)"""
    if not text or len(text) < 30:
        return ""

    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í•  (ê°œì„ ëœ ì •ê·œì‹)
    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]\s*', text.strip())
    sentences = [s.strip() for s in sentences if s.strip()]

    if not sentences:
        return text[:100].strip()

    # ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ í•„í„°ë§ ì‹œìŠ¤í…œ
    meaningful_sentences = []

    for sentence in sentences:
        if len(sentence) < 10:  # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸
            continue

        # ì˜ë¯¸ì—†ëŠ” ë¬¸ì¥ íŒ¨í„´ ì œì™¸
        meaningless_patterns = [
            r'^[ã…‹ã…ã„·ã… ã…œã…¡]+$',  # ììŒëª¨ìŒë§Œ
            r'^[!@#$%^&*()_+\-=\[\]{}|;\':",./<>?`~]+$',  # íŠ¹ìˆ˜ë¬¸ìë§Œ
            r'^\d+$',  # ìˆ«ìë§Œ
            r'^(ìŒ|ì–´|ì•„|ë„¤|ì˜ˆ|ì‘|ã…‡ã…‡|ã… ã… |ã…œã…œ)$',  # ë‹¨ìˆœ ê°íƒ„ì‚¬
        ]

        if any(re.match(pattern, sentence) for pattern in meaningless_patterns):
            continue

        # Epic7 ê´€ë ¨ ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œ ìŠ¤ì½”ì–´ë§
        meaningful_keywords = [
            'ë²„ê·¸', 'ì˜¤ë¥˜', 'ë¬¸ì œ', 'ì—ëŸ¬', 'ì•ˆë¨', 'ì‘ë™', 'ì‹¤í–‰',
            'ìºë¦­í„°', 'ìŠ¤í‚¬', 'ì•„í‹°íŒ©íŠ¸', 'ì¥ë¹„', 'ë˜ì „', 'ì•„ë ˆë‚˜', 
            'ê¸¸ë“œ', 'ì´ë²¤íŠ¸', 'ì—…ë°ì´íŠ¸', 'íŒ¨ì¹˜', 'ë°¸ëŸ°ìŠ¤', 'ë„ˆí”„',
            'ê²Œì„', 'í”Œë ˆì´', 'ìœ ì €', 'ìš´ì˜', 'ê³µì§€', 'í™•ë¥ ',
            'ë½‘ê¸°', 'ì†Œí™˜', '6ì„±', 'ê°ì„±', 'ì´ˆì›”', 'ë£¬', 'ì ¬'
        ]

        score = sum(1 for keyword in meaningful_keywords if keyword in sentence)

        # ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ìœ¼ë¡œ íŒë³„
        if score > 0 or len(sentence) >= 30:
            meaningful_sentences.append(sentence)

    if not meaningful_sentences:
        # í´ë°±: ì²« ë²ˆì§¸ ê¸´ ë¬¸ì¥
        long_sentences = [s for s in sentences if len(s) >= 20]
        if long_sentences:
            return long_sentences[0]
        else:
            return sentences[0] if sentences else text[:100]

    # ìµœì  ì¡°í•©: 1-3ê°œ ë¬¸ì¥ ì¡°í•©ìœ¼ë¡œ ì˜ë¯¸ìˆëŠ” ë‚´ìš© êµ¬ì„±
    result = meaningful_sentences[0]

    # ì²« ë²ˆì§¸ ë¬¸ì¥ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ë‘ ë²ˆì§¸ ë¬¸ì¥ ì¶”ê°€
    if len(result) < 50 and len(meaningful_sentences) > 1:
        result += ' ' + meaningful_sentences[1]

    # ì—¬ì „íˆ ë¶€ì¡±í•˜ë©´ ì„¸ ë²ˆì§¸ ë¬¸ì¥ê¹Œì§€ ì¶”ê°€
    if len(result) < 80 and len(meaningful_sentences) > 2:
        result += ' ' + meaningful_sentences[2]

    return result.strip()

# =============================================================================
# Phase 2: Stove ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ í•¨ìˆ˜ - ì„±ëŠ¥ ìµœì í™” ì™„ë£Œ
# =============================================================================

def get_stove_post_content(post_url: str, driver: webdriver.Chrome, 
                          source: str = "stove_korea_bug", 
                          schedule_type: str = "frequent") -> str:
    """Phase 2: ìŠ¤í† ë¸Œ ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ - ì„±ëŠ¥ ìµœì í™” ì™„ë£Œ"""

    # ìºì‹œ í™•ì¸
    cache = load_content_cache()
    url_hash = hash(post_url) % (10**8)

    if str(url_hash) in cache:
        cached_item = cache[str(url_hash)]
        cache_time = datetime.fromisoformat(cached_item.get('timestamp', '2000-01-01'))
        if datetime.now() - cache_time < timedelta(hours=24):
            print(f"[CACHE] ìºì‹œëœ ë‚´ìš© ì‚¬ìš©: {post_url}")
            return cached_item.get('content', "ê²Œì‹œê¸€ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    content_summary = "ê²Œì‹œê¸€ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    try:
        print(f"[DEBUG] ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ ì‹œë„: {post_url}")

        wait_time = CrawlingSchedule.get_wait_time(schedule_type)
        driver.set_page_load_timeout(wait_time + 10)
        driver.get(post_url)

        print(f"[DEBUG] í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘... ({wait_time}ì´ˆ)")
        time.sleep(wait_time)

        # JavaScript ì™„ì „ ë¡œë”© í™•ì¸
        WebDriverWait(driver, 15).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )

        # Phase 2 ìµœì í™”: ë‹¨ê³„ë³„ ìŠ¤í¬ë¡¤ë§ (ì„±ëŠ¥ ê°œì„ )
        print("[DEBUG] ìµœì í™”ëœ ìŠ¤í¬ë¡¤ë§ ì‹œì‘...")
        driver.execute_script("window.scrollTo(0, 500);")
        time.sleep(2)
        driver.execute_script("window.scrollTo(0, 1000);")
        time.sleep(2)
        driver.execute_script("window.scrollTo(0, 0);")
        time.sleep(1)
        print("[DEBUG] ìµœì í™”ëœ ìŠ¤í¬ë¡¤ë§ ì™„ë£Œ")

        # Phase 2: Master ë°œê²¬ CSS Selector ìš°ì„  ì ìš©
        content_selectors = [
            # Master ì§€ì ì‚¬í•­: ëª©ë¡ í˜ì´ì§€ì—ì„œ ì§ì ‘ ì¶”ì¶œ
            'meta[data-vmid="description"]',
            'meta[name="description"]',

            # ê°œë³„ í˜ì´ì§€ ì„ íƒìë“¤ (ë°±ì—…)
            'div.s-article-content',
            'div.s-article-content-text',
            'section.s-article-body',
            'div.s-board-content',

            # Phase 2: ì¶”ê°€ ë°±ì—… ì„ íƒì
            '.article-content',
            '.post-content',
            '[class*="content"]'
        ]

        # Phase 2: ì˜ë¯¸ìˆëŠ” ë³¸ë¬¸ ì¶”ì¶œ ì•Œê³ ë¦¬ì¦˜ ì ìš©
        for i, selector in enumerate(content_selectors):
            try:
                elements = driver.find_elements(By.CSS_SELECTOR, selector)
                if elements:
                    for element in elements:
                        # ë©”íƒ€ íƒœê·¸ëŠ” content ì†ì„±ì—ì„œ, ì¼ë°˜ íƒœê·¸ëŠ” textì—ì„œ ì¶”ì¶œ
                        if selector.startswith('meta'):
                            raw_text = element.get_attribute('content').strip()
                        else:
                            raw_text = element.text.strip()

                        if not raw_text or len(raw_text) < 30:
                            continue           

                        # Phase 2: ë©”íƒ€ë°ì´í„° í•„í„°ë§ ê°•í™”
                        skip_keywords = [
                            'install stove', 'ìŠ¤í† ë¸Œë¥¼ ì„¤ì¹˜', 'ë¡œê·¸ì¸ì´ í•„ìš”', 
                            'javascript', 'ëŒ“ê¸€', 'ê³µìœ ', 'ì¢‹ì•„ìš”', 'ì¶”ì²œ', 'ì‹ ê³ ',
                            'ì‘ì„±ì', 'ì‘ì„±ì¼', 'ì¡°íšŒìˆ˜', 'ì²¨ë¶€íŒŒì¼', 'ë‹¤ìš´ë¡œë“œ',
                            'copyright', 'ì €ì‘ê¶Œ', 'ì´ìš©ì•½ê´€', 'ê°œì¸ì •ë³´', 'ì¿ í‚¤',
                            'ê´‘ê³ ', 'ad', 'advertisement', 'í”„ë¡œëª¨ì…˜', 'ì´ë²¤íŠ¸',
                            'ë¡œê·¸ì¸', 'login', 'sign in', 'íšŒì›ê°€ì…', 'register',
                            'ë©”ë‰´', 'menu', 'navigation', 'ë„¤ë¹„ê²Œì´ì…˜', 'ì‚¬ì´ë“œë°”',
                            'ë°°ë„ˆ', 'banner', 'í‘¸í„°', 'footer', 'í—¤ë”', 'header'
                        ]

                        if any(skip.lower() in raw_text.lower() for skip in skip_keywords):
                            continue

                        # Phase 2: ì˜ë¯¸ìˆëŠ” ë¬¸ë‹¨ ì¶”ì¶œ (ì„±ëŠ¥ ìµœì í™”)
                        meaningful_content = extract_meaningful_content(raw_text)

                        # Phase 2: ìµœì†Œ ê¸¸ì´ 50ì ì´ìƒìœ¼ë¡œ ì¦ê°€
                        if len(meaningful_content) >= 50:
                            # 150ì ì´ë‚´ë¡œ ìš”ì•½
                            if len(meaningful_content) > 150:
                                content_summary = meaningful_content[:147] + '...'
                            else:
                                content_summary = meaningful_content

                            print(f"[SUCCESS] ì„ íƒì {i+1}/{len(content_selectors)} '{selector}'ë¡œ ë‚´ìš© ì¶”ì¶œ ì„±ê³µ")
                            print(f"[CONTENT] {content_summary[:80]}...")
                            break

                    if content_summary != "ê²Œì‹œê¸€ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.":
                        break

            except Exception as e:
                print(f"[DEBUG] ì„ íƒì '{selector}' ì‹¤íŒ¨: {e}")
                continue

        # ìºì‹œ ì €ì¥
        cache[str(url_hash)] = {
            'content': content_summary,
            'timestamp': datetime.now().isoformat(),
            'url': post_url,
            'source': source
        }
        save_content_cache(cache)

    except TimeoutException:
        print(f"[ERROR] í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ: {post_url}")
        content_summary = "â° ê²Œì‹œê¸€ ë¡œë”© ì‹œê°„ ì´ˆê³¼"
    except Exception as e:
        print(f"[ERROR] ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        content_summary = "ğŸ”— ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ ì‹¤íŒ¨"

    return content_summary

# =============================================================================
# ğŸš€ Master ìš”êµ¬ì‚¬í•­: Stove ê²Œì‹œíŒ í¬ë¡¤ë§ + ì¦‰ì‹œ ì²˜ë¦¬ í†µí•©
# =============================================================================

def crawl_stove_board(board_url: str, source: str, force_crawl: bool = False, 
                     schedule_type: str = "frequent", region: str = "korea",
                     on_post_process: Optional[Callable[[Dict], None]] = None) -> List[Dict]:
    """
    Stove ê²Œì‹œíŒ í¬ë¡¤ë§ + ì¦‰ì‹œ ì²˜ë¦¬ í†µí•©
    Master ìš”êµ¬ì‚¬í•­: ê²Œì‹œê¸€ë³„ ì¦‰ì‹œ ì²˜ë¦¬ (í¬ë¡¤ë§â†’ê°ì„±ë¶„ì„â†’ì•Œë¦¼â†’ë§ˆí‚¹)
    """

    posts = []
    link_data = load_crawled_links()

    print(f"[INFO] {source} í¬ë¡¤ë§ ì‹œì‘ - URL: {board_url}")
    print(f"[DEBUG] ê¸°ì¡´ ë§í¬ ìˆ˜: {len(link_data['links'])}, Force Crawl: {force_crawl}")

    driver = None
    try:
        driver = get_chrome_driver()

        wait_time = CrawlingSchedule.get_wait_time(schedule_type)
        driver.set_page_load_timeout(wait_time + 10)
        driver.implicitly_wait(15)

        print(f"[DEBUG] ê²Œì‹œíŒ ì ‘ì† ì¤‘: {board_url}")
        driver.get(board_url)

        print(f"[DEBUG] í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘... ({wait_time}ì´ˆ)")
        time.sleep(wait_time)

        # JavaScript ì™„ì „ ë¡œë”© í™•ì¸
        WebDriverWait(driver, 15).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )

        # Phase 2: ìµœì í™”ëœ ìŠ¤í¬ë¡¤ë§ (ì„±ëŠ¥ ê°œì„ )
        driver.execute_script("window.scrollTo(0, 800);")
        time.sleep(3)

        # ë””ë²„ê¹…ìš© HTML ì €ì¥
        debug_filename = f"{source}_debug_selenium.html"
        with open(debug_filename, "w", encoding="utf-8") as f:
            f.write(driver.page_source)
        print(f"[DEBUG] HTML ì €ì¥: {debug_filename}")

        # Phase 2: Master ë°œê²¬ ì„ íƒì ìš°ì„  ì ìš© - JavaScript ìµœì í™”
        user_posts = driver.execute_script("""
            var userPosts = [];

            // Phase 2: Master ì§€ì ì‚¬í•­ - section.s-board-item ìµœìš°ì„  ì ìš©
            const selectors = [
                'section.s-board-item',           // Master ë°œê²¬ ì„ íƒì (ìµœìš°ì„ )
                'h3.s-board-title',               // ê¸°ì¡´ ì„ íƒì (ë°±ì—…)
                '[class*="board-title"]',         // í´ë˜ìŠ¤ëª… í¬í•¨
                '[class*="post-title"]',          // post-title í¬í•¨
                '[class*="article-title"]',       // article-title í¬í•¨
                'h3[class*="title"]',            // h3 íƒœê·¸ title í¬í•¨
                'a[href*="/view/"]'              // view ë§í¬ ì§ì ‘ ì°¾ê¸°
            ];

            var elements = [];
            var successful_selector = '';

            // ì„ íƒìë³„ ì‹œë„
            for (var i = 0; i < selectors.length; i++) {
                try {
                    elements = document.querySelectorAll(selectors[i]);
                    if (elements && elements.length > 0) {
                        successful_selector = selectors[i];
                        console.log('Phase 2 ì„ íƒì ì„±ê³µ:', selectors[i], 'ê°œìˆ˜:', elements.length);
                        break;
                    }
                } catch (e) {
                    console.log('ì„ íƒì ì‹¤íŒ¨:', selectors[i], e);
                    continue;
                }
            }

            if (!elements || elements.length === 0) {
                console.log('ëª¨ë“  ì„ íƒì ì‹¤íŒ¨');
                return [];
            }

            console.log('ì´ ë°œê²¬ëœ ìš”ì†Œ ìˆ˜:', elements.length);

            // ê³µì§€ì‚¬í•­ IDë“¤ (ì œì™¸ ëŒ€ìƒ)
            const officialIds = ['10518001', '10855687', '10855562', '10855132'];

            // ê° ìš”ì†Œì—ì„œ ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ
            for (var i = 0; i < Math.min(elements.length, 20); i++) {
                var element = elements[i];

                try {
                    var linkElement, titleElement, contentElement = null;
                    var href = '', title = '', preview_content = '';

                    // ë§í¬ ìš”ì†Œ ì°¾ê¸°
                    if (successful_selector === 'section.s-board-item') {
                        // Phase 2: Master ì§€ì ì‚¬í•­ - ëª©ë¡ í˜ì´ì§€ì—ì„œ ì§ì ‘ ë³¸ë¬¸ ì¶”ì¶œ
                        linkElement = element.querySelector('a[href*="/view/"]');
                        titleElement = element.querySelector('.s-board-title-text, .board-title, h3 span, .title');

                        // Master ë°œê²¬: p.s-board-textì—ì„œ ë³¸ë¬¸ ì§ì ‘ ì¶”ì¶œ
                        contentElement = element.querySelector('p.s-board-text');
                        if (contentElement) {
                            preview_content = contentElement.textContent?.trim() || '';
                        }
                    } else {
                        // ê¸°íƒ€ ì„ íƒì ê¸°ë°˜ ì¶”ì¶œ
                        linkElement = element.closest('a[href*="/view/"]') || element.querySelector('a[href*="/view/"]');
                        titleElement = element;
                    }

                    // ë§í¬ ì¶”ì¶œ
                    if (linkElement && linkElement.href) {
                        href = linkElement.href;
                    }

                    // ì œëª© ì¶”ì¶œ
                    if (titleElement) {
                        title = titleElement.textContent?.trim() || titleElement.innerText?.trim() || '';
                    }

                    // ìœ íš¨ì„± ê²€ì‚¬
                    if (!href || !title || title.length < 3) {
                        continue;
                    }

                    // URLì—ì„œ ê²Œì‹œê¸€ ID ì¶”ì¶œ
                    var idMatch = href.match(/\/view\/(\d+)/);
                    if (!idMatch) {
                        continue;
                    }
                    var id = idMatch[1];

                    // ê³µì§€ì‚¬í•­ ì œì™¸
                    if (officialIds.includes(id)) {
                        console.log('ê³µì§€ì‚¬í•­ ì œì™¸:', id, title.substring(0, 20));
                        continue;
                    }

                    // ê³µì§€/ì´ë²¤íŠ¸ ë°°ì§€ í™•ì¸
                    var isNotice = element.querySelector('i.element-badge__s.notice, .notice, [class*="notice"]');
                    var isEvent = element.querySelector('i.element-badge__s.event, .event, [class*="event"]');
                    var isOfficial = element.querySelector('span.s-profile-staff-official, [class*="official"]');

                    if (isNotice || isEvent || isOfficial) {
                        console.log('ê³µì§€/ì´ë²¤íŠ¸ ì œì™¸:', title.substring(0, 20));
                        continue;
                    }

                    // ì œëª©ì—ì„œ [ê³µì§€], [ì´ë²¤íŠ¸] ë“± í‚¤ì›Œë“œ ì œì™¸  
                    var skipKeywords = ['[ê³µì§€]', '[ì´ë²¤íŠ¸]', '[ì•ˆë‚´]', '[ì ê²€]', '[ê³µì§€ì‚¬í•­]'];
                    var shouldSkip = skipKeywords.some(function(keyword) {
                        return title.includes(keyword);
                    });

                    if (shouldSkip) {
                        console.log('í‚¤ì›Œë“œ ì œì™¸:', title.substring(0, 20));
                        continue;
                    }

                    // URL ì •ê·œí™”
                    var fullUrl = href.startsWith('http') ? href : 'https://page.onstove.com' + href;

                    userPosts.push({
                        href: fullUrl,
                        id: id,
                        title: title.substring(0, 200).trim(),
                        preview_content: preview_content.substring(0, 150).trim(),
                        selector_used: successful_selector
                    });

                    console.log('Phase 2 ê²Œì‹œê¸€ ì¶”ê°€:', title.substring(0, 30));

                } catch (e) {
                    console.log('ê²Œì‹œê¸€ ì²˜ë¦¬ ì˜¤ë¥˜:', e.message);
                    continue;
                }
            }

            console.log('Phase 2 ìµœì¢… ì¶”ì¶œëœ ìœ ì € ê²Œì‹œê¸€ ìˆ˜:', userPosts.length);
            return userPosts;
        """)

        print(f"[DEBUG] Phase 2 JavaScriptë¡œ {len(user_posts)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")

        # ğŸš€ Master ìš”êµ¬ì‚¬í•­: ê° ê²Œì‹œê¸€ë³„ ì¦‰ì‹œ ì²˜ë¦¬
        for i, post_info in enumerate(user_posts, 1):
            try:
                href = post_info['href']
                title = post_info['title']
                post_id = post_info['id']
                preview_content = post_info.get('preview_content', '')

                # URL ë²„ê·¸ ìˆ˜ì • ì ìš©
                href = fix_url_bug(href)

                print(f"[DEBUG] ê²Œì‹œê¸€ {i}/{len(user_posts)}: {title[:40]}...")
                print(f"[DEBUG] URL: {href}")

                # ì‹œê°„ ê¸°ë°˜ ì¤‘ë³µ í™•ì¸ (24ì‹œê°„ ë‚´ ì²˜ë¦¬ëœ ê²½ìš°ë§Œ SKIP)
                if not force_crawl and is_recently_processed(href, link_data["links"]):
                    print(f"[SKIP] 24ì‹œê°„ ë‚´ ì²˜ë¦¬ëœ ë§í¬: {post_id}")
                    continue

                # ì œëª© ê¸¸ì´ ê²€ì¦
                if len(title) < 5:
                    print(f"[SKIP] ì œëª©ì´ ë„ˆë¬´ ì§§ìŒ: {title}")
                    continue

                # Phase 2: ëª©ë¡ í˜ì´ì§€ì—ì„œ ì¶”ì¶œí•œ ë³¸ë¬¸ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê°œë³„ í˜ì´ì§€ ë°©ë¬¸
                if preview_content and len(preview_content) >= 50:
                    content = preview_content
                    print(f"[PHASE2] ëª©ë¡ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ì§ì ‘ ì¶”ì¶œ ì„±ê³µ (90% ì‹œê°„ ë‹¨ì¶•)")
                else:
                    # ê°œë³„ í˜ì´ì§€ ë°©ë¬¸ (ë°±ì—…)
                    print(f"[FALLBACK] ê°œë³„ í˜ì´ì§€ ë°©ë¬¸í•˜ì—¬ ë³¸ë¬¸ ì¶”ì¶œ")
                    content = get_stove_post_content(href, driver, source, schedule_type)

                # ìµœì†Œ ë³¸ë¬¸ ê¸¸ì´ ê²€ì¦
                if len(content) < 20:
                    print(f"[SKIP] ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìŒ: {content[:50]}")
                    continue

                # ê²Œì‹œê¸€ ë°ì´í„° êµ¬ì„±
                post_data = {
                    'title': title,
                    'url': href,
                    'content': content,
                    'source': source,
                    'post_id': post_id,
                    'timestamp': datetime.now().isoformat(),
                    'region': region
                }

                posts.append(post_data)

                # ğŸš€ Master ìš”êµ¬ì‚¬í•­: ê²Œì‹œê¸€ë³„ ì¦‰ì‹œ ì²˜ë¦¬
                if on_post_process:
                    try:
                        on_post_process(post_data)
                        print(f"[IMMEDIATE] ì¦‰ì‹œ ì²˜ë¦¬ ì™„ë£Œ: {title[:30]}...")
                    except Exception as e:
                        print(f"[ERROR] ì¦‰ì‹œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

                print(f"[SUCCESS] ê²Œì‹œê¸€ ì¶”ê°€: {title[:40]}...")

            except Exception as e:
                print(f"[ERROR] ê²Œì‹œê¸€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue

        print(f"[INFO] {source} í¬ë¡¤ë§ ì™„ë£Œ: {len(posts)}ê°œ ê²Œì‹œê¸€")

    except TimeoutException:
        print(f"[ERROR] í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ: {board_url}")
    except Exception as e:
        print(f"[ERROR] {source} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    finally:
        if driver:
            try:
                driver.quit()
                print(f"[DEBUG] ChromeDriver ì¢…ë£Œ: {source}")
            except Exception as e:
                print(f"[WARNING] ChromeDriver ì¢…ë£Œ ì‹¤íŒ¨: {e}")

    return posts

# =============================================================================
# âœ¨ ì™„ì „ ì‹ ê·œ êµ¬í˜„: ë£¨ë¦¬ì›¹ Epic7 í¬ë¡¤ë§ (Master ìš”êµ¬ì‚¬í•­)
# =============================================================================

def crawl_ruliweb_epic7(force_crawl: bool = False, schedule_type: str = "frequent", 
                       on_post_process: Optional[Callable[[Dict], None]] = None) -> List[Dict]:
    """
    âœ¨ ì™„ì „ ì‹ ê·œ êµ¬í˜„: ë£¨ë¦¬ì›¹ ì—í”½ì„¸ë¸ ê²Œì‹œíŒ í¬ë¡¤ë§
    Master ìš”êµ¬ì‚¬í•­: 6ë²ˆì§¸ ì†ŒìŠ¤ ì™„ì „ êµ¬í˜„
    """
    
    posts = []
    link_data = load_crawled_links()
    
    print("[INFO] ë£¨ë¦¬ì›¹ Epic7 í¬ë¡¤ë§ ì‹œì‘")
    
    # BeautifulSoup ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
    if not BEAUTIFULSOUP_AVAILABLE:
        print("[ERROR] BeautifulSoup4ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return posts
    
    try:
        # ë£¨ë¦¬ì›¹ Epic7 ê²Œì‹œíŒ URL (Master ì§€ì •)
        url = "https://bbs.ruliweb.com/game/84834"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        wait_time = CrawlingSchedule.get_wait_time('ruliweb')
        
        print(f"[DEBUG] ë£¨ë¦¬ì›¹ ìš”ì²­ ì‹œì‘: {url}")
        response = requests.get(url, headers=headers, timeout=wait_time)
        response.raise_for_status()
        
        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        time.sleep(wait_time)
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ì„ íƒì (ì‹¤ì œ HTML êµ¬ì¡° ê¸°ë°˜)
        board_items = soup.select('table.board_list_table tbody tr')
        
        print(f"[DEBUG] ë£¨ë¦¬ì›¹ì—ì„œ {len(board_items)}ê°œ í•­ëª© ë°œê²¬")
        
        for item in board_items:
            try:
                # ì œëª© ë° ë§í¬ ì¶”ì¶œ
                title_element = item.select_one('td.subject a.deco')
                if not title_element:
                    continue
                
                title = title_element.get_text(strip=True)
                href = title_element.get('href', '')
                
                # URL ì •ê·œí™”
                if href.startswith('/'):
                    href = 'https://bbs.ruliweb.com' + href
                
                # ê¸°ë³¸ ê²€ì¦
                if not title or len(title) < 5 or not href:
                    continue
                
                # ê³µì§€ì‚¬í•­ ì œì™¸
                notice_element = item.select_one('.notice_icon, .icon_notice')
                if notice_element:
                    continue
                
                # ì¤‘ë³µ ì²´í¬
                if not force_crawl and is_recently_processed(href, link_data["links"]):
                    print(f"[SKIP] 24ì‹œê°„ ë‚´ ì²˜ë¦¬ëœ ë§í¬: {href}")
                    continue
                
                # ê²Œì‹œê¸€ ID ì¶”ì¶œ
                post_id_match = re.search(r'/hobby/(\d+)', href)
                post_id = post_id_match.group(1) if post_id_match else str(hash(href))
                
                # ì‘ì„±ì ì •ë³´
                author_element = item.select_one('td.writer .nick')
                author = author_element.get_text(strip=True) if author_element else "ìµëª…"
                
                # ì‘ì„±ì¼ ì •ë³´
                date_element = item.select_one('td.time')
                created_time = date_element.get_text(strip=True) if date_element else ""
                
                # ì¡°íšŒìˆ˜ ì •ë³´
                hit_element = item.select_one('td.hit')
                hit_count = hit_element.get_text(strip=True) if hit_element else "0"
                
                # Epic7 ê´€ë ¨ í‚¤ì›Œë“œ í•„í„°ë§ (ì¤‘ìš”!)
                epic7_keywords = [
                    'ì—í”½ì„¸ë¸', 'epic7', 'epic seven', 'ì—7', 'e7',
                    'ìŠ¤í‚¬', 'ìºë¦­í„°', 'ì•„í‹°íŒ©íŠ¸', 'ì¥ë¹„', 'ë²„ê·¸', 'ì˜¤ë¥˜',
                    'ì—…ë°ì´íŠ¸', 'íŒ¨ì¹˜', 'ë°¸ëŸ°ìŠ¤', 'ë„ˆí”„', 'ë²„í”„',
                    'ì†Œí™˜', 'ë½‘ê¸°', '6ì„±', 'ê°ì„±', 'ì´ˆì›”'
                ]
                
                if not any(keyword.lower() in title.lower() for keyword in epic7_keywords):
                    print(f"[SKIP] Epic7 ê´€ë ¨ ì—†ëŠ” ê²Œì‹œê¸€: {title[:30]}...")
                    continue
                
                # ê²Œì‹œê¸€ ë°ì´í„° êµ¬ì„±
                post_data = {
                    'title': title,
                    'url': href,
                    'content': title,  # ë£¨ë¦¬ì›¹ì€ ëª©ë¡ì—ì„œ ë³¸ë¬¸ ë¯¸ì œê³µ
                    'source': 'ruliweb_epic7',
                    'author': author,
                    'created_time': created_time,
                    'hit_count': hit_count,
                    'post_id': post_id,
                    'timestamp': datetime.now().isoformat()
                }
                
                posts.append(post_data)
                
                # ğŸš€ Master ìš”êµ¬ì‚¬í•­: ê²Œì‹œê¸€ë³„ ì¦‰ì‹œ ì²˜ë¦¬
                if on_post_process:
                    try:
                        on_post_process(post_data)
                        print(f"[IMMEDIATE] ë£¨ë¦¬ì›¹ ì¦‰ì‹œ ì²˜ë¦¬ ì™„ë£Œ: {title[:30]}...")
                    except Exception as e:
                        print(f"[ERROR] ë£¨ë¦¬ì›¹ ì¦‰ì‹œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                
                print(f"[SUCCESS] ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ì¶”ê°€: {title[:40]}...")
                
            except Exception as e:
                print(f"[ERROR] ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        print(f"[INFO] ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì™„ë£Œ: {len(posts)}ê°œ ê²Œì‹œê¸€")
        
    except requests.exceptions.RequestException as e:
        print(f"[ERROR] ë£¨ë¦¬ì›¹ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
    except Exception as e:
        print(f"[ERROR] ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    return posts

# =============================================================================
# Reddit Epic7 í¬ë¡¤ë§ (ê¸°ì¡´ ìœ ì§€)
# =============================================================================

def crawl_reddit_epic7(force_crawl: bool = False, schedule_type: str = "frequent",
                      on_post_process: Optional[Callable[[Dict], None]] = None) -> List[Dict]:
    """Reddit r/EpicSeven ì„œë¸Œë ˆë”§ í¬ë¡¤ë§"""
    
    posts = []
    link_data = load_crawled_links()
    
    print("[INFO] Reddit Epic7 í¬ë¡¤ë§ ì‹œì‘")
    
    if not REDDIT_AVAILABLE:
        print("[ERROR] PRAW ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Reddit í¬ë¡¤ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return posts
    
    try:
        # Reddit API í™˜ê²½ë³€ìˆ˜ í™•ì¸
        client_id = os.environ.get('REDDIT_CLIENT_ID')
        client_secret = os.environ.get('REDDIT_CLIENT_SECRET')
        user_agent = os.environ.get('REDDIT_USER_AGENT', 'Epic7Monitor/1.0')
        
        if not client_id or not client_secret:
            print("[ERROR] Reddit API í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return posts
        
        # Reddit ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        reddit = praw.Reddit(
            client_id=client_id,
            client_secret=client_secret,
            user_agent=user_agent
        )
        
        # r/EpicSeven ì„œë¸Œë ˆë”§
        subreddit = reddit.subreddit('EpicSeven')
        
        # ìµœì‹  ê²Œì‹œê¸€ 20ê°œ ê°€ì ¸ì˜¤ê¸°
        for submission in subreddit.new(limit=20):
            try:
                # ê¸°ë³¸ ê²€ì¦
                if not submission.title or len(submission.title) < 5:
                    continue
                
                # URL êµ¬ì„±
                post_url = f"https://www.reddit.com{submission.permalink}"
                
                # ì¤‘ë³µ ì²´í¬
                if not force_crawl and is_recently_processed(post_url, link_data["links"]):
                    continue
                
                # ìŠ¤íŒ¸ í‚¤ì›Œë“œ í•„í„°
                spam_keywords = ['buy', 'sell', 'account', 'cheap', 'discord.gg']
                if any(keyword.lower() in submission.title.lower() for keyword in spam_keywords):
                    continue
                
                # Epic7 í•µì‹¬ í‚¤ì›Œë“œ í•„í„°
                epic7_keywords = [
                    'epic7', 'epic seven', 'e7', 'character', 'artifact', 
                    'equipment', 'bug', 'update', 'patch', 'balance',
                    'summon', '6star', 'awakening', 'imprint'
                ]
                
                if not any(keyword.lower() in submission.title.lower() for keyword in epic7_keywords):
                    continue
                
                # ê²Œì‹œê¸€ ë³¸ë¬¸ ì¶”ì¶œ
                content = submission.selftext[:200] if submission.selftext else submission.title
                
                # ê²Œì‹œê¸€ ë°ì´í„° êµ¬ì„±
                post_data = {
                    'title': submission.title,
                    'url': post_url,
                    'content': content,
                    'source': 'reddit_epic7',
                    'author': str(submission.author) if submission.author else "deleted",
                    'created_time': datetime.fromtimestamp(submission.created_utc).isoformat(),
                    'score': submission.score,
                    'num_comments': submission.num_comments,
                    'post_id': submission.id,
                    'timestamp': datetime.now().isoformat()
                }
                
                posts.append(post_data)
                
                # ğŸš€ Master ìš”êµ¬ì‚¬í•­: ê²Œì‹œê¸€ë³„ ì¦‰ì‹œ ì²˜ë¦¬
                if on_post_process:
                    try:
                        on_post_process(post_data)
                        print(f"[IMMEDIATE] Reddit ì¦‰ì‹œ ì²˜ë¦¬ ì™„ë£Œ: {submission.title[:30]}...")
                    except Exception as e:
                        print(f"[ERROR] Reddit ì¦‰ì‹œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                
                print(f"[SUCCESS] Reddit ê²Œì‹œê¸€ ì¶”ê°€: {submission.title[:40]}...")
                
            except Exception as e:
                print(f"[ERROR] Reddit ê²Œì‹œê¸€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        print(f"[INFO] Reddit í¬ë¡¤ë§ ì™„ë£Œ: {len(posts)}ê°œ ê²Œì‹œê¸€")
        
    except Exception as e:
        print(f"[ERROR] Reddit í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    return posts

# =============================================================================
# ğŸš€ Master ìš”êµ¬ì‚¬í•­: í†µí•© í¬ë¡¤ë§ í•¨ìˆ˜ (6ê°œ ì†ŒìŠ¤ ì™„ì „ êµ¬í˜„)
# =============================================================================

def crawl_frequent_sites(force_crawl: bool = False, schedule_type: str = "frequent", 
                        region: str = "all") -> List[Dict]:
    """
    ğŸš€ Master ìš”êµ¬ì‚¬í•­: 15ë¶„ ì£¼ê¸° ë¹ˆë²ˆí•œ í¬ë¡¤ë§ - 6ê°œ ì†ŒìŠ¤ ì™„ì „ êµ¬í˜„
    í•œêµ­/ê¸€ë¡œë²Œ ë¶„ë¦¬ êµ¬ì¡° ì§€ì›
    """
    
    all_posts = []
    
    print(f"[INFO] ë¹ˆë²ˆí•œ í¬ë¡¤ë§ ì‹œì‘ - ì§€ì—­: {region}, Force: {force_crawl}")
    
    # Master ìš”êµ¬ì‚¬í•­: 6ê°œ í¬ë¡¤ë§ ì†ŒìŠ¤ ì •ì˜
    crawl_tasks = []
    
    if region in ["all", "korea"]:
        crawl_tasks.extend([
            ('í•œêµ­ ë²„ê·¸ ê²Œì‹œíŒ', lambda: crawl_stove_board(
                "https://page.onstove.com/epicseven/kr/list/1012?page=1&direction=LATEST",
                "stove_korea_bug", force_crawl, schedule_type, "korea",
                immediate_processor.process_post_immediately
            )),
            ('í•œêµ­ ììœ ê²Œì‹œíŒ', lambda: crawl_stove_board(
                "https://page.onstove.com/epicseven/kr/list/1005?page=1&direction=LATEST",
                "stove_korea_general", force_crawl, schedule_type, "korea",
                immediate_processor.process_post_immediately
            )),
            ('ë£¨ë¦¬ì›¹ Epic7', lambda: crawl_ruliweb_epic7(
                force_crawl, schedule_type,
                immediate_processor.process_post_immediately
            ))
        ])
    
    if region in ["all", "global"]:
        crawl_tasks.extend([
            ('ê¸€ë¡œë²Œ ë²„ê·¸ ê²Œì‹œíŒ', lambda: crawl_stove_board(
                "https://page.onstove.com/epicseven/global/list/998?page=1&direction=LATEST",
                "stove_global_bug", force_crawl, schedule_type, "global",
                immediate_processor.process_post_immediately
            )),
            ('ê¸€ë¡œë²Œ ììœ ê²Œì‹œíŒ', lambda: crawl_stove_board(
                "https://page.onstove.com/epicseven/global/list/989?page=1&direction=LATEST",
                "stove_global_general", force_crawl, schedule_type, "global",
                immediate_processor.process_post_immediately
            )),
            ('Reddit Epic7', lambda: crawl_reddit_epic7(
                force_crawl, schedule_type,
                immediate_processor.process_post_immediately
            ))
        ])
    
    # ë³‘ë ¬ í¬ë¡¤ë§ ì‹¤í–‰
    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
        future_to_site = {executor.submit(task[1]): task[0] for task in crawl_tasks}
        
        for future in concurrent.futures.as_completed(future_to_site):
            site_name = future_to_site[future]
            try:
                site_posts = future.result(timeout=300)  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
                all_posts.extend(site_posts)
                print(f"[SUCCESS] {site_name}: {len(site_posts)}ê°œ ê²Œì‹œê¸€")
            except Exception as e:
                print(f"[ERROR] {site_name} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    # ì¬ì‹œë„ í ì²˜ë¦¬
    immediate_processor.process_retry_queue()
    
    # í†µê³„ ì¶œë ¥
    stats = immediate_processor.get_stats()
    print(f"[STATS] ì „ì²´: {len(all_posts)}ê°œ, ì¦‰ì‹œì²˜ë¦¬: {stats['processed']}ê°œ, ì‹¤íŒ¨: {stats['failed']}ê°œ")
    
    return all_posts

def crawl_regular_sites(force_crawl: bool = False, schedule_type: str = "regular",
                       region: str = "all") -> List[Dict]:
    """30ë¶„ ì£¼ê¸° ì •ê·œ í¬ë¡¤ë§ (frequent_sitesì™€ ë™ì¼í•˜ê²Œ 6ê°œ ì†ŒìŠ¤ ì™„ì „ ì§€ì›)"""
    return crawl_frequent_sites(force_crawl, schedule_type, region)

def crawl_by_schedule(schedule_or_site: str, force_crawl: bool = False, region: str = "all") -> List[Dict]:
    """ìŠ¤ì¼€ì¤„ë³„ í¬ë¡¤ë§ í†µí•© í•¨ìˆ˜ - ì‚¬ì´íŠ¸ëª… í˜¸í™˜ì„± ì¶”ê°€"""
    
    print(f"[INFO] ìŠ¤ì¼€ì¤„ë³„ í¬ë¡¤ë§ ì‹œì‘: {schedule_or_site}, ì§€ì—­: {region}")
    
    # ì‚¬ì´íŠ¸ëª…ì´ ë“¤ì–´ì˜¨ ê²½ìš° ë§¤í•‘
    site_to_schedule = {
        'stove_korea_bug': 'frequent',
        'stove_korea_general': 'regular',
        'stove_global_bug': 'frequent', 
        'stove_global_general': 'regular',
        'ruliweb_epic7': 'regular',
        'reddit_epicseven': 'regular'
    }
    
    # ì‚¬ì´íŠ¸ëª…ì¸ì§€ ìŠ¤ì¼€ì¤„ëª…ì¸ì§€ íŒë³„
    if schedule_or_site in site_to_schedule:
        schedule = site_to_schedule[schedule_or_site]
        print(f"[INFO] ì‚¬ì´íŠ¸ëª… '{schedule_or_site}'ì„ ìŠ¤ì¼€ì¤„ '{schedule}'ë¡œ ë§¤í•‘")
    else:
        schedule = schedule_or_site
        
    # ê¸°ì¡´ ìŠ¤ì¼€ì¤„ ì²˜ë¦¬ ë¡œì§ (ê·¸ëŒ€ë¡œ ìœ ì§€)
    if schedule in ["15min", "frequent"]:
        return crawl_frequent_sites(force_crawl, "frequent", region)
    elif schedule in ["30min", "regular"]:
        return crawl_regular_sites(force_crawl, "regular", region)
    elif schedule in ["24h", "daily"]:
        # ì¼ê°„ ë¦¬í¬íŠ¸ìš©ì€ ê¸°ì¡´ ë°ì´í„° í™œìš©
        return []
    else:
        print(f"[ERROR] ì•Œ ìˆ˜ ì—†ëŠ” ìŠ¤ì¼€ì¤„: {schedule}")
        return []
      
def get_all_posts_for_report(hours: int = 24) -> List[Dict]:
    """ì¼ê°„ ë¦¬í¬íŠ¸ìš© ê²Œì‹œê¸€ ìˆ˜ì§‘ (ê°ì„± ë°ì´í„°ì—ì„œ ì¶”ì¶œ)"""
    try:
        # ê°ì„± ë°ì´í„° ë§¤ë‹ˆì €ì—ì„œ ì¼ê°„ ë°ì´í„° ìˆ˜ì§‘
        if EPIC7_MODULES_AVAILABLE:
            from sentiment_data_manager import get_today_sentiment_summary
            return get_today_sentiment_summary(hours)
        else:
            print("[WARNING] ê°ì„± ë°ì´í„° ë§¤ë‹ˆì € ì—†ìŒ, ë¹ˆ ë¦¬í¬íŠ¸ ë°˜í™˜")
            return []
    except Exception as e:
        print(f"[ERROR] ì¼ê°„ ë¦¬í¬íŠ¸ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

# =============================================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# =============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - 6ê°œ ì†ŒìŠ¤ ì™„ì „ êµ¬í˜„ í…ŒìŠ¤íŠ¸"""
    
    print("=== Epic7 í¬ë¡¤ëŸ¬ v4.4 - 6ê°œ ì†ŒìŠ¤ ì™„ì „ êµ¬í˜„ í…ŒìŠ¤íŠ¸ ===")
    
    # í•œêµ­ ì‚¬ì´íŠ¸ë§Œ í…ŒìŠ¤íŠ¸
    print("\n1. í•œêµ­ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸")
    korea_posts = crawl_frequent_sites(force_crawl=True, region="korea")
    print(f"í•œêµ­ ì‚¬ì´íŠ¸ ê²°ê³¼: {len(korea_posts)}ê°œ")
    
    # ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ë§Œ í…ŒìŠ¤íŠ¸
    print("\n2. ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸")
    global_posts = crawl_frequent_sites(force_crawl=True, region="global")
    print(f"ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ ê²°ê³¼: {len(global_posts)}ê°œ")
    
    # ì „ì²´ ì‚¬ì´íŠ¸ í…ŒìŠ¤íŠ¸
    print("\n3. ì „ì²´ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸")
    all_posts = crawl_frequent_sites(force_crawl=True, region="all")
    print(f"ì „ì²´ ì‚¬ì´íŠ¸ ê²°ê³¼: {len(all_posts)}ê°œ")
    
    # ì¦‰ì‹œ ì²˜ë¦¬ í†µê³„
    stats = immediate_processor.get_stats()
    print(f"\nì¦‰ì‹œ ì²˜ë¦¬ í†µê³„: {stats}")
    
    print("\n=== 6ê°œ ì†ŒìŠ¤ ì™„ì „ êµ¬í˜„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")

if __name__ == "__main__":
    main()