#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Epic7 ë‹¤êµ­ê°€ í¬ë¡¤ëŸ¬ v4.2 - ì¤‘ë³µ ì²´í¬ ë¡œì§ ê°œì„  ì™„ì„±ë³¸
Master ìš”ì²­: ì•Œë¦¼ ì „ì†¡ ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì‹œê°„ ê¸°ë°˜ ì¤‘ë³µ ê´€ë¦¬ ì‹œìŠ¤í…œ

í•µì‹¬ ìˆ˜ì •ì‚¬í•­:
- 24ì‹œê°„ ê¸°ë°˜ ì¤‘ë³µ ì²´í¬ ë¡œì§ êµ¬í˜„
- ì•Œë¦¼ ì„±ê³µ í›„ ë§ˆí‚¹ ì‹œìŠ¤í…œ ë¶„ë¦¬
- í¬ë¡¤ë§ ë§í¬ íŒŒì¼ êµ¬ì¡° ê°œì„  (URL + ì‹œê°„ + ì•Œë¦¼ìƒíƒœ)

Author: Epic7 Monitoring Team  
Version: 4.2 (ì¤‘ë³µ ë¡œì§ ê°œì„ )
Date: 2025-07-24
"""

import time
import random
import re
import requests
import concurrent.futures
import os
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from urllib.parse import urljoin, urlparse

# Selenium ê´€ë ¨ import
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.keys import Keys

# Reddit í¬ë¡¤ë§ìš© import (Phase 3)
try:
    import praw
    REDDIT_AVAILABLE = True
except ImportError:
    print("[WARNING] PRAW ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Reddit í¬ë¡¤ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    REDDIT_AVAILABLE = False

# =============================================================================
# í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ ì„¤ì • í´ë˜ìŠ¤
# =============================================================================

class CrawlingSchedule:
    """í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ë³„ ì„¤ì • ê´€ë¦¬"""

    FREQUENT_WAIT_TIME = 25      # 15ë¶„ ì£¼ê¸° ëŒ€ê¸°ì‹œê°„ (ìµœì í™”)
    REGULAR_WAIT_TIME = 30       # 30ë¶„ ì£¼ê¸° ëŒ€ê¸°ì‹œê°„  
    REDDIT_WAIT_TIME = 15        # Reddit ëŒ€ê¸°ì‹œê°„
    RULIWEB_WAIT_TIME = 20       # ë£¨ë¦¬ì›¹ ëŒ€ê¸°ì‹œê°„

    # ìŠ¤í¬ë¡¤ íšŸìˆ˜ ì„¤ì •
    FREQUENT_SCROLL_COUNT = 2    # 15ë¶„ ì£¼ê¸° ìŠ¤í¬ë¡¤ (ì„±ëŠ¥ ìµœì í™”)
    REGULAR_SCROLL_COUNT = 3

    @staticmethod
    def get_wait_time(schedule_type: str) -> int:
        """ìŠ¤ì¼€ì¤„ íƒ€ì…ë³„ ëŒ€ê¸°ì‹œê°„ ë°˜í™˜"""
        if schedule_type == 'frequent':
            return CrawlingSchedule.FREQUENT_WAIT_TIME
        elif schedule_type == 'regular':
            return CrawlingSchedule.REGULAR_WAIT_TIME
        elif schedule_type == 'reddit':
            return CrawlingSchedule.REDDIT_WAIT_TIME
        elif schedule_type == 'ruliweb':
            return CrawlingSchedule.RULIWEB_WAIT_TIME
        else:
            return CrawlingSchedule.REGULAR_WAIT_TIME

    @staticmethod
    def get_scroll_count(schedule_type: str) -> int:
        """ìŠ¤ì¼€ì¤„ íƒ€ì…ë³„ ìŠ¤í¬ë¡¤ íšŸìˆ˜ ë°˜í™˜"""
        if schedule_type == 'frequent':
            return CrawlingSchedule.FREQUENT_SCROLL_COUNT
        else:
            return CrawlingSchedule.REGULAR_SCROLL_COUNT

# =============================================================================
# íŒŒì¼ ê´€ë¦¬ ì‹œìŠ¤í…œ - ğŸš€ ì‹œê°„ ê¸°ë°˜ ì¤‘ë³µ ê´€ë¦¬ ê°œì„ 
# =============================================================================

def get_crawled_links_file():
    """ì›Œí¬í”Œë¡œìš°ë³„ ë…ë¦½ì ì¸ í¬ë¡¤ë§ ë§í¬ íŒŒì¼ëª… ìƒì„±"""
    workflow_name = os.environ.get('GITHUB_WORKFLOW', 'default')

    if 'debug' in workflow_name.lower() or 'test' in workflow_name.lower():
        return "crawled_links_debug.json"
    elif 'monitor' in workflow_name.lower():
        return "crawled_links_monitor.json"
    else:
        return "crawled_links.json"

def get_content_cache_file():
    """ì›Œí¬í”Œë¡œìš°ë³„ ë…ë¦½ì ì¸ ì½˜í…ì¸  ìºì‹œ íŒŒì¼ëª… ìƒì„±"""
    workflow_name = os.environ.get('GITHUB_WORKFLOW', 'default')

    if 'debug' in workflow_name.lower():
        return "content_cache_debug.json"
    else:
        return "content_cache.json"

# ğŸš€ ìˆ˜ì •ëœ ë¡œë“œ í•¨ìˆ˜ - ì‹œê°„ ê¸°ë°˜ êµ¬ì¡°ë¡œ ë³€í™˜
def load_crawled_links():
    """í¬ë¡¤ë§ ë§í¬ ë¡œë“œ - ì‹œê°„ ê¸°ë°˜ êµ¬ì¡° ì ìš©"""
    crawled_links_file = get_crawled_links_file()
    
    if os.path.exists(crawled_links_file):
        try:
            with open(crawled_links_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
                # ğŸš€ ê¸°ì¡´ ë‹¨ìˆœ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¥¼ ìƒˆ êµ¬ì¡°ë¡œ ë³€í™˜
                if isinstance(data, dict) and "links" in data:
                    if isinstance(data["links"], list) and len(data["links"]) > 0:
                        # ê¸°ì¡´ ë‹¨ìˆœ ë§í¬ë¥¼ ì‹œê°„ êµ¬ì¡°ë¡œ ë³€í™˜
                        if isinstance(data["links"][0], str):
                            converted_links = []
                            for link in data["links"]:
                                converted_links.append({
                                    "url": link,
                                    "processed_at": (datetime.now() - timedelta(hours=25)).isoformat(),  # 25ì‹œê°„ ì „ìœ¼ë¡œ ì„¤ì • (ì¬ì²˜ë¦¬ í—ˆìš©)
                                    "notified": False
                                })
                            data["links"] = converted_links
                            print(f"[INFO] ê¸°ì¡´ {len(converted_links)}ê°œ ë§í¬ë¥¼ ìƒˆ êµ¬ì¡°ë¡œ ë³€í™˜")
                
                # ğŸš€ 24ì‹œê°„ ì§€ë‚œ í•­ëª© ìë™ ì œê±°
                now = datetime.now()
                valid_links = []
                for item in data.get("links", []):
                    try:
                        processed_time = datetime.fromisoformat(item["processed_at"])
                        if now - processed_time < timedelta(hours=24):
                            valid_links.append(item)
                    except:
                        # ì˜ëª»ëœ í˜•ì‹ì€ ì œê±°
                        continue
                
                data["links"] = valid_links
                print(f"[INFO] 24ì‹œê°„ ê¸°ì¤€ ìœ íš¨í•œ ë§í¬: {len(valid_links)}ê°œ")
                return data
                        
        except Exception as e:
            print(f"[WARNING] í¬ë¡¤ë§ ë§í¬ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
    
    return {"links": [], "last_updated": datetime.now().isoformat()}

# ğŸš€ ìˆ˜ì •ëœ ì €ì¥ í•¨ìˆ˜ - í¬ê¸° ì œí•œ ê°•í™”
def save_crawled_links(link_data):
    """í¬ë¡¤ë§ ë§í¬ ì €ì¥ - ì ê·¹ì  í¬ê¸° ê´€ë¦¬"""
    try:
        # ğŸš€ í¬ê¸° ì œí•œì„ 100ê°œë¡œ ì¶•ì†Œ (ë” ì ê·¹ì  ê´€ë¦¬)
        if len(link_data["links"]) > 100:
            # ìµœì‹  100ê°œë§Œ ìœ ì§€
            link_data["links"] = sorted(
                link_data["links"], 
                key=lambda x: x.get("processed_at", ""), 
                reverse=True
            )[:100]
            print(f"[INFO] ë§í¬ ëª©ë¡ì„ ìµœì‹  100ê°œë¡œ ì •ë¦¬")

        link_data["last_updated"] = datetime.now().isoformat()

        crawled_links_file = get_crawled_links_file()
        with open(crawled_links_file, 'w', encoding='utf-8') as f:
            json.dump(link_data, f, ensure_ascii=False, indent=2)
            
        print(f"[INFO] í¬ë¡¤ë§ ë§í¬ ì €ì¥ ì™„ë£Œ: {len(link_data['links'])}ê°œ")

    except Exception as e:
        print(f"[ERROR] ë§í¬ ì €ì¥ ì‹¤íŒ¨: {e}")

# ğŸš€ ìƒˆë¡œ ì¶”ê°€ëœ í•µì‹¬ í•¨ìˆ˜ë“¤
def is_recently_processed(url: str, links_data: List[Dict], hours: int = 24) -> bool:
    """ì‹œê°„ ê¸°ë°˜ ì¤‘ë³µ ì²´í¬ - 24ì‹œê°„ ë‚´ ì²˜ë¦¬ëœ ë§í¬ì¸ì§€ í™•ì¸"""
    try:
        now = datetime.now()
        for item in links_data:
            if item.get("url") == url:
                processed_time = datetime.fromisoformat(item["processed_at"])
                if now - processed_time < timedelta(hours=hours):
                    return True
        return False
    except Exception as e:
        print(f"[DEBUG] ì¤‘ë³µ ì²´í¬ ì˜¤ë¥˜: {e}")
        return False

def mark_as_processed(url: str, notified: bool = False):
    """ê²Œì‹œê¸€ì„ ì²˜ë¦¬ë¨ìœ¼ë¡œ ë§ˆí‚¹ - ì•Œë¦¼ ì„±ê³µ í›„ì—ë§Œ í˜¸ì¶œ"""
    try:
        link_data = load_crawled_links()
        
        # ê¸°ì¡´ í•­ëª© ì—…ë°ì´íŠ¸ ë˜ëŠ” ìƒˆ í•­ëª© ì¶”ê°€
        found = False
        for item in link_data["links"]:
            if item.get("url") == url:
                item["processed_at"] = datetime.now().isoformat()
                item["notified"] = notified
                found = True
                break
        
        if not found:
            link_data["links"].append({
                "url": url,
                "processed_at": datetime.now().isoformat(),
                "notified": notified
            })
        
        save_crawled_links(link_data)
        print(f"[INFO] ë§í¬ ì²˜ë¦¬ ì™„ë£Œ ë§ˆí‚¹: {url[:50]}... (ì•Œë¦¼: {notified})")
        
    except Exception as e:
        print(f"[ERROR] ë§í¬ ë§ˆí‚¹ ì‹¤íŒ¨: {e}")

def load_content_cache():
    """ê²Œì‹œê¸€ ë‚´ìš© ìºì‹œ ë¡œë“œ"""
    content_cache_file = get_content_cache_file()

    if os.path.exists(content_cache_file):
        try:
            with open(content_cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            print(f"[WARNING] {content_cache_file} íŒŒì¼ ì½ê¸° ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±")
    return {}

def save_content_cache(cache_data):
    """ê²Œì‹œê¸€ ë‚´ìš© ìºì‹œ ì €ì¥"""
    try:
        if len(cache_data) > 500:
            sorted_items = sorted(cache_data.items(), 
                                key=lambda x: x[1].get('timestamp', ''), reverse=True)
            cache_data = dict(sorted_items[:500])

        content_cache_file = get_content_cache_file()
        with open(content_cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)

    except Exception as e:
        print(f"[ERROR] ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

# =============================================================================
# Chrome Driver ê´€ë¦¬ - ğŸš€ ë¦¬ì†ŒìŠ¤ ìµœì í™” ê°•í™”
# =============================================================================

def get_chrome_driver():
    """Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” - ğŸš€ ë¦¬ì†ŒìŠ¤ ìµœì í™” ë° ì•ˆì •ì„± ê°•í™”"""
    options = Options()

    # ê¸°ë³¸ ì˜µì…˜ë“¤
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--disable-extensions')
    options.add_argument('--disable-plugins')
    options.add_argument('--disable-images')
    options.add_argument('--window-size=1920,1080')

    # ğŸš€ Master ìš”ì²­: ì¶”ê°€ ë¦¬ì†ŒìŠ¤ ìµœì í™” ì˜µì…˜ (í•µì‹¬ ìˆ˜ì •)
    options.add_argument('--memory-pressure-off')
    options.add_argument('--max_old_space_size=2048')
    options.add_argument('--disable-background-timer-throttling')
    options.add_argument('--disable-backgrounding-occluded-windows')
    options.add_argument('--disable-renderer-backgrounding')
    options.add_argument('--disable-features=TranslateUI')
    options.add_argument('--disable-default-apps')
    options.add_argument('--disable-web-security')
    options.add_argument('--disable-features=VizDisplayCompositor')

    # ë´‡ íƒì§€ ìš°íšŒ
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)

    # ëœë¤ User-Agent
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36'
    ]
    options.add_argument(f'--user-agent={random.choice(user_agents)}')

    # ì„±ëŠ¥ ìµœì í™”
    prefs = {
        'profile.default_content_setting_values': {
            'images': 2, 'plugins': 2, 'popups': 2,
            'geolocation': 2, 'notifications': 2, 'media_stream': 2,
        }
    }
    options.add_experimental_option('prefs', prefs)

    # 3ë‹¨ê³„ í´ë°± ë©”ì»¤ë‹ˆì¦˜
    possible_paths = [
        '/usr/bin/chromedriver',
        '/usr/local/bin/chromedriver', 
        '/snap/bin/chromium.chromedriver'
    ]

    # 1ë‹¨ê³„: ì‹œìŠ¤í…œ ê²½ë¡œë“¤ ì‹œë„
    for path in possible_paths:
        try:
            if os.path.exists(path):
                print(f"[DEBUG] ChromeDriver ì‹œë„: {path}")
                service = Service(path)
                driver = webdriver.Chrome(service=service, options=options)
                print(f"[DEBUG] ChromeDriver ì„±ê³µ: {path}")
                return driver
        except Exception as e:
            print(f"[DEBUG] ChromeDriver ì‹¤íŒ¨ {path}: {str(e)[:100]}...")
            continue

    # 2ë‹¨ê³„: WebDriver Manager
    try:
        print("[DEBUG] WebDriver Manager ì‹œë„")
        from webdriver_manager.chrome import ChromeDriverManager
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        print("[DEBUG] WebDriver Manager ì„±ê³µ")
        return driver
    except Exception as e:
        print(f"[DEBUG] WebDriver Manager ì‹¤íŒ¨: {str(e)[:100]}...")

    raise Exception("ëª¨ë“  ChromeDriver ì´ˆê¸°í™” ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

# =============================================================================
# URL ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°
# =============================================================================

def fix_url_bug(url):
    """URL ë²„ê·¸ ìˆ˜ì • í•¨ìˆ˜"""
    if not url:
        return url

    # ttps:// â†’ https:// ìˆ˜ì •
    if url.startswith('ttps://'):
        url = 'h' + url
        print(f"[URL FIX] ttps â†’ https: {url}")

    # ìƒëŒ€ ê²½ë¡œ â†’ ì ˆëŒ€ ê²½ë¡œ
    elif url.startswith('/'):
        if 'onstove.com' in url or 'epicseven' in url:
            url = 'https://page.onstove.com' + url
        elif 'ruliweb.com' in url:
            url = 'https://bbs.ruliweb.com' + url
        elif 'reddit.com' in url:
            url = 'https://www.reddit.com' + url
        print(f"[URL FIX] ìƒëŒ€ê²½ë¡œ ìˆ˜ì •: {url}")

    # í”„ë¡œí† ì½œ ëˆ„ë½
    elif not url.startswith(('http://', 'https://')):
        url = 'https://' + url
        print(f"[URL FIX] í”„ë¡œí† ì½œ ì¶”ê°€: {url}")

    return url

# =============================================================================
# Phase 2: ì˜ë¯¸ìˆëŠ” ë³¸ë¬¸ ì¶”ì¶œ í•¨ìˆ˜ (ì„±ëŠ¥ ìµœì í™”)
# =============================================================================

def extract_meaningful_content(text: str) -> str:
    """Phase 2: ì˜ë¯¸ìˆëŠ” ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ ì•Œê³ ë¦¬ì¦˜ (ì„±ëŠ¥ ìµœì í™”)"""
    if not text or len(text) < 30:
        return ""

    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í•  (ê°œì„ ëœ ì •ê·œì‹)
    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]\s*', text.strip())
    sentences = [s.strip() for s in sentences if s.strip()]

    if not sentences:
        return text[:100].strip()

    # ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ í•„í„°ë§ ì‹œìŠ¤í…œ
    meaningful_sentences = []

    for sentence in sentences:
        if len(sentence) < 10:  # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸
            continue

        # ì˜ë¯¸ì—†ëŠ” ë¬¸ì¥ íŒ¨í„´ ì œì™¸
        meaningless_patterns = [
            r'^[ã…‹ã…ã„·ã… ã…œã…¡]+$',  # ììŒëª¨ìŒë§Œ
            r'^[!@#$%^&*()_+\-=\[\]{}|;\':",./<>?`~]+$',  # íŠ¹ìˆ˜ë¬¸ìë§Œ
            r'^\d+$',  # ìˆ«ìë§Œ
            r'^(ìŒ|ì–´|ì•„|ë„¤|ì˜ˆ|ì‘|ã…‡ã…‡|ã… ã… |ã…œã…œ)$',  # ë‹¨ìˆœ ê°íƒ„ì‚¬
        ]

        if any(re.match(pattern, sentence) for pattern in meaningless_patterns):
            continue

        # Epic7 ê´€ë ¨ ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œ ìŠ¤ì½”ì–´ë§
        meaningful_keywords = [
            'ë²„ê·¸', 'ì˜¤ë¥˜', 'ë¬¸ì œ', 'ì—ëŸ¬', 'ì•ˆë¨', 'ì‘ë™', 'ì‹¤í–‰',
            'ìºë¦­í„°', 'ìŠ¤í‚¬', 'ì•„í‹°íŒ©íŠ¸', 'ì¥ë¹„', 'ë˜ì „', 'ì•„ë ˆë‚˜', 
            'ê¸¸ë“œ', 'ì´ë²¤íŠ¸', 'ì—…ë°ì´íŠ¸', 'íŒ¨ì¹˜', 'ë°¸ëŸ°ìŠ¤', 'ë„ˆí”„',
            'ê²Œì„', 'í”Œë ˆì´', 'ìœ ì €', 'ìš´ì˜', 'ê³µì§€', 'í™•ë¥ ',
            'ë½‘ê¸°', 'ì†Œí™˜', '6ì„±', 'ê°ì„±', 'ì´ˆì›”', 'ë£¬', 'ì ¬'
        ]

        score = sum(1 for keyword in meaningful_keywords if keyword in sentence)

        # ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ìœ¼ë¡œ íŒë³„
        if score > 0 or len(sentence) >= 30:
            meaningful_sentences.append(sentence)

    if not meaningful_sentences:
        # í´ë°±: ì²« ë²ˆì§¸ ê¸´ ë¬¸ì¥
        long_sentences = [s for s in sentences if len(s) >= 20]
        if long_sentences:
            return long_sentences[0]
        else:
            return sentences[0] if sentences else text[:100]

    # ìµœì  ì¡°í•©: 1-3ê°œ ë¬¸ì¥ ì¡°í•©ìœ¼ë¡œ ì˜ë¯¸ìˆëŠ” ë‚´ìš© êµ¬ì„±
    result = meaningful_sentences[0]

    # ì²« ë²ˆì§¸ ë¬¸ì¥ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ë‘ ë²ˆì§¸ ë¬¸ì¥ ì¶”ê°€
    if len(result) < 50 and len(meaningful_sentences) > 1:
        result += ' ' + meaningful_sentences[1]

    # ì—¬ì „íˆ ë¶€ì¡±í•˜ë©´ ì„¸ ë²ˆì§¸ ë¬¸ì¥ê¹Œì§€ ì¶”ê°€
    if len(result) < 80 and len(meaningful_sentences) > 2:
        result += ' ' + meaningful_sentences[2]

    return result.strip()

# =============================================================================
# Phase 2: Stove ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ í•¨ìˆ˜ - ì„±ëŠ¥ ìµœì í™” ì™„ë£Œ
# =============================================================================

def get_stove_post_content(post_url: str, driver: webdriver.Chrome, 
                          source: str = "stove_korea_bug", 
                          schedule_type: str = "frequent") -> str:
    """Phase 2: ìŠ¤í† ë¸Œ ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ - ì„±ëŠ¥ ìµœì í™” ì™„ë£Œ"""

    # ìºì‹œ í™•ì¸
    cache = load_content_cache()
    url_hash = hash(post_url) % (10**8)

    if str(url_hash) in cache:
        cached_item = cache[str(url_hash)]
        cache_time = datetime.fromisoformat(cached_item.get('timestamp', '2000-01-01'))
        if datetime.now() - cache_time < timedelta(hours=24):
            print(f"[CACHE] ìºì‹œëœ ë‚´ìš© ì‚¬ìš©: {post_url}")
            return cached_item.get('content', "ê²Œì‹œê¸€ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    content_summary = "ê²Œì‹œê¸€ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    try:
        print(f"[DEBUG] ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ ì‹œë„: {post_url}")

        wait_time = CrawlingSchedule.get_wait_time(schedule_type)
        driver.set_page_load_timeout(wait_time + 10)
        driver.get(post_url)

        print(f"[DEBUG] í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘... ({wait_time}ì´ˆ)")
        time.sleep(wait_time)

        # JavaScript ì™„ì „ ë¡œë”© í™•ì¸
        WebDriverWait(driver, 15).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )

        # Phase 2 ìµœì í™”: ë‹¨ê³„ë³„ ìŠ¤í¬ë¡¤ë§ (ì„±ëŠ¥ ê°œì„ )
        print("[DEBUG] ìµœì í™”ëœ ìŠ¤í¬ë¡¤ë§ ì‹œì‘...")
        driver.execute_script("window.scrollTo(0, 500);")
        time.sleep(2)
        driver.execute_script("window.scrollTo(0, 1000);")
        time.sleep(2)
        driver.execute_script("window.scrollTo(0, 0);")
        time.sleep(1)
        print("[DEBUG] ìµœì í™”ëœ ìŠ¤í¬ë¡¤ë§ ì™„ë£Œ")

        # Phase 2: Master ë°œê²¬ CSS Selector ìš°ì„  ì ìš©
        content_selectors = [
            # Master ì§€ì ì‚¬í•­: ëª©ë¡ í˜ì´ì§€ì—ì„œ ì§ì ‘ ì¶”ì¶œ
            'meta[data-vmid="description"]',
            'meta[name="description"]',

            # ê°œë³„ í˜ì´ì§€ ì„ íƒìë“¤ (ë°±ì—…)
            'div.s-article-content',
            'div.s-article-content-text',
            'section.s-article-body',
            'div.s-board-content',

            # Phase 2: ì¶”ê°€ ë°±ì—… ì„ íƒì
            '.article-content',
            '.post-content',
            '[class*="content"]'
        ]

        # Phase 2: ì˜ë¯¸ìˆëŠ” ë³¸ë¬¸ ì¶”ì¶œ ì•Œê³ ë¦¬ì¦˜ ì ìš©
        for i, selector in enumerate(content_selectors):
            try:
                elements = driver.find_elements(By.CSS_SELECTOR, selector)
                if elements:
                    for element in elements:
                        # ë©”íƒ€ íƒœê·¸ëŠ” content ì†ì„±ì—ì„œ, ì¼ë°˜ íƒœê·¸ëŠ” textì—ì„œ ì¶”ì¶œ
                        if selector.startswith('meta'):
                            raw_text = element.get_attribute('content').strip()
                        else:
                            raw_text = element.text.strip()

                        if not raw_text or len(raw_text) < 30:
                            continue           

                        # Phase 2: ë©”íƒ€ë°ì´í„° í•„í„°ë§ ê°•í™”
                        skip_keywords = [
                            'install stove', 'ìŠ¤í† ë¸Œë¥¼ ì„¤ì¹˜', 'ë¡œê·¸ì¸ì´ í•„ìš”', 
                            'javascript', 'ëŒ“ê¸€', 'ê³µìœ ', 'ì¢‹ì•„ìš”', 'ì¶”ì²œ', 'ì‹ ê³ ',
                            'ì‘ì„±ì', 'ì‘ì„±ì¼', 'ì¡°íšŒìˆ˜', 'ì²¨ë¶€íŒŒì¼', 'ë‹¤ìš´ë¡œë“œ',
                            'copyright', 'ì €ì‘ê¶Œ', 'ì´ìš©ì•½ê´€', 'ê°œì¸ì •ë³´', 'ì¿ í‚¤',
                            'ê´‘ê³ ', 'ad', 'advertisement', 'í”„ë¡œëª¨ì…˜', 'ì´ë²¤íŠ¸',
                            'ë¡œê·¸ì¸', 'login', 'sign in', 'íšŒì›ê°€ì…', 'register',
                            'ë©”ë‰´', 'menu', 'navigation', 'ë„¤ë¹„ê²Œì´ì…˜', 'ì‚¬ì´ë“œë°”',
                            'ë°°ë„ˆ', 'banner', 'í‘¸í„°', 'footer', 'í—¤ë”', 'header'
                        ]

                        if any(skip.lower() in raw_text.lower() for skip in skip_keywords):
                            continue

                        # Phase 2: ì˜ë¯¸ìˆëŠ” ë¬¸ë‹¨ ì¶”ì¶œ (ì„±ëŠ¥ ìµœì í™”)
                        meaningful_content = extract_meaningful_content(raw_text)

                        # Phase 2: ìµœì†Œ ê¸¸ì´ 50ì ì´ìƒìœ¼ë¡œ ì¦ê°€
                        if len(meaningful_content) >= 50:
                            # 150ì ì´ë‚´ë¡œ ìš”ì•½
                            if len(meaningful_content) > 150:
                                content_summary = meaningful_content[:147] + '...'
                            else:
                                content_summary = meaningful_content

                            print(f"[SUCCESS] ì„ íƒì {i+1}/{len(content_selectors)} '{selector}'ë¡œ ë‚´ìš© ì¶”ì¶œ ì„±ê³µ")
                            print(f"[CONTENT] {content_summary[:80]}...")
                            break

                    if content_summary != "ê²Œì‹œê¸€ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.":
                        break

            except Exception as e:
                print(f"[DEBUG] ì„ íƒì '{selector}' ì‹¤íŒ¨: {e}")
                continue

        # ìºì‹œ ì €ì¥
        cache[str(url_hash)] = {
            'content': content_summary,
            'timestamp': datetime.now().isoformat(),
            'url': post_url,
            'source': source
        }
        save_content_cache(cache)

    except TimeoutException:
        print(f"[ERROR] í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ: {post_url}")
        content_summary = "â° ê²Œì‹œê¸€ ë¡œë”© ì‹œê°„ ì´ˆê³¼"
    except Exception as e:
        print(f"[ERROR] ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        content_summary = "ğŸ”— ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ ì‹¤íŒ¨"

    return content_summary

# =============================================================================
# Phase 2: Stove ê²Œì‹œíŒ í¬ë¡¤ë§ í•¨ìˆ˜ - ğŸš€ ì¤‘ë³µ ì²´í¬ ë¡œì§ ê°œì„ 
# =============================================================================

def crawl_stove_board(board_url: str, source: str, force_crawl: bool = False, 
                     schedule_type: str = "frequent", region: str = "korea") -> List[Dict]:
    """Stove ê²Œì‹œíŒ í¬ë¡¤ë§ - ğŸš€ ì¤‘ë³µ ì²´í¬ ë¡œì§ ê°œì„  ì ìš©"""

    posts = []
    link_data = load_crawled_links()

    print(f"[INFO] {source} í¬ë¡¤ë§ ì‹œì‘ - URL: {board_url}")
    print(f"[DEBUG] ê¸°ì¡´ ë§í¬ ìˆ˜: {len(link_data['links'])}, Force Crawl: {force_crawl}")

    driver = None
    try:
        driver = get_chrome_driver()

        wait_time = CrawlingSchedule.get_wait_time(schedule_type)
        driver.set_page_load_timeout(wait_time + 10)
        driver.implicitly_wait(15)

        print(f"[DEBUG] ê²Œì‹œíŒ ì ‘ì† ì¤‘: {board_url}")
        driver.get(board_url)

        print(f"[DEBUG] í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘... ({wait_time}ì´ˆ)")
        time.sleep(wait_time)

        # JavaScript ì™„ì „ ë¡œë”© í™•ì¸
        WebDriverWait(driver, 15).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )

        # Phase 2: ìµœì í™”ëœ ìŠ¤í¬ë¡¤ë§ (ì„±ëŠ¥ ê°œì„ )
        driver.execute_script("window.scrollTo(0, 800);")
        time.sleep(3)

        # ë””ë²„ê¹…ìš© HTML ì €ì¥
        debug_filename = f"{source}_debug_selenium.html"
        with open(debug_filename, "w", encoding="utf-8") as f:
            f.write(driver.page_source)
        print(f"[DEBUG] HTML ì €ì¥: {debug_filename}")

        # Phase 2: Master ë°œê²¬ ì„ íƒì ìš°ì„  ì ìš© - JavaScript ìµœì í™”
        user_posts = driver.execute_script("""
            var userPosts = [];

            // Phase 2: Master ì§€ì ì‚¬í•­ - section.s-board-item ìµœìš°ì„  ì ìš©
            const selectors = [
                'section.s-board-item',           // Master ë°œê²¬ ì„ íƒì (ìµœìš°ì„ )
                'h3.s-board-title',               // ê¸°ì¡´ ì„ íƒì (ë°±ì—…)
                '[class*="board-title"]',         // í´ë˜ìŠ¤ëª… í¬í•¨
                '[class*="post-title"]',          // post-title í¬í•¨
                '[class*="article-title"]',       // article-title í¬í•¨
                'h3[class*="title"]',            // h3 íƒœê·¸ title í¬í•¨
                'a[href*="/view/"]'              // view ë§í¬ ì§ì ‘ ì°¾ê¸°
            ];

            var elements = [];
            var successful_selector = '';

            // ì„ íƒìë³„ ì‹œë„
            for (var i = 0; i < selectors.length; i++) {
                try {
                    elements = document.querySelectorAll(selectors[i]);
                    if (elements && elements.length > 0) {
                        successful_selector = selectors[i];
                        console.log('Phase 2 ì„ íƒì ì„±ê³µ:', selectors[i], 'ê°œìˆ˜:', elements.length);
                        break;
                    }
                } catch (e) {
                    console.log('ì„ íƒì ì‹¤íŒ¨:', selectors[i], e);
                    continue;
                }
            }

            if (!elements || elements.length === 0) {
                console.log('ëª¨ë“  ì„ íƒì ì‹¤íŒ¨');
                return [];
            }

            console.log('ì´ ë°œê²¬ëœ ìš”ì†Œ ìˆ˜:', elements.length);

            // ê³µì§€ì‚¬í•­ IDë“¤ (ì œì™¸ ëŒ€ìƒ)
            const officialIds = ['10518001', '10855687', '10855562', '10855132'];

            // ê° ìš”ì†Œì—ì„œ ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ
            for (var i = 0; i < Math.min(elements.length, 20); i++) {
                var element = elements[i];

                try {
                    var linkElement, titleElement, contentElement = null;
                    var href = '', title = '', preview_content = '';

                    // ë§í¬ ìš”ì†Œ ì°¾ê¸°
                    if (successful_selector === 'section.s-board-item') {
                        // Phase 2: Master ì§€ì ì‚¬í•­ - ëª©ë¡ í˜ì´ì§€ì—ì„œ ì§ì ‘ ë³¸ë¬¸ ì¶”ì¶œ
                        linkElement = element.querySelector('a[href*="/view/"]');
                        titleElement = element.querySelector('.s-board-title-text, .board-title, h3 span, .title');

                        // Master ë°œê²¬: p.s-board-textì—ì„œ ë³¸ë¬¸ ì§ì ‘ ì¶”ì¶œ
                        contentElement = element.querySelector('p.s-board-text');
                        if (contentElement) {
                            preview_content = contentElement.textContent?.trim() || '';
                        }
                    } else {
                        // ê¸°íƒ€ ì„ íƒì ê¸°ë°˜ ì¶”ì¶œ
                        linkElement = element.closest('a[href*="/view/"]') || element.querySelector('a[href*="/view/"]');
                        titleElement = element;
                    }

                    // ë§í¬ ì¶”ì¶œ
                    if (linkElement && linkElement.href) {
                        href = linkElement.href;
                    }

                    // ì œëª© ì¶”ì¶œ
                    if (titleElement) {
                        title = titleElement.textContent?.trim() || titleElement.innerText?.trim() || '';
                    }

                    // ìœ íš¨ì„± ê²€ì‚¬
                    if (!href || !title || title.length < 3) {
                        continue;
                    }

                    // URLì—ì„œ ê²Œì‹œê¸€ ID ì¶”ì¶œ
                    var idMatch = href.match(/\/view\/(\d+)/);
                    if (!idMatch) {
                        continue;
                    }
                    var id = idMatch[1];

                    // ê³µì§€ì‚¬í•­ ì œì™¸
                    if (officialIds.includes(id)) {
                        console.log('ê³µì§€ì‚¬í•­ ì œì™¸:', id, title.substring(0, 20));
                        continue;
                    }

                    // ê³µì§€/ì´ë²¤íŠ¸ ë°°ì§€ í™•ì¸
                    var isNotice = element.querySelector('i.element-badge__s.notice, .notice, [class*="notice"]');
                    var isEvent = element.querySelector('i.element-badge__s.event, .event, [class*="event"]');
                    var isOfficial = element.querySelector('span.s-profile-staff-official, [class*="official"]');

                    if (isNotice || isEvent || isOfficial) {
                        console.log('ê³µì§€/ì´ë²¤íŠ¸ ì œì™¸:', title.substring(0, 20));
                        continue;
                    }

                    // ì œëª©ì—ì„œ [ê³µì§€], [ì´ë²¤íŠ¸] ë“± í‚¤ì›Œë“œ ì œì™¸  
                    var skipKeywords = ['[ê³µì§€]', '[ì´ë²¤íŠ¸]', '[ì•ˆë‚´]', '[ì ê²€]', '[ê³µì§€ì‚¬í•­]'];
                    var shouldSkip = skipKeywords.some(function(keyword) {
                        return title.includes(keyword);
                    });

                    if (shouldSkip) {
                        console.log('í‚¤ì›Œë“œ ì œì™¸:', title.substring(0, 20));
                        continue;
                    }

                    // URL ì •ê·œí™”
                    var fullUrl = href.startsWith('http') ? href : 'https://page.onstove.com' + href;

                    userPosts.push({
                        href: fullUrl,
                        id: id,
                        title: title.substring(0, 200).trim(),
                        preview_content: preview_content.substring(0, 150).trim(),
                        selector_used: successful_selector
                    });

                    console.log('Phase 2 ê²Œì‹œê¸€ ì¶”ê°€:', title.substring(0, 30));

                } catch (e) {
                    console.log('ê²Œì‹œê¸€ ì²˜ë¦¬ ì˜¤ë¥˜:', e.message);
                    continue;
                }
            }

            console.log('Phase 2 ìµœì¢… ì¶”ì¶œëœ ìœ ì € ê²Œì‹œê¸€ ìˆ˜:', userPosts.length);
            return userPosts;
        """)

        print(f"[DEBUG] Phase 2 JavaScriptë¡œ {len(user_posts)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")

        # ê° ê²Œì‹œê¸€ ì²˜ë¦¬ - ğŸš€ ì¤‘ë³µ ì²´í¬ ë¡œì§ ê°œì„  ì ìš©
        for i, post_info in enumerate(user_posts, 1):
            try:
                href = post_info['href']
                title = post_info['title']
                post_id = post_info['id']
                preview_content = post_info.get('preview_content', '')

                # URL ë²„ê·¸ ìˆ˜ì • ì ìš©
                href = fix_url_bug(href)

                print(f"[DEBUG] ê²Œì‹œê¸€ {i}/{len(user_posts)}: {title[:40]}...")
                print(f"[DEBUG] URL: {href}")

                # ğŸš€ í•µì‹¬ ìˆ˜ì •: ì‹œê°„ ê¸°ë°˜ ì¤‘ë³µ í™•ì¸ (24ì‹œê°„ ë‚´ ì²˜ë¦¬ëœ ê²½ìš°ë§Œ SKIP)
                if not force_crawl and is_recently_processed(href, link_data["links"]):
                    print(f"[SKIP] 24ì‹œê°„ ë‚´ ì²˜ë¦¬ëœ ë§í¬: {post_id}")
                    continue

                # ì œëª© ê¸¸ì´ ê²€ì¦
                if len(title) < 5:
                    print(f"[SKIP] ì œëª©ì´ ë„ˆë¬´ ì§§ìŒ: {title}")
                    continue

                # Phase 2: ëª©ë¡ í˜ì´ì§€ì—ì„œ ì¶”ì¶œí•œ ë³¸ë¬¸ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê°œë³„ í˜ì´ì§€ ë°©ë¬¸
                if preview_content and len(preview_content) >= 50:
                    content = preview_content
                    print(f"[PHASE2] ëª©ë¡ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ì§ì ‘ ì¶”ì¶œ ì„±ê³µ (90% ì‹œê°„ ë‹¨ì¶•)")
                else:
                    # ê°œë³„ í˜ì´ì§€ ë°©ë¬¸ (ë°±ì—…)
                    content = get_stove_post_content(href, driver, source, schedule_type)

                # ê²Œì‹œê¸€ ë°ì´í„° êµ¬ì„±
                post_data = {
                    "title": title,
                    "url": href,
                    "content": content,
                    "timestamp": datetime.now().isoformat(),
                    "source": source,
                    "id": post_id,
                    "region": region,
                    "schedule_type": schedule_type
                }

                posts.append(post_data)
                # ğŸš€ í•µì‹¬ ìˆ˜ì •: ì—¬ê¸°ì„œëŠ” ë§í¬ë¥¼ ì¶”ê°€í•˜ì§€ ì•ŠìŒ (ì•Œë¦¼ ì„±ê³µ í›„ì—ë§Œ ì¶”ê°€)

                print(f"[SUCCESS] ìƒˆ ê²Œì‹œê¸€ ìˆ˜ì§‘ ({i}): {title[:30]}...")
                print(f"[CONTENT] {content[:80]}...")

                # í¬ë¡¤ë§ ê°„ ëŒ€ê¸° (Rate Limiting)
                time.sleep(random.uniform(1, 3))

            except Exception as e:
                print(f"[ERROR] ê²Œì‹œê¸€ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue

        print(f"[INFO] {source} í¬ë¡¤ë§ ì™„ë£Œ: {len(user_posts)}ê°œ ì¤‘ {len(posts)}ê°œ ìƒˆ ê²Œì‹œê¸€")

    except Exception as e:
        print(f"[ERROR] {source} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    finally:
        if driver:
            try:
                driver.quit()
            except:
                pass

    # ğŸš€ í•µì‹¬ ìˆ˜ì •: ë§í¬ ë°ì´í„°ëŠ” ì—¬ê¸°ì„œ ì €ì¥í•˜ì§€ ì•ŠìŒ (ì•Œë¦¼ ì„±ê³µ í›„ì—ë§Œ ì €ì¥)
    return posts

# =============================================================================
# Phase 3: Reddit í¬ë¡¤ë§ ì™„ì „ êµ¬í˜„
# =============================================================================

def crawl_reddit_epic7(force_crawl: bool = False, limit: int = 10) -> List[Dict]:
    """Phase 3: Reddit r/EpicSeven ì„œë¸Œë ˆë”§ í¬ë¡¤ë§ ì™„ì „ êµ¬í˜„"""

    if not REDDIT_AVAILABLE:
        print("[WARNING] PRAW ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ Reddit í¬ë¡¤ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return []

    posts = []

    try:
        print("[INFO] Phase 3: Reddit í¬ë¡¤ë§ ì‹œì‘")

        # Reddit API ì„¤ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ê¸°)
        reddit = praw.Reddit(
            client_id=os.environ.get('REDDIT_CLIENT_ID', 'your_client_id'),
            client_secret=os.environ.get('REDDIT_CLIENT_SECRET', 'your_client_secret'),
            user_agent=os.environ.get('REDDIT_USER_AGENT', 'Epic7Monitor/1.0')
        )

        # r/EpicSeven ì„œë¸Œë ˆë”§ ì ‘ê·¼
        subreddit = reddit.subreddit('EpicSeven')

        # ìµœì‹  ê²Œì‹œê¸€ë“¤ ê°€ì ¸ì˜¤ê¸°
        submissions = subreddit.new(limit=limit)

        link_data = load_crawled_links()

        for submission in submissions:
            try:
                # Reddit URL ìƒì„±
                reddit_url = f"https://www.reddit.com{submission.permalink}"

                # ğŸš€ ì‹œê°„ ê¸°ë°˜ ì¤‘ë³µ í™•ì¸
                if not force_crawl and is_recently_processed(reddit_url, link_data["links"]):
                    continue

                # ì œëª© ê²€ì¦
                if len(submission.title) < 5:
                    continue

                # ìŠ¤íŒ¸/ê´‘ê³ ì„± ê²Œì‹œë¬¼ í•„í„°ë§
                spam_keywords = ['buy', 'sell', 'trade', 'account', 'giveaway', 'free']
                if any(keyword.lower() in submission.title.lower() for keyword in spam_keywords):
                    continue

                # Epic7 ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
                epic7_keywords = ['epic seven', 'epic7', 'e7', 'character', 'hero', 'artifact', 
                                'summon', 'gacha', 'gear', 'equipment', 'guild', 'arena']
                if not any(keyword.lower() in submission.title.lower() for keyword in epic7_keywords):
                    # ë³¸ë¬¸ì—ì„œë„ í™•ì¸
                    if hasattr(submission, 'selftext') and submission.selftext:
                        if not any(keyword.lower() in submission.selftext.lower() for keyword in epic7_keywords):
                            continue

                # ë‚´ìš© ì¶”ì¶œ
                content = ""
                if hasattr(submission, 'selftext') and submission.selftext:
                    content = submission.selftext[:200].strip()
                else:
                    content = f"Reddit ê²Œì‹œê¸€ - ë§í¬: {reddit_url}"

                # ê²Œì‹œê¸€ ë°ì´í„° êµ¬ì„±
                post_data = {
                    "title": submission.title,
                    "url": reddit_url,
                    "content": content,
                    "timestamp": datetime.now().isoformat(),
                    "source": "reddit_epicseven",
                    "id": submission.id,
                    "region": "global",
                    "schedule_type": "frequent",
                    "author": str(submission.author) if submission.author else "deleted",
                    "score": submission.score,
                    "comments": submission.num_comments
                }

                posts.append(post_data)
                # ğŸš€ ì—¬ê¸°ì„œë„ ë§í¬ë¥¼ ì¶”ê°€í•˜ì§€ ì•ŠìŒ (ì•Œë¦¼ ì„±ê³µ í›„ì—ë§Œ ì¶”ê°€)

                print(f"[SUCCESS] Reddit ê²Œì‹œê¸€ ì¶”ê°€: {submission.title[:50]}...")

            except Exception as e:
                print(f"[ERROR] Reddit ê²Œì‹œê¸€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue

        print(f"[INFO] Phase 3: Reddit í¬ë¡¤ë§ ì™„ë£Œ - {len(posts)}ê°œ ìƒˆ ê²Œì‹œê¸€")

    except Exception as e:
        print(f"[ERROR] Phase 3: Reddit í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

    return posts

# =============================================================================
# ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ í•¨ìˆ˜ (ê¸°ì¡´ ìœ ì§€)
# =============================================================================

def crawl_ruliweb_epic7() -> List[Dict]:
    """ë£¨ë¦¬ì›¹ ì—í”½ì„¸ë¸ ê²Œì‹œíŒ í¬ë¡¤ë§"""
    posts = []
    
    try:
        print("[INFO] ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì‹œì‘")
        
        # ê°„ë‹¨í•œ requests ê¸°ë°˜ í¬ë¡¤ë§
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36'
        }
        
        url = "https://bbs.ruliweb.com/game/85208"
        response = requests.get(url, headers=headers, timeout=30)
        
        if response.status_code == 200:
            print("[INFO] ë£¨ë¦¬ì›¹ ì ‘ì† ì„±ê³µ - ê¸°ë³¸ í¬ë¡¤ë§ ìˆ˜í–‰")
            # ê°„ë‹¨í•œ íŒŒì‹± ë¡œì§ (ìƒì„¸ êµ¬í˜„ ìƒëµ) 
            posts = []
        else:
            print(f"[WARNING] ë£¨ë¦¬ì›¹ ì ‘ì† ì‹¤íŒ¨: {response.status_code}")
            
    except Exception as e:
        print(f"[ERROR] ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    return posts

# =============================================================================
# Phase 1: í†µí•© í¬ë¡¤ë§ í•¨ìˆ˜ë“¤ - ğŸš€ ì•ˆì •ì„± ê°•í™”
# =============================================================================

def crawl_frequent_sites(force_crawl: bool = False) -> List[Dict]:
    """Phase 1: 15ë¶„ ì£¼ê¸° - ì „ì²´ í¬ë¡¤ë§ ğŸš€ ì•ˆì •ì„± ê°•í™”"""
    all_posts = []

    print("[INFO] === Phase 1: 15ë¶„ ì£¼ê¸° ì „ì²´ í¬ë¡¤ë§ ì‹œì‘ (ì•ˆì •ì„± ê°•í™”) ===")

    # ğŸš€ Master ìš”ì²­: ì‚¬ì´íŠ¸ë³„ ë…ë¦½ ì‹¤í–‰ìœ¼ë¡œ ì•ˆì •ì„± ê°•í™”
    crawl_tasks = [
        ('í•œêµ­ ë²„ê·¸ ê²Œì‹œíŒ', lambda: crawl_stove_board(
            "https://page.onstove.com/epicseven/kr/list/1012?page=1&direction=LATEST",
            "stove_korea_bug", force_crawl, "frequent", "korea")),
        ('ê¸€ë¡œë²Œ ë²„ê·¸ ê²Œì‹œíŒ', lambda: crawl_stove_board(
            "https://page.onstove.com/epicseven/global/list/998?page=1&direction=LATEST", 
            "stove_global_bug", force_crawl, "frequent", "global")),
        ('í•œêµ­ ììœ ê²Œì‹œíŒ', lambda: crawl_stove_board(
            "https://page.onstove.com/epicseven/kr/list/1005?page=1&direction=LATEST",
            "stove_korea_general", force_crawl, "frequent", "korea")),
        ('ê¸€ë¡œë²Œ ììœ ê²Œì‹œíŒ', lambda: crawl_stove_board(
            "https://page.onstove.com/epicseven/global/list/989?page=1&direction=LATEST",
            "stove_global_general", force_crawl, "frequent", "global"))
    ]

    # ğŸš€ ê° ì‚¬ì´íŠ¸ë¥¼ ë…ë¦½ì ìœ¼ë¡œ í¬ë¡¤ë§ (í•µì‹¬ ìˆ˜ì •)
    for site_name, crawl_func in crawl_tasks:
        try:
            print(f"[INFO] ğŸŒ {site_name} í¬ë¡¤ë§ ì‹œì‘...")
            posts = crawl_func()
            all_posts.extend(posts)
            print(f"[INFO] âœ… {site_name}: {len(posts)}ê°œ")
            
            # ì‚¬ì´íŠ¸ ê°„ ëŒ€ê¸° (ë©”ëª¨ë¦¬ ì •ë¦¬)
            time.sleep(random.uniform(3, 6))
            
        except Exception as e:
            print(f"[ERROR] âŒ {site_name} ì‹¤íŒ¨í•˜ì§€ë§Œ ê³„ì† ì§„í–‰: {e}")
            continue  # ğŸš€ í•µì‹¬: ì‹¤íŒ¨í•´ë„ ë‹¤ìŒ ì‚¬ì´íŠ¸ ê³„ì†

    # Redditê³¼ ë£¨ë¦¬ì›¹ë„ ë…ë¦½ ì‹¤í–‰
    try:
        reddit_posts = crawl_reddit_epic7(force_crawl=force_crawl, limit=8)
        all_posts.extend(reddit_posts)
        print(f"[INFO] âœ… Reddit: {len(reddit_posts)}ê°œ")
    except Exception as e:
        print(f"[ERROR] âŒ Reddit ì‹¤íŒ¨í•˜ì§€ë§Œ ê³„ì† ì§„í–‰: {e}")

    try:
        ruliweb_posts = crawl_ruliweb_epic7()
        all_posts.extend(ruliweb_posts)
        print(f"[INFO] âœ… ë£¨ë¦¬ì›¹: {len(ruliweb_posts)}ê°œ")
    except Exception as e:
        print(f"[ERROR] âŒ ë£¨ë¦¬ì›¹ ì‹¤íŒ¨í•˜ì§€ë§Œ ê³„ì† ì§„í–‰: {e}")

    print(f"[INFO] ğŸ‰ Phase 1 ì™„ë£Œ: ì´ {len(all_posts)}ê°œ ìƒˆ ê²Œì‹œê¸€ (ì•ˆì •ì„± ë³´ì¥)")
    return all_posts

def crawl_regular_sites(force_crawl: bool = False) -> List[Dict]:
    """30ë¶„ ì£¼ê¸° - ì¼ë°˜ ê²Œì‹œíŒ í¬ë¡¤ë§ (ê¸°ì¡´ ìœ ì§€)"""
    all_posts = []

    print("[INFO] === 30ë¶„ ì£¼ê¸°: ì¼ë°˜ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹œì‘ ===")

    try:
        # í•œêµ­ ììœ ê²Œì‹œíŒ
        stove_kr_general_posts = crawl_stove_board(
            board_url="https://page.onstove.com/epicseven/kr/list/1005?page=1&direction=LATEST",
            source="stove_korea_general", 
            force_crawl=force_crawl,
            schedule_type="regular",
            region="korea"
        )
        all_posts.extend(stove_kr_general_posts)
        print(f"[INFO] í•œêµ­ ììœ ê²Œì‹œíŒ: {len(stove_kr_general_posts)}ê°œ")

        time.sleep(random.uniform(8, 12))

        # ê¸€ë¡œë²Œ ììœ ê²Œì‹œíŒ
        stove_global_general_posts = crawl_stove_board(
            board_url="https://page.onstove.com/epicseven/global/list/989?page=1&direction=LATEST",
            source="stove_global_general",
            force_crawl=force_crawl,
            schedule_type="regular",
            region="global"
        )
        all_posts.extend(stove_global_general_posts)
        print(f"[INFO] ê¸€ë¡œë²Œ ììœ ê²Œì‹œíŒ: {len(stove_global_general_posts)}ê°œ")

        time.sleep(random.uniform(8, 12))

        # ë£¨ë¦¬ì›¹ (ì¶”ê°€)
        ruliweb_posts = crawl_ruliweb_epic7()
        all_posts.extend(ruliweb_posts)
        print(f"[INFO] ë£¨ë¦¬ì›¹: {len(ruliweb_posts)}ê°œ")

        time.sleep(random.uniform(5, 8))

        # Phase 3: Reddit í¬ë¡¤ë§ ì¶”ê°€
        reddit_posts = crawl_reddit_epic7(force_crawl=force_crawl, limit=15)
        all_posts.extend(reddit_posts)
        print(f"[INFO] Phase 3: Reddit ì¶”ê°€: {len(reddit_posts)}ê°œ")

    except Exception as e:
        print(f"[ERROR] ì¼ë°˜ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

    print(f"[INFO] === 30ë¶„ ì£¼ê¸° ì™„ë£Œ: ì´ {len(all_posts)}ê°œ ìƒˆ ê²Œì‹œê¸€ ===")
    return all_posts

def crawl_by_schedule(schedule_type: str, force_crawl: bool = False) -> List[Dict]:
    """ìŠ¤ì¼€ì¤„ íƒ€ì…ì— ë”°ë¥¸ í¬ë¡¤ë§ ë¶„ê¸° (í˜¸í™˜ì„± ìœ ì§€)"""

    if schedule_type == "frequent" or schedule_type == "15min":
        return crawl_frequent_sites(force_crawl)
    elif schedule_type == "regular" or schedule_type == "30min":
        return crawl_regular_sites(force_crawl)
    else:
        print(f"[ERROR] ì•Œ ìˆ˜ ì—†ëŠ” ìŠ¤ì¼€ì¤„ íƒ€ì…: {schedule_type}")
        return []

def get_all_posts_for_report() -> List[Dict]:
    """ë¦¬í¬íŠ¸ìš© - ëª¨ë“  ì‚¬ì´íŠ¸ í¬ë¡¤ë§ (í˜¸í™˜ì„± ìœ ì§€)"""
    print("[INFO] === ë¦¬í¬íŠ¸ìš© ì „ì²´ í¬ë¡¤ë§ ì‹œì‘ ===")
    
    # ê¸°ë³¸ì ìœ¼ë¡œ frequent ëª¨ë“œë¡œ ì‹¤í–‰
    return crawl_frequent_sites(force_crawl=False)

# =============================================================================
# ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
# =============================================================================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Epic7 í¬ë¡¤ëŸ¬ v4.2 - ì¤‘ë³µ ë¡œì§ ê°œì„ ")
    parser.add_argument('--test', choices=['frequent', 'regular', 'reddit'], help='í…ŒìŠ¤íŠ¸ ëª¨ë“œ')
    parser.add_argument('--force-crawl', action='store_true', help='ê°•ì œ í¬ë¡¤ë§')
    
    args = parser.parse_args()
    
    if args.test:
        if args.test == 'frequent':
            posts = crawl_frequent_sites(args.force_crawl)
        elif args.test == 'regular':
            posts = crawl_regular_sites(args.force_crawl)
        elif args.test == 'reddit':
            posts = crawl_reddit_epic7(args.force_crawl)
        
        print(f"\n=== í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===")
        print(f"í¬ë¡¤ë§ëœ ê²Œì‹œê¸€ ìˆ˜: {len(posts)}")
        for i, post in enumerate(posts[:3], 1):
            print(f"{i}. {post['title'][:50]}... (ì¶œì²˜: {post['source']})")
    else:
        print("ì‚¬ìš©ë²•: python crawler.py --test [frequent|regular|reddit] [--force-crawl]")