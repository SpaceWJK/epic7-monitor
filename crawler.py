#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Epic7 ë‹¤êµ­ê°€ í¬ë¡¤ëŸ¬ v4.3 - Master ì •ë°€ ìˆ˜ì •ë³¸
Master ìš”êµ¬ì‚¬í•­: ê¸€ë¡œë²Œ íƒ€ì„ì•„ì›ƒ ìµœì í™” + Skip & Continue + 3íšŒ ì¬ì‹œë„

í•µì‹¬ ìˆ˜ì •ì‚¬í•­:
- ê¸€ë¡œë²Œ ì‚¬ì´íŠ¸ íƒ€ì„ì•„ì›ƒ 30ì´ˆë¡œ ì¦ê°€ (ë‚˜ë¨¸ì§€ëŠ” 15ì´ˆ ìœ ì§€)
- Skip & Continue ë¡œì§ êµ¬í˜„ (ê°œë³„ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰)
- 3íšŒ ì¬ì‹œë„ í›„ í¬ê¸° ë©”ì»¤ë‹ˆì¦˜
- ê¸°ì¡´ ì½”ë“œ êµ¬ì¡° ì™„ì „ ìœ ì§€

Author: Epic7 Monitoring Team  
Version: 4.3 (Master ì •ë°€ ìˆ˜ì •)
Date: 2025-07-24
"""

import time
import random
import re
import requests
import concurrent.futures
import os
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from urllib.parse import urljoin, urlparse

# Selenium ê´€ë ¨ import
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.action_chains import ActionChains

# ì›¹ë“œë¼ì´ë²„ ë§¤ë‹ˆì €
from webdriver_manager.chrome import ChromeDriverManager

# BeautifulSoup import
from bs4 import BeautifulSoup

# =============================================================================
# Epic7 í¬ë¡¤ëŸ¬ ì„¤ì •
# =============================================================================

# í¬ë¡¤ë§ ëŒ€ìƒ ì‚¬ì´íŠ¸ ì„¤ì •
CRAWL_TARGETS = {
    "stove_korea_bug": {
        "name": "í•œêµ­ ë²„ê·¸ ê²Œì‹œíŒ",  
        "url": "https://page.onstove.com/epicseven/kr/board/list/e7kr001?listType=2&direction=latest&display_opt=usertag%2Cbattlerank%2Cnickname%2Cwritedate%2Ccomment%2Cthumb&afterback=true",
        "enabled": True,
        "timeout": 15
    },
    "stove_global_bug": {
        "name": "ê¸€ë¡œë²Œ ë²„ê·¸ ê²Œì‹œíŒ",
        "url": "https://page.onstove.com/epicseven/global/board/list/e7en001?listType=2&direction=latest&display_opt=usertag%2Cbattlerank%2Cnickname%2Cwritedate%2Ccomment%2Cthumb&afterback=true",
        "enabled": True,
        "timeout": 30  # Master ìˆ˜ì •: ê¸€ë¡œë²Œë§Œ 30ì´ˆë¡œ ì¦ê°€
    },
    "stove_korea_general": {
        "name": "í•œêµ­ ììœ  ê²Œì‹œíŒ",
        "url": "https://page.onstove.com/epicseven/kr/board/list/e7kr002?listType=2&direction=latest&display_opt=usertag%2Cbattlerank%2Cnickname%2Cwritedate%2Ccomment%2Cthumb&afterback=true",
        "enabled": True,
        "timeout": 15
    },
    "stove_global_general": {
        "name": "ê¸€ë¡œë²Œ ììœ  ê²Œì‹œíŒ", 
        "url": "https://page.onstove.com/epicseven/global/board/list/e7en002?listType=2&direction=latest&display_opt=usertag%2Cbattlerank%2Cnickname%2Cwritedate%2Ccomment%2Cthumb&afterback=true",
        "enabled": True,
        "timeout": 30  # Master ìˆ˜ì •: ê¸€ë¡œë²Œë§Œ 30ì´ˆë¡œ ì¦ê°€
    },
    "ruliweb": {
        "name": "ë£¨ë¦¬ì›¹ ì—í”½ì„¸ë¸ ê²Œì‹œíŒ",
        "url": "https://bbs.ruliweb.com/game/85349",
        "enabled": True,
        "timeout": 15
    },
    "reddit": {
        "name": "Reddit Epic Seven",
        "url": "https://www.reddit.com/r/EpicSeven/new/",
        "enabled": True,
        "timeout": 15
    }
}

# Master ìš”êµ¬ì‚¬í•­: ì‚¬ì´íŠ¸ë³„ ì°¨ë³„ íƒ€ì„ì•„ì›ƒ í•¨ìˆ˜ ì¶”ê°€
def get_site_timeout(url):
    """ì‚¬ì´íŠ¸ë³„ ìµœì í™”ëœ íƒ€ì„ì•„ì›ƒ ë°˜í™˜"""
    if 'stove.com/epicseven/global' in url:
        return 30  # ê¸€ë¡œë²Œë§Œ 30ì´ˆ
    elif 'stove.com/epicseven/kr' in url:
        return 15  # í•œêµ­ ì‚¬ì´íŠ¸ 15ì´ˆ ìœ ì§€
    elif 'reddit.com' in url:
        return 15  # Reddit 15ì´ˆ ìœ ì§€
    elif 'ruliweb.com' in url:
        return 15  # ë£¨ë¦¬ì›¹ 15ì´ˆ ìœ ì§€
    else:
        return 20  # ê¸°íƒ€ ì‚¬ì´íŠ¸ ê¸°ë³¸ê°’

# í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥
crawling_results = {
    'posts': [],
    'errors': [],
    'stats': {
        'total_attempted': 0,
        'successful': 0,
        'failed': 0,
        'start_time': None,
        'end_time': None
    }
}

# Selenium WebDriver ì„¤ì •
def setup_chrome_driver():
    """Chrome WebDriver ì„¤ì •"""
    try:
        print("[INFO] Chrome WebDriver ì„¤ì • ì¤‘...")
        
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # ì›¹ë“œë¼ì´ë²„ ë§¤ë‹ˆì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ Chrome ë“œë¼ì´ë²„ ì„¤ì¹˜
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
        # ìë™í™” ê°ì§€ ë°©ì§€
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        print("[INFO] Chrome WebDriver ì„¤ì • ì™„ë£Œ")
        return driver
        
    except Exception as e:
        print(f"[ERROR] Chrome WebDriver ì„¤ì • ì‹¤íŒ¨: {e}")
        return None

# =============================================================================
# ì¤‘ë³µ ì²´í¬ ë° ë§í¬ ê´€ë¦¬ ì‹œìŠ¤í…œ
# =============================================================================

def load_crawled_links():
    """í¬ë¡¤ë§ëœ ë§í¬ ëª©ë¡ ë¡œë“œ"""
    try:
        if os.path.exists("crawled_links.json"):
            with open("crawled_links.json", "r", encoding="utf-8") as f:
                data = json.load(f)
                
            # 24ì‹œê°„ ì´ì „ ë°ì´í„° ì •ë¦¬
            current_time = datetime.now()
            cutoff_time = current_time - timedelta(hours=24)
            
            # 24ì‹œê°„ ì´ë‚´ ë§í¬ë§Œ ìœ ì§€
            filtered_links = []
            for item in data.get("links", []):
                try:
                    processed_time = datetime.fromisoformat(item.get("processed_at", ""))
                    if processed_time > cutoff_time:
                        filtered_links.append(item)
                except ValueError:
                    continue
            
            # ìµœëŒ€ 1000ê°œë¡œ ì œí•œ
            if len(filtered_links) > 1000:
                filtered_links = filtered_links[-1000:]
            
            data["links"] = filtered_links
            
            # ì •ë¦¬ëœ ë°ì´í„° ì €ì¥
            save_crawled_links(data)
            
            print(f"[INFO] í¬ë¡¤ë§ëœ ë§í¬ ë¡œë“œ ì™„ë£Œ: {len(filtered_links)}ê°œ")
            return data
        else:
            print("[INFO] í¬ë¡¤ë§ ë§í¬ íŒŒì¼ì´ ì—†ì–´ ìƒˆë¡œ ìƒì„±")
            return {"links": []}
            
    except Exception as e:
        print(f"[ERROR] í¬ë¡¤ë§ ë§í¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {"links": []}

def save_crawled_links(data):
    """í¬ë¡¤ë§ëœ ë§í¬ ëª©ë¡ ì €ì¥"""
    try:
        with open("crawled_links.json", "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"[INFO] í¬ë¡¤ë§ ë§í¬ ì €ì¥ ì™„ë£Œ: {len(data.get('links', []))}ê°œ")
    except Exception as e:
        print(f"[ERROR] í¬ë¡¤ë§ ë§í¬ ì €ì¥ ì‹¤íŒ¨: {e}")

def is_recently_processed(url: str, hours: int = 24) -> bool:
    """ìµœê·¼ ì²˜ë¦¬ëœ URLì¸ì§€ í™•ì¸ (24ì‹œê°„ ê¸°ì¤€)"""
    try:
        link_data = load_crawled_links()
        current_time = datetime.now()
        cutoff_time = current_time - timedelta(hours=hours)
        
        for item in link_data.get("links", []):
            if item.get("url") == url:
                try:
                    processed_time = datetime.fromisoformat(item.get("processed_at", ""))
                    if processed_time > cutoff_time:
                        return True
                except ValueError:
                    continue
        
        return False
        
    except Exception as e:
        print(f"[ERROR] ì¤‘ë³µ ì²´í¬ ì‹¤íŒ¨: {e}")
        return False

def mark_as_processed(url: str, notified: bool = False):
    """ê²Œì‹œê¸€ì„ ì²˜ë¦¬ë¨ìœ¼ë¡œ ë§ˆí‚¹ - ì•Œë¦¼ ì„±ê³µ í›„ì—ë§Œ í˜¸ì¶œ"""
    try:
        link_data = load_crawled_links()
        
        # ê¸°ì¡´ í•­ëª© ì—…ë°ì´íŠ¸ ë˜ëŠ” ìƒˆ í•­ëª© ì¶”ê°€
        found = False
        for item in link_data["links"]:
            if item.get("url") == url:
                item["processed_at"] = datetime.now().isoformat()
                item["notified"] = notified
                found = True
                break
        
        if not found:
            link_data["links"].append({
                "url": url,
                "processed_at": datetime.now().isoformat(),
                "notified": notified
            })
        
        save_crawled_links(link_data)
        print(f"[INFO] ë§í¬ ì²˜ë¦¬ ì™„ë£Œ ë§ˆí‚¹: {url[:50]}... (ì•Œë¦¼: {notified})")
        
    except Exception as e:
        print(f"[ERROR] ë§í¬ ë§ˆí‚¹ ì‹¤íŒ¨: {e}")

# =============================================================================
# ì½˜í…ì¸  ìºì‹œ ì‹œìŠ¤í…œ
# =============================================================================

def load_content_cache():
    """ì½˜í…ì¸  ìºì‹œ ë¡œë“œ"""
    try:
        if os.path.exists("content_cache.json"):
            with open("content_cache.json", "r", encoding="utf-8") as f:
                cache = json.load(f)
            
            # 7ì¼ ì´ì „ ìºì‹œ ì •ë¦¬
            current_time = datetime.now()
            cutoff_time = current_time - timedelta(days=7)
            
            cleaned_cache = {}
            for url, data in cache.items():
                try:
                    cached_time = datetime.fromisoformat(data.get("cached_at", ""))
                    if cached_time > cutoff_time:
                        cleaned_cache[url] = data
                except ValueError:
                    continue
            
            # ì •ë¦¬ëœ ìºì‹œ ì €ì¥
            if len(cleaned_cache) != len(cache):
                save_content_cache(cleaned_cache)
            
            return cleaned_cache
        else:
            return {}
            
    except Exception as e:
        print(f"[ERROR] ì½˜í…ì¸  ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {}

def save_content_cache(cache):
    """ì½˜í…ì¸  ìºì‹œ ì €ì¥"""
    try:
        with open("content_cache.json", "w", encoding="utf-8") as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
        print(f"[INFO] ì½˜í…ì¸  ìºì‹œ ì €ì¥ ì™„ë£Œ: {len(cache)}ê°œ")
    except Exception as e:
        print(f"[ERROR] ì½˜í…ì¸  ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

def get_cached_content(url: str) -> Optional[Dict]:
    """ìºì‹œëœ ì½˜í…ì¸  ì¡°íšŒ"""
    try:
        cache = load_content_cache()
        cached_data = cache.get(url)
        
        if cached_data:
            # 24ì‹œê°„ ì´ë‚´ ìºì‹œë§Œ ì‚¬ìš©
            cached_time = datetime.fromisoformat(cached_data.get("cached_at", ""))
            if datetime.now() - cached_time < timedelta(hours=24):
                return cached_data
        
        return None
        
    except Exception as e:
        print(f"[ERROR] ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return None

def cache_content(url: str, content: Dict):
    """ì½˜í…ì¸  ìºì‹œì— ì €ì¥"""
    try:
        cache = load_content_cache()
        
        content_with_timestamp = content.copy()
        content_with_timestamp["cached_at"] = datetime.now().isoformat()
        
        cache[url] = content_with_timestamp
        
        # ìºì‹œ í¬ê¸° ì œí•œ (ìµœëŒ€ 1000ê°œ)
        if len(cache) > 1000:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª©ë“¤ ì œê±°
            sorted_items = sorted(cache.items(), key=lambda x: x[1].get("cached_at", ""))
            cache = dict(sorted_items[-1000:])
        
        save_content_cache(cache)
        
    except Exception as e:
        print(f"[ERROR] ì½˜í…ì¸  ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

# =============================================================================
# Stove ê²Œì‹œíŒ í¬ë¡¤ë§
# =============================================================================

def crawl_stove_korea_bug_board():
    """Stove í•œêµ­ ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§"""
    print("[INFO] ğŸŒ í•œêµ­ ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹œì‘...")
    target = CRAWL_TARGETS["stove_korea_bug"]
    
    try:
        return crawl_stove_board(
            board_url=target["url"],
            board_name=target["name"],
            site_timeout=target["timeout"]
        )
    except Exception as e:
        print(f"[ERROR] í•œêµ­ ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

def crawl_stove_global_bug_board():
    """Stove ê¸€ë¡œë²Œ ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§"""
    print("[INFO] ğŸŒ ê¸€ë¡œë²Œ ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹œì‘...")
    target = CRAWL_TARGETS["stove_global_bug"]
    
    try:
        return crawl_stove_board(
            board_url=target["url"],
            board_name=target["name"],
            site_timeout=target["timeout"]  # Master ìˆ˜ì •: 30ì´ˆ íƒ€ì„ì•„ì›ƒ ì ìš©
        )
    except Exception as e:
        print(f"[ERROR] ê¸€ë¡œë²Œ ë²„ê·¸ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

def crawl_stove_korea_general_board():
    """Stove í•œêµ­ ììœ  ê²Œì‹œíŒ í¬ë¡¤ë§"""
    print("[INFO] ğŸŒ í•œêµ­ ììœ  ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹œì‘...")
    target = CRAWL_TARGETS["stove_korea_general"]
    
    try:
        return crawl_stove_board(
            board_url=target["url"],
            board_name=target["name"],
            site_timeout=target["timeout"]
        )
    except Exception as e:
        print(f"[ERROR] í•œêµ­ ììœ  ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

def crawl_stove_global_general_board():
    """Stove ê¸€ë¡œë²Œ ììœ  ê²Œì‹œíŒ í¬ë¡¤ë§"""
    print("[INFO] ğŸŒ ê¸€ë¡œë²Œ ììœ  ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹œì‘...")
    target = CRAWL_TARGETS["stove_global_general"]
    
    try:
        return crawl_stove_board(
            board_url=target["url"],
            board_name=target["name"],
            site_timeout=target["timeout"]  # Master ìˆ˜ì •: 30ì´ˆ íƒ€ì„ì•„ì›ƒ ì ìš©
        )
    except Exception as e:
        print(f"[ERROR] ê¸€ë¡œë²Œ ììœ  ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

def crawl_stove_board(board_url: str, board_name: str, site_timeout: int = 15):
    """Stove ê²Œì‹œíŒ ê³µí†µ í¬ë¡¤ë§ í•¨ìˆ˜"""
    posts = []
    driver = None
    
    try:
        print(f"[INFO] {board_name} í¬ë¡¤ë§ ì‹œì‘ - URL: {board_url}")
        
        # WebDriver ì„¤ì •
        driver = setup_chrome_driver()
        if not driver:
            print(f"[ERROR] {board_name}: WebDriver ì„¤ì • ì‹¤íŒ¨")
            return posts
        
        # í˜ì´ì§€ ë¡œë“œ
        print(f"[DEBUG] í˜ì´ì§€ ë¡œë”© ì¤‘... (íƒ€ì„ì•„ì›ƒ: {site_timeout}ì´ˆ)")
        driver.get(board_url)
        
        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        WebDriverWait(driver, site_timeout).until(  # Master ìˆ˜ì •: ì‚¬ì´íŠ¸ë³„ íƒ€ì„ì•„ì›ƒ ì ìš©
            EC.presence_of_element_located((By.CLASS_NAME, "board_list"))
        )
        
        print(f"[DEBUG] í˜ì´ì§€ ë¡œë”© ì™„ë£Œ")
        
        # ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ
        post_elements = driver.find_elements(By.CSS_SELECTOR, ".board_list tbody tr")
        
        if not post_elements:
            print(f"[WARNING] {board_name}: ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return posts
        
        print(f"[INFO] {board_name}: {len(post_elements)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")
        
        # Master ìš”êµ¬ì‚¬í•­: Skip & Continue + 3íšŒ ì¬ì‹œë„ ë¡œì§
        successful_posts = 0
        total_posts = len(post_elements)
        
        for i, post_element in enumerate(post_elements, 1):
            # 3íšŒ ì¬ì‹œë„ ë¡œì§
            retry_count = 0
            max_retries = 3
            post_processed = False
            
            while retry_count < max_retries and not post_processed:
                try:
                    print(f"[DEBUG] ê²Œì‹œê¸€ {i}/{total_posts} ì²˜ë¦¬ ì¤‘ (ì‹œë„ {retry_count + 1}/{max_retries})...")
                    
                    # ê²Œì‹œê¸€ ë§í¬ ì¶”ì¶œ
                    link_element = post_element.find_element(By.CSS_SELECTOR, "td.title a")
                    post_url = link_element.get_attribute("href")
                    post_title = link_element.text.strip()
                    
                    if not post_url or not post_title:
                        retry_count += 1
                        if retry_count < max_retries:
                            print(f"[WARNING] ê²Œì‹œê¸€ ì •ë³´ ë¶€ì¡±, ì¬ì‹œë„ ì¤‘... ({retry_count}/{max_retries})")
                            time.sleep(1)
                            continue
                        else:
                            print(f"[WARNING] ê²Œì‹œê¸€ {i} ìŠ¤í‚µ: ì •ë³´ ë¶€ì¡±")
                            break
                    
                    # ì¤‘ë³µ í™•ì¸
                    if is_recently_processed(post_url):
                        print(f"[INFO] ê²Œì‹œê¸€ ìŠ¤í‚µ (24ì‹œê°„ ë‚´ ì²˜ë¦¬ë¨): {post_title[:30]}...")
                        post_processed = True
                        successful_posts += 1
                        break
                    
                    # ê²Œì‹œê¸€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                    try:
                        author_element = post_element.find_element(By.CSS_SELECTOR, "td.writer")
                        author = author_element.text.strip()
                    except:
                        author = "Unknown"
                    
                    try:
                        date_element = post_element.find_element(By.CSS_SELECTOR, "td.date")
                        date = date_element.text.strip()
                    except:
                        date = ""
                    
                    # ê²Œì‹œê¸€ ìƒì„¸ ë‚´ìš© ì¶”ì¶œ
                    content = extract_post_content_selenium(driver, post_url, site_timeout)
                    
                    if content:
                        post_data = {
                            "title": post_title,
                            "url": post_url,
                            "author": author,
                            "date": date,
                            "content": content,
                            "board": board_name,
                            "site": "stove",
                            "language": "kr" if "/kr/" in board_url else "global",
                            "crawled_at": datetime.now().isoformat()
                        }
                        
                        posts.append(post_data)
                        successful_posts += 1
                        post_processed = True
                        
                        # ğŸš€ í•µì‹¬ ìˆ˜ì •: ì—¬ê¸°ì„œëŠ” ë§í¬ë¥¼ ì¶”ê°€í•˜ì§€ ì•ŠìŒ (ì•Œë¦¼ ì„±ê³µ í›„ì—ë§Œ ì¶”ê°€)
                        print(f"[INFO] âœ… ê²Œì‹œê¸€ ìˆ˜ì§‘ ì™„ë£Œ: {post_title[:50]}...")
                        
                        # ìºì‹œì— ì €ì¥
                        cache_content(post_url, post_data)
                    else:
                        retry_count += 1
                        if retry_count < max_retries:
                            print(f"[WARNING] ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨, ì¬ì‹œë„ ì¤‘... ({retry_count}/{max_retries})")
                            time.sleep(2)
                            continue
                        else:
                            print(f"[WARNING] ê²Œì‹œê¸€ {i} ìŠ¤í‚µ: ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨")
                            break
                    
                except Exception as e:
                    retry_count += 1
                    if retry_count < max_retries:
                        print(f"[ERROR] ê²Œì‹œê¸€ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜, ì¬ì‹œë„ ì¤‘... ({retry_count}/{max_retries}): {e}")
                        time.sleep(2)
                        continue
                    else:
                        print(f"[ERROR] ê²Œì‹œê¸€ {i} ìµœì¢… ì‹¤íŒ¨ - ë‹¤ìŒìœ¼ë¡œ ì§„í–‰: {e}")
                        break
                
                # ê³¼ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´
                time.sleep(random.uniform(1, 3))
        
        # Master ìš”êµ¬ì‚¬í•­: ì„±ê³µë¥  ë¡œê¹…
        success_rate = (successful_posts / total_posts * 100) if total_posts > 0 else 0
        print(f"[INFO] {board_name} í¬ë¡¤ë§ ì™„ë£Œ: {successful_posts}/{total_posts} ({success_rate:.1f}%)")
        
    except TimeoutException:
        print(f"[ERROR] {board_name}: í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ")
    except Exception as e:
        print(f"[ERROR] {board_name} í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
    finally:
        if driver:
            try:
                driver.quit()
            except:
                pass
    
    return posts

def extract_post_content_selenium(driver, url: str, timeout: int = 25):
    """Seleniumì„ ì‚¬ìš©í•œ ê²Œì‹œê¸€ ìƒì„¸ ë‚´ìš© ì¶”ì¶œ"""
    try:
        print(f"[DEBUG] ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ ì‹œë„: {url}")
        
        # ìºì‹œ í™•ì¸
        cached_content = get_cached_content(url)
        if cached_content:
            print(f"[DEBUG] ìºì‹œì—ì„œ ë¡œë“œ: {url}")
            return cached_content.get("content", "")
        
        # ìƒˆ íƒ­ì—ì„œ ê²Œì‹œê¸€ ì—´ê¸°
        driver.execute_script(f"window.open('{url}', '_blank');")
        driver.switch_to.window(driver.window_handles[-1])
        
        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        print(f"[DEBUG] í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘... ({timeout}ì´ˆ)")
        WebDriverWait(driver, timeout).until(  # Master ìˆ˜ì •: ë™ì  íƒ€ì„ì•„ì›ƒ ì ìš©
            EC.presence_of_element_located((By.CLASS_NAME, "board_view"))
        )
        
        # ìµœì í™”ëœ ìŠ¤í¬ë¡¤ë§
        print(f"[DEBUG] ìµœì í™”ëœ ìŠ¤í¬ë¡¤ë§ ì‹œì‘...")
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight/3);")
        time.sleep(1)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight/2);")
        time.sleep(1)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)
        print(f"[DEBUG] ìµœì í™”ëœ ìŠ¤í¬ë¡¤ë§ ì™„ë£Œ")
        
        # ì½˜í…ì¸  ì¶”ì¶œ
        content_element = driver.find_element(By.CSS_SELECTOR, ".board_view .view_content")
        content = content_element.text.strip()
        
        # íƒ­ ë‹«ê¸°
        driver.close()
        driver.switch_to.window(driver.window_handles[0])
        
        return content
        
    except TimeoutException:
        print(f"[ERROR] ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ íƒ€ì„ì•„ì›ƒ: {url}")
        # Master ìˆ˜ì •: íƒ­ ì •ë¦¬ í›„ None ë°˜í™˜ (ì „ì²´ ì¤‘ë‹¨í•˜ì§€ ì•ŠìŒ)
        try:
            if len(driver.window_handles) > 1:
                driver.close()
                driver.switch_to.window(driver.window_handles[0])
        except:
            pass
        return None
    except Exception as e:
        print(f"[ERROR] ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {url} - {e}")
        # Master ìˆ˜ì •: íƒ­ ì •ë¦¬ í›„ None ë°˜í™˜ (ì „ì²´ ì¤‘ë‹¨í•˜ì§€ ì•ŠìŒ)
        try:
            if len(driver.window_handles) > 1:
                driver.close()
                driver.switch_to.window(driver.window_handles[0])
        except:
            pass
        return None

# =============================================================================
# ë£¨ë¦¬ì›¹ í¬ë¡¤ë§
# =============================================================================

def crawl_ruliweb_epic7():
    """ë£¨ë¦¬ì›¹ ì—í”½ì„¸ë¸ ê²Œì‹œíŒ í¬ë¡¤ë§"""
    print("[INFO] ğŸŒ ë£¨ë¦¬ì›¹ ì—í”½ì„¸ë¸ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹œì‘...")
    posts = []
    
    try:
        target = CRAWL_TARGETS["ruliweb"]
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        response = requests.get(target["url"], headers=headers, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ
        post_elements = soup.select('table.board_list_table tbody tr')
        
        if not post_elements:
            print("[WARNING] ë£¨ë¦¬ì›¹: ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return posts
        
        print(f"[INFO] ë£¨ë¦¬ì›¹: {len(post_elements)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")
        
        # Master ìš”êµ¬ì‚¬í•­: Skip & Continue + 3íšŒ ì¬ì‹œë„ ë¡œì§
        successful_posts = 0
        total_posts = len(post_elements)
        
        for i, post_element in enumerate(post_elements, 1):
            # 3íšŒ ì¬ì‹œë„ ë¡œì§
            retry_count = 0
            max_retries = 3
            post_processed = False
            
            while retry_count < max_retries and not post_processed:
                try:
                    print(f"[DEBUG] ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ {i}/{total_posts} ì²˜ë¦¬ ì¤‘ (ì‹œë„ {retry_count + 1}/{max_retries})...")
                    
                    # ê³µì§€ì‚¬í•­ ì œì™¸
                    if post_element.select_one('.notice'):
                        post_processed = True
                        continue
                    
                    # ê²Œì‹œê¸€ ë§í¬ ë° ì œëª© ì¶”ì¶œ
                    title_element = post_element.select_one('td.subject a.deco')
                    if not title_element:
                        retry_count += 1
                        if retry_count < max_retries:
                            print(f"[WARNING] ê²Œì‹œê¸€ ì •ë³´ ë¶€ì¡±, ì¬ì‹œë„ ì¤‘... ({retry_count}/{max_retries})")
                            time.sleep(1)
                            continue
                        else:
                            print(f"[WARNING] ê²Œì‹œê¸€ {i} ìŠ¤í‚µ: ì •ë³´ ë¶€ì¡±")
                            break
                    
                    post_title = title_element.text.strip()
                    post_url = urljoin(target["url"], title_element.get('href', ''))
                    
                    # ì¤‘ë³µ í™•ì¸
                    if is_recently_processed(post_url):
                        print(f"[INFO] ê²Œì‹œê¸€ ìŠ¤í‚µ (24ì‹œê°„ ë‚´ ì²˜ë¦¬ë¨): {post_title[:30]}...")
                        post_processed = True
                        successful_posts += 1
                        break
                    
                    # ê²Œì‹œê¸€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                    try:
                        author_element = post_element.select_one('td.writer')
                        author = author_element.text.strip() if author_element else "Unknown"
                    except:
                        author = "Unknown"
                    
                    try:
                        date_element = post_element.select_one('td.time')
                        date = date_element.text.strip() if date_element else ""
                    except:
                        date = ""
                    
                    # ê²Œì‹œê¸€ ìƒì„¸ ë‚´ìš© ì¶”ì¶œ
                    content = extract_ruliweb_post_content(post_url)
                    
                    if content:
                        post_data = {
                            "title": post_title,
                            "url": post_url,
                            "author": author,
                            "date": date,
                            "content": content,
                            "board": "ë£¨ë¦¬ì›¹ ì—í”½ì„¸ë¸",
                            "site": "ruliweb",
                            "language": "kr",
                            "crawled_at": datetime.now().isoformat()
                        }
                        
                        posts.append(post_data)
                        successful_posts += 1
                        post_processed = True
                        
                        # ğŸš€ í•µì‹¬ ìˆ˜ì •: ì—¬ê¸°ì„œëŠ” ë§í¬ë¥¼ ì¶”ê°€í•˜ì§€ ì•ŠìŒ (ì•Œë¦¼ ì„±ê³µ í›„ì—ë§Œ ì¶”ê°€)
                        print(f"[INFO] âœ… ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì™„ë£Œ: {post_title[:50]}...")
                        
                        # ìºì‹œì— ì €ì¥
                        cache_content(post_url, post_data)
                    else:
                        retry_count += 1
                        if retry_count < max_retries:
                            print(f"[WARNING] ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨, ì¬ì‹œë„ ì¤‘... ({retry_count}/{max_retries})")
                            time.sleep(2)
                            continue
                        else:
                            print(f"[WARNING] ê²Œì‹œê¸€ {i} ìŠ¤í‚µ: ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨")
                            break
                    
                except Exception as e:
                    retry_count += 1
                    if retry_count < max_retries:
                        print(f"[ERROR] ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜, ì¬ì‹œë„ ì¤‘... ({retry_count}/{max_retries}): {e}")
                        time.sleep(2)
                        continue
                    else:
                        print(f"[ERROR] ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ {i} ìµœì¢… ì‹¤íŒ¨ - ë‹¤ìŒìœ¼ë¡œ ì§„í–‰: {e}")
                        break
                    
                # ê³¼ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´
                time.sleep(random.uniform(2, 4))
        
        # Master ìš”êµ¬ì‚¬í•­: ì„±ê³µë¥  ë¡œê¹…
        success_rate = (successful_posts / total_posts * 100) if total_posts > 0 else 0
        print(f"[INFO] ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì™„ë£Œ: {successful_posts}/{total_posts} ({success_rate:.1f}%)")
        
    except requests.RequestException as e:
        print(f"[ERROR] ë£¨ë¦¬ì›¹ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {e}")
    except Exception as e:
        print(f"[ERROR] ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
    
    return posts

def extract_ruliweb_post_content(url: str):
    """ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ìƒì„¸ ë‚´ìš© ì¶”ì¶œ"""
    try:
        print(f"[DEBUG] ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ: {url}")
        
        # ìºì‹œ í™•ì¸
        cached_content = get_cached_content(url)
        if cached_content:
            print(f"[DEBUG] ìºì‹œì—ì„œ ë¡œë“œ: {url}")
            return cached_content.get("content", "")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ
        content_element = soup.select_one('.view_content, .article_content')
        if content_element:
            content = content_element.get_text(strip=True)
            return content
        else:
            print(f"[WARNING] ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {url}")
            return ""
            
    except requests.RequestException as e:
        print(f"[ERROR] ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ìš”ì²­ ì‹¤íŒ¨: {url} - {e}")
        return ""
    except Exception as e:
        print(f"[ERROR] ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {url} - {e}")
        return ""

# =============================================================================
# Reddit í¬ë¡¤ë§
# =============================================================================

def crawl_reddit_epic7():
    """Reddit Epic Seven ì„œë¸Œë ˆë”§ í¬ë¡¤ë§"""
    print("[INFO] ğŸŒ Reddit Epic Seven í¬ë¡¤ë§ ì‹œì‘...")
    posts = []
    driver = None
    
    try:
        target = CRAWL_TARGETS["reddit"]
        
        # WebDriver ì„¤ì •
        driver = setup_chrome_driver()
        if not driver:
            print("[ERROR] Reddit: WebDriver ì„¤ì • ì‹¤íŒ¨")
            return posts
        
        print(f"[INFO] Reddit í˜ì´ì§€ ë¡œë”©: {target['url']}")
        driver.get(target["url"])
        
        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (Redditì€ ë™ì  ë¡œë”©)
        time.sleep(5)
        
        # ìŠ¤í¬ë¡¤í•˜ì—¬ ë” ë§ì€ ê²Œì‹œê¸€ ë¡œë“œ
        for i in range(3):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
        
        # ê²Œì‹œê¸€ ìš”ì†Œ ì°¾ê¸°
        post_elements = driver.find_elements(By.CSS_SELECTOR, '[data-testid="post-container"]')
        
        if not post_elements:
            print("[WARNING] Reddit: ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return posts
        
        print(f"[INFO] Reddit: {len(post_elements)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")
        
        # Master ìš”êµ¬ì‚¬í•­: Skip & Continue + 3íšŒ ì¬ì‹œë„ ë¡œì§
        successful_posts = 0
        total_posts = len(post_elements)
        
        for i, post_element in enumerate(post_elements, 1):
            # 3íšŒ ì¬ì‹œë„ ë¡œì§
            retry_count = 0
            max_retries = 3
            post_processed = False
            
            while retry_count < max_retries and not post_processed:
                try:
                    print(f"[DEBUG] Reddit ê²Œì‹œê¸€ {i}/{total_posts} ì²˜ë¦¬ ì¤‘ (ì‹œë„ {retry_count + 1}/{max_retries})...")
                    
                    # ê²Œì‹œê¸€ ì œëª© ë° ë§í¬ ì¶”ì¶œ
                    title_element = post_element.find_element(By.CSS_SELECTOR, '[data-testid="post-title"]')
                    post_title = title_element.text.strip()
                    
                    # Reddit ê²Œì‹œê¸€ URL ì¶”ì¶œ
                    link_element = post_element.find_element(By.CSS_SELECTOR, 'a[data-testid="post-title"]')
                    post_url = link_element.get_attribute("href")
                    
                    if not post_url or not post_title:
                        retry_count += 1
                        if retry_count < max_retries:
                            print(f"[WARNING] ê²Œì‹œê¸€ ì •ë³´ ë¶€ì¡±, ì¬ì‹œë„ ì¤‘... ({retry_count}/{max_retries})")
                            time.sleep(1)
                            continue
                        else:
                            print(f"[WARNING] ê²Œì‹œê¸€ {i} ìŠ¤í‚µ: ì •ë³´ ë¶€ì¡±")
                            break
                    
                    # ì¤‘ë³µ í™•ì¸
                    if is_recently_processed(post_url):
                        print(f"[INFO] ê²Œì‹œê¸€ ìŠ¤í‚µ (24ì‹œê°„ ë‚´ ì²˜ë¦¬ë¨): {post_title[:30]}...")
                        post_processed = True
                        successful_posts += 1
                        break
                    
                    # ê²Œì‹œê¸€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                    try:
                        author_element = post_element.find_element(By.CSS_SELECTOR, '[data-testid="post-byline"] a')
                        author = author_element.text.strip()
                    except:
                        author = "Unknown"
                    
                    try:
                        time_element = post_element.find_element(By.CSS_SELECTOR, '[data-testid="post-timestamp"]')
                        date = time_element.get_attribute("title") or time_element.text.strip()
                    except:
                        date = ""
                    
                    # ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ (Redditì€ ì œëª©ì´ ì£¼ìš” ë‚´ìš©)
                    try:
                        content_element = post_element.find_element(By.CSS_SELECTOR, '[data-testid="post-content"] p')
                        content = content_element.text.strip()
                    except:
                        content = post_title  # ë‚´ìš©ì´ ì—†ìœ¼ë©´ ì œëª© ì‚¬ìš©
                    
                    if content:
                        post_data = {
                            "title": post_title,
                            "url": post_url,
                            "author": author,
                            "date": date,
                            "content": content,
                            "board": "Reddit Epic Seven",
                            "site": "reddit",
                            "language": "en",
                            "crawled_at": datetime.now().isoformat()
                        }
                        
                        posts.append(post_data)
                        successful_posts += 1
                        post_processed = True
                        
                        # ğŸš€ í•µì‹¬ ìˆ˜ì •: ì—¬ê¸°ì„œëŠ” ë§í¬ë¥¼ ì¶”ê°€í•˜ì§€ ì•ŠìŒ (ì•Œë¦¼ ì„±ê³µ í›„ì—ë§Œ ì¶”ê°€)
                        print(f"[INFO] âœ… Reddit ê²Œì‹œê¸€ ìˆ˜ì§‘ ì™„ë£Œ: {post_title[:50]}...")
                        
                        # ìºì‹œì— ì €ì¥
                        cache_content(post_url, post_data)
                    else:
                        retry_count += 1
                        if retry_count < max_retries:
                            print(f"[WARNING] ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨, ì¬ì‹œë„ ì¤‘... ({retry_count}/{max_retries})")
                            time.sleep(2)
                            continue
                        else:
                            print(f"[WARNING] ê²Œì‹œê¸€ {i} ìŠ¤í‚µ: ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨")
                            break
                    
                except Exception as e:
                    retry_count += 1
                    if retry_count < max_retries:
                        print(f"[ERROR] Reddit ê²Œì‹œê¸€ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜, ì¬ì‹œë„ ì¤‘... ({retry_count}/{max_retries}): {e}")
                        time.sleep(2)
                        continue
                    else:
                        print(f"[ERROR] Reddit ê²Œì‹œê¸€ {i} ìµœì¢… ì‹¤íŒ¨ - ë‹¤ìŒìœ¼ë¡œ ì§„í–‰: {e}")
                        break
                
                # ê³¼ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´
                time.sleep(random.uniform(1, 2))
        
        # Master ìš”êµ¬ì‚¬í•­: ì„±ê³µë¥  ë¡œê¹…
        success_rate = (successful_posts / total_posts * 100) if total_posts > 0 else 0
        print(f"[INFO] Reddit í¬ë¡¤ë§ ì™„ë£Œ: {successful_posts}/{total_posts} ({success_rate:.1f}%)")
        
    except Exception as e:
        print(f"[ERROR] Reddit í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
    finally:
        if driver:
            try:
                driver.quit()
            except:
                pass
    
    return posts

# =============================================================================
# ìŠ¤ì¼€ì¤„ ê¸°ë°˜ í¬ë¡¤ë§ ì œì–´
# =============================================================================

def crawl_by_schedule(schedule_type: str) -> List[Dict]:
    """ìŠ¤ì¼€ì¤„ì— ë”°ë¥¸ í¬ë¡¤ë§ ì‹¤í–‰"""
    all_posts = []
    
    try:
        print(f"[INFO] === {schedule_type} í¬ë¡¤ë§ ì‹œì‘ ===")
        
        if schedule_type == "15min":
            # 15ë¶„ ì£¼ê¸°: ì „ì²´ ì‚¬ì´íŠ¸ í¬ë¡¤ë§
            print("[INFO] 15ë¶„ ì£¼ê¸°: ì „ì²´ ì‚¬ì´íŠ¸ í¬ë¡¤ë§")
            all_posts.extend(crawl_frequent_sites())
            
        elif schedule_type == "30min":
            # 30ë¶„ ì£¼ê¸°: ì¼ë°˜ ê²Œì‹œíŒ í¬ë¡¤ë§
            print("[INFO] 30ë¶„ ì£¼ê¸°: ì¼ë°˜ ê²Œì‹œíŒ í¬ë¡¤ë§")
            all_posts.extend(crawl_regular_sites())
            
        else:
            print(f"[WARNING] ì•Œ ìˆ˜ ì—†ëŠ” ìŠ¤ì¼€ì¤„ íƒ€ì…: {schedule_type}")
            return []
        
        print(f"[INFO] === {schedule_type} í¬ë¡¤ë§ ì™„ë£Œ: {len(all_posts)}ê°œ ê²Œì‹œê¸€ ===")
        return all_posts
        
    except Exception as e:
        print(f"[ERROR] {schedule_type} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

def crawl_frequent_sites() -> List[Dict]:
    """15ë¶„ ì£¼ê¸° í¬ë¡¤ë§ ëŒ€ìƒ ì‚¬ì´íŠ¸ë“¤"""
    all_posts = []
    
    try:
        # ë²„ê·¸ ê²Œì‹œíŒ ìš°ì„  í¬ë¡¤ë§
        all_posts.extend(crawl_stove_korea_bug_board())
        all_posts.extend(crawl_stove_global_bug_board())
        
        # Reddit í¬ë¡¤ë§
        all_posts.extend(crawl_reddit_epic7())
        
        return all_posts
        
    except Exception as e:
        print(f"[ERROR] ë¹ˆë²ˆ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return all_posts

def crawl_regular_sites() -> List[Dict]:
    """30ë¶„ ì£¼ê¸° í¬ë¡¤ë§ ëŒ€ìƒ ì‚¬ì´íŠ¸ë“¤"""
    all_posts = []
    
    try:
        # ì¼ë°˜ ê²Œì‹œíŒ í¬ë¡¤ë§
        all_posts.extend(crawl_stove_korea_general_board())
        all_posts.extend(crawl_stove_global_general_board())
        
        # ë£¨ë¦¬ì›¹ í¬ë¡¤ë§
        all_posts.extend(crawl_ruliweb_epic7())
        
        return all_posts
        
    except Exception e:
        print(f"[ERROR] ì¼ë°˜ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return all_posts

# =============================================================================
# ë¦¬í¬íŠ¸ìš© ë°ì´í„° ìˆ˜ì§‘
# =============================================================================

def get_all_posts_for_report(hours: int = 24) -> List[Dict]:
    """ë¦¬í¬íŠ¸ ìƒì„±ì„ ìœ„í•œ ëª¨ë“  ê²Œì‹œê¸€ ë°ì´í„° ìˆ˜ì§‘"""
    all_posts = []
    
    try:
        print(f"[INFO] ë¦¬í¬íŠ¸ìš© ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ (ìµœê·¼ {hours}ì‹œê°„)")
        
        # ìºì‹œì—ì„œ ìµœê·¼ ë°ì´í„° ë¡œë“œ
        cache = load_content_cache()
        current_time = datetime.now()
        cutoff_time = current_time - timedelta(hours=hours)
        
        for url, data in cache.items():
            try:
                cached_time = datetime.fromisoformat(data.get("cached_at", ""))
                if cached_time > cutoff_time:
                    all_posts.append(data)
            except ValueError:
                continue
        
        print(f"[INFO] ë¦¬í¬íŠ¸ìš© ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(all_posts)}ê°œ ê²Œì‹œê¸€")
        return all_posts
        
    except Exception as e:
        print(f"[ERROR] ë¦¬í¬íŠ¸ìš© ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

# =============================================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ë“¤
# =============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        print("ğŸ® Epic7 í¬ë¡¤ëŸ¬ v4.3 ì‹œì‘")
        print("=" * 50)
        
        # ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰
        all_posts = []
        
        # ê° ì‚¬ì´íŠ¸ë³„ í¬ë¡¤ë§
        all_posts.extend(crawl_stove_korea_bug_board())
        all_posts.extend(crawl_stove_global_bug_board())
        all_posts.extend(crawl_stove_korea_general_board())
        all_posts.extend(crawl_stove_global_general_board())
        all_posts.extend(crawl_ruliweb_epic7())
        all_posts.extend(crawl_reddit_epic7())
        
        print("=" * 50)
        print(f"ğŸ¯ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ: {len(all_posts)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘")
        
        return all_posts
        
    except Exception as e:
        print(f"[ERROR] ë©”ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return []

if __name__ == "__main__":
    main()