# crawler.py
import requests
from bs4 import BeautifulSoup
import random
import time
from random import uniform

# ì„¸ì…˜ ê°ì²´ ìƒì„± (ì»¤ë„¥ì…˜ í’€ ìœ ì§€)
session = requests.Session()

# ëœë¤ User-Agent, Referer, Accept-Language ë¡œë´‡ íƒì§€ íšŒí”¼ìš©
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko)'
]
REFERERS = ['https://google.com', 'https://bing.com', 'https://duckduckgo.com']
LANGUAGES = ['ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7', 'en-US,en;q=0.9']

# ëœë¤ í—¤ë” ìƒì„± í•¨ìˆ˜
def get_headers():
    return {
        'User-Agent': random.choice(USER_AGENTS),
        'Referer': random.choice(REFERERS),
        'Accept-Language': random.choice(LANGUAGES),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
    }

# í˜ì´ì§€ í¬ë¡¤ë§ í•¨ìˆ˜
def fetch_posts(url, selector, source, force_bug=False):
    """
    ì§€ì •ëœ URLê³¼ CSS ì„ íƒìë¡œ ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ
    :param url: í˜ì´ì§€ URL
    :param selector: ê²Œì‹œê¸€ ì„ íƒì
    :param source: ê²Œì‹œê¸€ ì¶œì²˜(ì»¤ë®¤ë‹ˆí‹° ì´ë¦„)
    :param force_bug: Trueë©´ ë¬´ì¡°ê±´ bugë¡œ ì²˜ë¦¬
    """
    time.sleep(uniform(0.5, 1.5))  # ëœë¤ ë”œë ˆì´
    try:
        resp = session.get(url, headers=get_headers(), timeout=10)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')
        elements = soup.select(selector)
        posts = []
        for el in elements:
            title = el.get_text(strip=True)
            link = el.get('href')
            if link and not link.startswith("http"):
                link = url + link
            post_data = {'title': title, 'url': link, 'source': source}
            if force_bug:
                post_data['force_bug'] = True  # Stove ë²„ê·¸ ê²Œì‹œíŒì€ ë¬´ì¡°ê±´ bug
            posts.append(post_data)
        return posts
    except Exception as e:
        print(f"[!] {source} ì˜¤ë¥˜: {e}")
        return []

# Redditì€ JSON API í™œìš©
def crawl_reddit():
    time.sleep(uniform(0.5, 1.5))
    try:
        headers = get_headers()
        headers['User-Agent'] = 'Mozilla/5.0 RedditMonitor'
        resp = session.get("https://www.reddit.com/r/EpicSeven/.json", headers=headers, timeout=10)
        resp.raise_for_status()
        data = resp.json()
        posts = []
        for p in data["data"]["children"]:
            posts.append({
                "title": p["data"]["title"],
                "url": "https://reddit.com" + p["data"]["permalink"],
                "source": "Reddit"
            })
        return posts
    except Exception as e:
        print(f"[!] reddit ì˜¤ë¥˜: {e}")
        return []

# ğŸš€ ìµœì¢… crawl_all_sites: ëª¨ë“  ì‚¬ì´íŠ¸ ìˆœíšŒ + Stove ë²„ê·¸ ê²Œì‹œíŒì€ force_bug
def crawl_all_sites():
    posts = []
    posts += fetch_posts("https://arca.live/b/epic7", "a.title", "ì•„ì¹´ë¼ì´ë¸Œ")
    posts += fetch_posts("https://bbs.ruliweb.com/game/84925", "a.deco", "ë£¨ë¦¬ì›¹")
    posts += fetch_posts("https://page.onstove.com/epicseven/kr", "a.article-link", "ìŠ¤í† ë¸Œ")
    posts += fetch_posts("https://forum.epic7.global/", "a.node-title", "ê¸€ë¡œë²Œ í¬ëŸ¼")
    posts += fetch_posts("https://forum.gamer.com.tw/A.php?bsn=35366", "a.b-list__main__title", "ë°”í•˜ë¬´íŠ¸")
    posts += crawl_reddit()

    # ğŸ”¥ Stove ë²„ê·¸ ê²Œì‹œíŒì€ ë¬´ì¡°ê±´ bug ì•Œë¦¼
    posts += fetch_posts(
        "https://page.onstove.com/epicseven/kr/list/1012",
        "a.article-link",
        "ìŠ¤í† ë¸Œ ë²„ê·¸ ê²Œì‹œíŒ",
        force_bug=True
    )

    return posts
